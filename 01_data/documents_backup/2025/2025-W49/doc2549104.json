{
  "context": "AutoNeural, an NPU-native VLM architecture, improves efficiency and performance on edge devices by using integer-only inference, MobileNetV5-style backbone, and a hybrid design with SSM and Transformer layers, reducing quantization errors and latency. WhileNeural Processing Units (NPUs)offer high theoretical efficiency for edge AI, state-of-the-artVision--Language Models (VLMs)tailored for GPUs often falter on these substrates. We attribute this hardware-model mismatch to two primary factors: the quantization brittleness ofVision Transformers (ViTs)and the I/O-bound nature of autoregressive attention mechanisms, which fail to utilize the high arithmetic throughput of NPUs. To bridge this gap, we propose AutoNeural, an NPU-native VLM architecture co-designed for integer-only inference. We replace the standard ViT encoder with aMobileNetV5-style backbone utilizingdepthwise separable convolutions, which ensures bounded activation distributions for stable INT4/8/16 quantization. Complementing this, our language backbone integratesState-Space Model (SSM)principles withTransformer layers, employing efficientgated convolutionsto achieve linear-time complexity. This hybrid design eliminates the heavy memory I/O overhead ofKey-Value cachingduring generation. Our approach delivers substantial efficiency gains, reducing quantization error of vision encoder by up to 7x and end-to-end latency by 14x compared to conventional baselines. The AutoNeural also delivers 3x decoding speed and 4x longer context window than the baseline. We validate these improvements via a real-world automotive case study on theQualcomm SA8295P SoC, demonstrating real-time performance for cockpit applications. Our results highlight that rethinking model topology specifically for NPU constraints is a prerequisite for robust multi-modal edge intelligence. AutoNeural-VL-1.5B — the world's first real-time multimodal model built for in-car AI. It runs fully local on the Qualcomm SA8295P NPU with a software–hardware co-designed architecture, setting a new bar for speed and quality. AutoNeural redefines what AI can do in the car. Imagine how helpful your car can be when it truly understands you and the world around it in real-time. We co-developed the model with Geely for next-generation production smart cockpit experiences. Compared to current solution, it delivers: This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ·Sign uporlog into comment",
  "metadata": {
    "title": "AutoNeural: Co-Designing Vision-Language Models for NPU Inference",
    "authors": [],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2512.02924",
    "upvote": 5,
    "tags": [
      "vlm",
      "throughput npus",
      "16 quantization"
    ]
  }
}