{
  "context": "PretrainZero is a reinforcement active learning framework that enhances general reasoning capabilities by pretraining large models on a corpus without verifiable labels, improving performance on benchmarks compared to domain-specific training. Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recentreinforcement learning(RL) based large-thinking models demonstrate impressive expert-level abilities, i.e., software and math, but still rely heavily on verifiable rewards in specific domains, placing a significant bottleneck to extend the performance boundary of general reasoning capabilities. In this work, we propose PretrainZero, a reinforcementactive learningframework built on the pretraining corpus to extendRLfrom domain-specific post-training to general pretraining. PretrainZero features the following characteristics: 1)Active pretraining: inspired by theactive learningability of humans, PretrainZero learns a unified reasoning policy to actively identify reasonable and informative contents from pretraining corpus, and reason to predict these contents byRL. 2)Self-supervised learning: without any verifiable labels,pretrained reward models, or supervised fine-tuning, we directly pretrain reasoners from 3 to 30B base models on the general Wikipedia corpus usingRL, significantly breaking the verification data-wall for general reasoning. 3) Verification scaling: by tackling increasingly challenging masked spans, PretrainZero substantially enhances the general reasoning abilities of pretrained base models. In reinforcement pretraining, PretrainZero improvesQwen3-4B-Basefor 8.43, 5.96 and 10.60 onMMLU-Pro,SuperGPQAandmath average benchmarks. In post-training, the pretrained models can also serve as reasoning foundation models for downstreamRLVR tasks. Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recent reinforcement learning (RL) based large-thinking models demonstrate impressive expert-level abilities, i.e., software and math, but still rely heavily on verifiable rewards in specific domains, placing a significant bottleneck to extend the performance boundary of general reasoning capabilities. In this work, we propose PretrainZero, a reinforcement active learning framework built on the pretraining corpus to extend RL from domain-specific post-training to general pretraining. PretrainZero features the following characteristics: 1) Active pretraining: inspired by the active learning ability of humans, PretrainZero learns a unified reasoning policy to actively identify reasonable and informative contents from pretraining corpus, and reason to predict these contents by RL. 2) Self-supervised learning: without any verifiable labels, pretrained reward models, or supervised fine-tuning, we directly pretrain reasoners from 3 to 30B base models on the general Wikipedia corpus using RL, significantly breaking the verification data-wall for general reasoning. 3) Verification scaling: by tackling increasingly challenging masked spans, PretrainZero substantially enhances the general reasoning abilities of pretrained base models. In reinforcement pretraining, PretrainZero improves Qwen3-4B-Base for 8.43, 5.96 and 10.60 on MMLU-Pro, SuperGPQA and math average benchmarks. In post-training, the pretrained models can also serve as reasoning foundation models for downstream RLVR tasks. Â·Sign uporlog into comment",
  "metadata": {
    "paper_name": "PretrainZero: Reinforcement Active Pretraining",
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2512.03442",
    "upvote": 18,
    "tag1": "general",
    "tag2": "pretraining",
    "tag3": "pretrainzero"
  }
}