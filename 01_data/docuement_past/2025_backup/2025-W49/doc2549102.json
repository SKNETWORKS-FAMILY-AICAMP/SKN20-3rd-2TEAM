{
  "context": "SkillFactory is a method for fine-tuning models to learn cognitive skills through supervised fine-tuning before reinforcement learning, enhancing their robustness and generalization post-RL. Reasoning modelsleveraginglong chains of thoughtemploy variouscognitive skills, such asverificationof their answers,backtracking,retryingby an alternate method, and more. Previous work has shown that when a base language model exhibits these skills, training that model further withreinforcement learning(RL) can learn to leverage them. How can we get models to leverage skills that aren't exhibited by base models? Our work,SkillFactory, is a method forfine-tuningmodels to roughly learn these skills during asupervised fine-tuning(SFT) stage prior to RL. Our approach does not rely on distillation from a stronger model, but instead uses samples from the model itself, rearranged to provide training data in the format of those skills. These \"silver\" SFT traces may be imperfect, but are nevertheless effective for priming a model to acquire skills during RL. Our evaluation shows that (1) starting fromSkillFactorySFT initialization helps a model to generalize to harder variants of a task post-RL, despite lower performance pre-RL; (2)cognitive skillsare indeed used by the model; (3) RLedSkillFactorymodels are more robust toregressionon out-of-domain tasks than RLed base models. Our work suggests thatinductive biaseslearned prior to RL help models learn robust cognitive skill use. SkillFactory: Self-Distillation For Learning Cognitive Behaviors Â·Sign uporlog into comment",
  "metadata": {
    "paper_name": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors",
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2512.04072",
    "upvote": 2,
    "tag1": "skill",
    "tag2": "cognitive",
    "tag3": "learn"
  }
}