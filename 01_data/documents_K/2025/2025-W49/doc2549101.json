{
  "context": "StreamGaze is a benchmark that evaluates how effectively models use gaze signals for temporal and proactive reasoning in streaming videos, revealing limitations in current MLLM capabilities. Streaming video understanding requires models not only to process temporally incoming frames, but also to anticipate user intention for realistic applications like AR glasses. While prior streaming benchmarks evaluatetemporal reasoning, none measure whetherMLLMscan interpret or leverage humangaze signalswithin a streaming setting. To fill this gap, we introduce StreamGaze, the first benchmark designed to evaluate how effectivelyMLLMsuse gaze for temporal andproactivereasoning in streaming videos. StreamGaze introduces gaze-guidedpast,present, andproactivetasks that comprehensively evaluate streaming video understanding. These tasks assess whether models can use real-time gaze to follow shifting attention and infer user intentions from onlypastand currently observed frames. To build StreamGaze, we develop a gaze-video QA generation pipeline that aligns egocentric videos with raw gaze trajectories viafixation extraction,region-specific visual prompting, andscanpath construction. This pipeline producesspatio-temporally grounded QApairs that closely reflecthuman perceptual dynamics. Across all StreamGaze tasks, we observe substantial performance gaps between state-of-the-artMLLMsand human performance, revealing fundamental limitations in gaze-basedtemporal reasoning, intention modeling, andproactiveprediction. We further provide detailed analyses ofgaze-prompting strategies,reasoning behaviors, and task-specificfailure modes, offering deeper insight into why currentMLLMsstruggle and what capabilities future models must develop. All data and code will be publicly released to support continued research in gaze-guided streaming video understanding. Streaming video understanding requires models not only to process temporally incoming frames, but also to anticipate user intention for realistic applications like AR glasses. While prior streaming benchmarks evaluate temporal reasoning, none measure whether MLLMs can interpret or leverage human gaze signals within a streaming setting. To fill this gap, we introduce StreamGaze, the first benchmark designed to evaluate how effectively MLLMs use gaze for temporal and proactive reasoning in streaming videos. StreamGaze introduces gaze-guided past, present, and proactive tasks that comprehensively evaluate streaming video understanding. These tasks assess whether models can use real-time gaze to follow shifting attention and infer user intentions from only past and currently observed frames. To build StreamGaze, we develop a gaze-video QA generation pipeline that aligns egocentric videos with raw gaze trajectories via fixation extraction, region-specific visual prompting, and scanpath construction. This pipeline produces spatio-temporally grounded QA pairs that closely reflect human perceptual dynamics. Across all StreamGaze tasks, we observe substantial performance gaps between state-of-the-art MLLMs and human performance, revealing fundamental limitations in gaze-based temporal reasoning, intention modeling, and proactive prediction. We further provide detailed analyses of gaze-prompting strategies, reasoning behaviors, and task-specific failure modes, offering deeper insight into why current MLLMs struggle and what capabilities future models must develop. All data and code will be publicly released to support continued research in gaze-guided streaming video understanding. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "title": "StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos",
    "authors": [],
    "publication_year": 2025,
    "github_url": "https://github.com/daeunni/StreamGaze",
    "huggingface_url": "https://huggingface.co/papers/2512.01707",
    "upvote": 7,
    "tags": [
      "video understanding",
      "limitations gaze",
      "gaze guidedpast"
    ]
  }
}