{
  "context": "Multiplayer Nash Preference Optimization (MNPO) extends Nash learning from human feedback to handle complex, non-transitive human preferences by formulating alignment as an n-player game. Reinforcement learning from human feedback(RLHF) has emerged as the standard\nparadigm for aligninglarge language models(LLMs) with human preferences.\nHowever, reward-based methods built on theBradley-Terry assumptionstruggle to\ncapture the non-transitive and heterogeneous nature of real-world preferences.\nTo address this, recent studies have reframed alignment as a two-player Nash\ngame, giving rise toNash learning from human feedback(NLHF). While this\nperspective has inspired algorithms such asINPO,ONPO, andEGPOwith strong\ntheoretical and empirical guarantees, they remain fundamentally restricted to\ntwo-player interactions, creating a single-opponent bias that fails to capture\nthe full complexity of realistic preference structures. In this work, we\nintroduceMultiplayer Nash Preference Optimization(MNPO), a novel framework\nthat generalizesNLHFto the multiplayer regime. It formulates alignment as an\nn-player game, where each policy competes against a population of opponents\nwhile being regularized toward a reference model. Our framework establishes\nwell-definedNash equilibriain multiplayer settings and extends the concept ofduality gapto quantify approximation quality. We demonstrate thatMNPOinherits the equilibrium guarantees of two-player methods while enabling richer\ncompetitive dynamics and improved coverage of diverse preference structures.\nThrough comprehensive empirical evaluation, we show thatMNPOconsistently\noutperforms existingNLHFbaselines oninstruction-following benchmarks,\nachieving superior alignment quality underheterogeneous annotator conditionsandmixed-policy evaluationscenarios. Together, these results establishMNPOas a principled and scalable framework for aligningLLMswith complex,\nnon-transitive human preferences. Code is available at\nhttps://github.com/smiles724/MNPO. ðŸš€ New paper: Multiplayer Nash Preference Optimization (MNPO) Preference optimization for LLMs has mostly been stuck in the two-player game setting (DPO, IPO, INPO, EGPOâ€¦). But real human feedback is messy, diverse, and non-transitiveâ€”it looks much more like a multiplayer game. We introduce MNPO, the first framework that generalizes Nash learning from human feedback to the multiplayer regime.âœ… Theoretically grounded: defines multiplayer Nash equilibria & duality gap.âœ… Algorithmically scalable: unifies many existing PO methods as special cases.âœ… Empirically strong: MNPO outperforms all NLHF baselines on AlpacaEval 2, Arena-Hard, and MT-Benchâ€”sometimes even surpassing much larger LLMs and GPT-5 on alignment benchmarks Paper:https://arxiv.org/abs/2509.23102 Code:https://github.com/smiles724/MNPO This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "title": "Multiplayer Nash Preference Optimization",
    "authors": [
      "Fang Wu",
      "Weihao Xuan",
      "Peng Xia"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/smiles724/MNPO",
    "huggingface_url": "https://huggingface.co/papers/2509.23102",
    "upvote": 62,
    "tags": []
  }
}