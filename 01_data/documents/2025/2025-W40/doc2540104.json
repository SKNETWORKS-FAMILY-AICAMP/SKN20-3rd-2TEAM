{
    "context": "DeeptraceReward is a benchmark dataset that annotates human-perceived deepfake traces in videos, used to train multimodal language models for detecting AI-generated videos. Can humans identify AI-generated (fake) videos and provide grounded reasons?\nWhile video generation models have advanced rapidly, a critical dimension --\nwhether humans can detectdeepfake traceswithin a generated video, i.e.,spatiotemporal grounded visual artifactsthat reveal a video as machine\ngenerated -- has been largely overlooked. We introduce DeeptraceReward, the\nfirstfine-grained,spatially- and temporally- aware benchmarkthat annotates\nhuman-perceived fake traces for video generation reward. The dataset comprises\n4.3K detailed annotations across 3.3K high-quality generated videos. Each\nannotation provides a natural-language explanation, pinpoints a bounding-box\nregion containing the perceived trace, and marks precise onset and offset\ntimestamps. We consolidate these annotations into 9 major categories ofdeepfake tracesthat lead humans to identify a video as AI-generated, and trainmultimodal language models(LMs) asreward modelsto mimic human judgments and\nlocalizations. On DeeptraceReward, our 7B reward model outperforms GPT-5 by\n34.7% on average across fake clue identification, grounding, and explanation.\nInterestingly, we observe a consistent difficulty gradient: binary fake v.s.\nreal classification is substantially easier thanfine-graineddeepfake trace\ndetection; within the latter, performance degrades from natural language\nexplanations (easiest), tospatial grounding, totemporal labeling(hardest).\nBy foregrounding human-perceiveddeepfake traces, DeeptraceReward provides a\nrigorous testbed and training signal for socially aware and trustworthy video\ngeneration. https://deeptracereward.github.io/ This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
    "metadata": {
        "title": "Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal\n  LLMs",
        "authors": [
            "Pan Lu"
        ],
        "publication_year": 2025,
        "github_url": "",
        "huggingface_url": "https://huggingface.co/papers/2509.22646",
        "upvote": 16
    }
}