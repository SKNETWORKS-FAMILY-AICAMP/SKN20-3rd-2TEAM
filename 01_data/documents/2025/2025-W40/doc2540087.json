{
    "context": "A framework called Socratic-Zero autonomously generates high-quality training data through the co-evolution of three agents, improving reasoning tasks in large language models. Recent breakthroughs inlarge language models(LLMs) onreasoning tasksrely\nheavily on massive, high-quality datasets-typically human-annotated and thus\ndifficult to scale. Whiledata synthesisordistillationoffers a promising\nalternative, existing methods struggle with inconsistent data quality and an\ninability to dynamically adapt to the evolving capabilities of the model,\nleading to suboptimal training signals. To address these limitations, we\nintroduce Socratic-Zero, a fully autonomous framework that generates\nhigh-quality training data from minimal seed examples through the co-evolution\nof three agents: theTeacher, theSolver, and theGenerator. TheSolvercontinuously refines its reasoning by learning frompreference feedbackon both\nsuccessful and failedtrajectories; theTeacheradaptively crafts increasingly\nchallenging questions based on theSolver's weaknesses; and theGeneratordistills theTeacher'squestion-design strategyto enable scalable,\nhigh-fidelitycurriculum generation. This closed-loop system produces aself-improving curriculum-requiring no pre-existing tasks or labels.\nRemarkably, starting from only 100 seed questions, ourSocratic-Solver-8Bachieves an average gain of +20.2 percentage points over priordata synthesismethods across sevenmathematical reasoning benchmarks(AMC23, AIME24-25,\nOlympiad, MATH-500, Minerva, and GSM8K), with consistent gains on bothQwen3andGLM4series models. Even more surprisingly, synthetic data fromSocratic-Generator-32Benables studentLLMsto achieve superior performance\ncompared to other state-of-the-art (SOTA) commercialLLMson these benchmarks,\nincludingQwen3-235B-A22B,DeepSeek-V3.1-671B,GPT-5,Gemini-2.5-Pro,Grok-4,\nandClaude-4.1-Opus. Struggling to boost LLM reasoning? ğŸ¤¯ The endless need for data is a huge bottleneck.Current methods often train solvers OR generators in isolation, ignoring their crucial interaction.We introduce Socratic-Zero: A new framework where agents co-evolve, bootstrapping SOTA reasoning from (almost) nothing. ğŸ“š Paper:https://arxiv.org/pdf/2509.24726 Inspired by Socratic \"midwifery,\" Socratic-Zero creates a self-improving \"iron triangle\" ecosystem to produce better solvers AND generators.ğŸ§‘â€ğŸ“ Solver (Student): Solves problems & learns from its mistakes.ğŸ‘©â€ğŸ« Teacher (Master): Crafts new problems targeting the Solver's specific weaknesses.âœï¸ Generator (Apprentice): Learns the Teacher's expert strategy to create a scalable, high-quality curriculum. The system is fully autonomous. Starting with just 100 seed questions, it creates a closed \"teach-learn-practice\" loop, driving a spiral of improvement.ğŸš€ Solver Result: Our Socratic-Solver-8B achieves a +20% average gain across 7 math benchmarks, all without massive external datasets!But it's not just about solving. Our generator learns to create world-class problems.ğŸ¤¯ Generator Result: Our Socratic-Generator-32B produces data that enables a student model to outperform those trained on data from giants like GPT-5, Gemini-2.5-Pro, Claude-4.1-Opus, Grok-4, Qwen3-235B, & DeepSeek-V3.1! Co-evolution is key. By teaching agents how to teach each other, we unlock a new path for scalable, data-efficient reasoning.We welcome your feedback and critiques!ğŸ“š Paper:https://arxiv.org/pdf/2509.24726ğŸ’» Code:https://github.com/Frostlinx/Socratic-Zero  Â·Sign uporlog into comment",
    "metadata": {
        "title": "Socratic-Zero : Bootstrapping Reasoning via Data-Free Agent Co-evolution",
        "authors": [
            "Shaobo Wang"
        ],
        "publication_year": 2025,
        "github_url": "https://github.com/Frostlinx/Socratic-Zero",
        "huggingface_url": "https://huggingface.co/papers/2509.24726",
        "upvote": 19
    }
}