{
  "context": "SPEAR, a curriculum-based self-imitation learning method, manages exploration-exploitation balance in reinforcement learning for LLMs by using intrinsic rewards and trajectory-level entropy control. Reinforcement learning(RL) is the dominant paradigm for sharpening strategic\ntool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks,\nyet it faces a fundamental challenge ofexploration-exploitation trade-off.\nExisting studies stimulate exploration through the lens ofpolicy entropy, but\nsuch mechanical entropy maximization is prone to RL training instability due to\nthe multi-turn distribution shifting. In this paper, we target the progressive\nexploration-exploitation balance under the guidance of the agent own\nexperiences without succumbing to either entropy collapsing or runaway\ndivergence. We propose SPEAR, acurriculum-based self-imitation learning(SIL)\nrecipe for training agentic LLMs. It extends the vanilla SIL framework, where areplay bufferstores self-generated promising trajectories for off-policy\nupdate, by gradually steering the policy evolution within a well-balanced range\nof entropy across stages. Specifically, our approach incorporates a curriculum\nto manage the exploration process, utilizingintrinsic rewardsto foster\nskill-level exploration and facilitating action-level exploration through SIL.\nAt first, the auxiliary tool call reward plays a critical role in the\naccumulation of tool-use skills, enabling broad exposure to the unfamiliar\ndistributions of the environment feedback with an upward entropy trend. As\ntraining progresses,self-imitationgets strengthened to exploit existing\nsuccessful patterns from replayed experiences for comparative action-level\nexploration, accelerating solution iteration without unbounded entropy growth.\nTo further stabilize training, we recalibrate the advantages of experiences in\nthereplay bufferto address the potentialpolicy drift. Reugularizations such\nas the clipping of tokens with high covariance between probability and\nadvantage are introduced to the trajectory-levelentropy controlto curb\nover-confidence. Reinforcement learning (RL) is the dominant paradigm for sharpening strategic tool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks, yet it faces a fundamental challenge of exploration-exploitation trade-off. Existing studies stimulate exploration through the lens of policy entropy, but such mechanical entropy maximization is prone to RL training instability due to the multi-turn distribution shifting. In this paper, we target the progressive exploration-exploitation balance under the guidance of the agent own experiences without succumbing to either entropy collapsing or runaway divergence. We propose SPEAR, a curriculum-based self-imitation learning (SIL) recipe for training agentic LLMs. It extends the vanilla SIL framework, where a replay buffer stores self-generated promising trajectories for off-policy update, by gradually steering the policy evolution within a well-balanced range of entropy across stages. Specifically, our approach incorporates a curriculum to manage the exploration process, utilizing intrinsic rewards to foster skill-level exploration and facilitating action-level exploration through SIL. At first, the auxiliary tool call reward plays a critical role in the accumulation of tool-use skills, enabling broad exposure to the unfamiliar distributions of the environment feedback with an upward entropy trend. As training progresses, self-imitation gets strengthened to exploit existing successful patterns from replayed experiences for comparative action-level exploration, accelerating solution iteration without unbounded entropy growth. To further stabilize training, we recalibrate the advantages of experiences in the replay buffer to address the potential policy drift. Reugularizations such as the clipping of tokens with high covariance between probability and advantage are introduced to the trajectory-level entropy control to curb over-confidence. Code:https://github.com/TencentYoutuResearch/SPEARCheckpoints:https://huggingface.co/collections/yolay/spear-68da1c8b75098b1868db59c8 This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Code:https://github.com/TencentYoutuResearch/SPEARCheckpoints:https://huggingface.co/collections/yolay/spear-68da1c8b75098b1868db59c8 Â·Sign uporlog into comment",
  "metadata": {
    "title": "Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive\n  Exploration for Agentic Reinforcement Learning",
    "authors": [
      "Yulei Qin",
      "Yuzheng Cai",
      "Xing Sun"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/TencentYoutuResearch/SPEAR",
    "huggingface_url": "https://huggingface.co/papers/2509.22601",
    "upvote": 29,
    "tags": []
  }
}