{
    "context": "Rubric-based rewards mitigate reward over-optimization in reinforcement fine-tuning by leveraging off-policy examples while maintaining reward reliability. Reinforcement fine-tuning(RFT) often suffers from reward\nover-optimization, where apolicy modelhacks the reward signals to achieve\nhigh scores while producing low-quality outputs. Our theoretical analysis shows\nthat the key lies inreward misspecificationat the high-reward tail: the\ninability to reliably distinguish Excellent responses from merely Great ones.\nThis motivate us to focus on thehigh-reward region. However, such tail\nexamples are scarce under thebase LLM. Whileoff-policy exemplars(e.g. from\nstronger models or rewrites) are easier to obtain, naively training on them\nyields a misspecified reward for the policy we aim to align. To address this,\nwe studyrubric-based rewards. By design, rubrics can leverage off-policy\nexamples while remaining insensitive to their artifacts. To elicit rubrics that\ncapture the high-reward tail, we highlight the importance of distinguishing\namong great and diverse responses, and introduce a workflow to implement this\nidea. We empirically demonstrate thatrubric-based rewardssubstantially\nmitigatereward over-optimizationand deliver effective LLM post-training\nimprovements. Our code can be accessed at\nhttps://github.com/Jun-Kai-Zhang/rubrics.git . Reinforcement fine-tuning (RFT) often suffers from reward over-optimization, where a policy model hacks the reward signals to achieve high scores while producing low-quality outputs. Our theoretical analysis shows that the key lies in reward misspecification at the high-reward tail: the inability to reliably distinguish excellent responses from merely great ones. This motivate us to focus on the high-reward region. However, such tail examples are scarce under the base LLM. While off-policy exemplars (e.g. from stronger models or rewrites) are easier to obtain, naively training on them yields a misspecified reward for the policy we aim to align. To address this, we study rubric-based rewards. By design, rubrics can leverage off-policy examples while remaining insensitive to their artifacts. To elicit rubrics that capture the high-reward tail, we highlight the importance of distinguishing among great and diverse responses, and introduce a workflow to implement this idea. We empirically demonstrate that rubric-based rewards substantially mitigate reward over-optimization and deliver effective LLM post-training improvements. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
    "metadata": {
        "title": "Chasing the Tail: Effective Rubric-based Reward Modeling for Large\n  Language Model Post-Training",
        "authors": [
            "Swarnashree Mysore Sathyendra"
        ],
        "publication_year": 2025,
        "github_url": "",
        "huggingface_url": "https://huggingface.co/papers/2509.21500",
        "upvote": 18
    }
}