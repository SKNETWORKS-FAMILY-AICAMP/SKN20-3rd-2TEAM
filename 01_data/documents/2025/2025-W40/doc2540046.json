{
  "context": "PromptCoT 2.0 uses an EM loop to generate harder and more diverse synthetic prompts, improving reasoning capabilities in large language models through self-play and supervised fine-tuning. Large language models (LLMs) are evolving from conversational systems into\nstrongreasonersfor tasks such asOlympiad mathematicsand competitive\nprogramming. While scaling parameters and test-time computation has driven\nprogress, a key bottleneck is the lack of high-quality training problems:\nhuman-curated datasets are costly and limited, while existing synthetic corpora\nare often too easy or narrow. PromptCoT 1.0 showed that injectingrationalesintoprompt synthesisincreases problem difficulty. Building on this, we\npresent PromptCoT 2.0, a scalable framework that replaces hand-crafted\nheuristics with anexpectation-maximization (EM) loop, whererationalesare\niteratively refined to guide prompt construction. This produces problems that\nare both harder and more diverse than prior corpora. The synthetic prompts\nsupport two post-training regimes: (1)Self-Play, where strong models improve\nautonomously via verifiable feedback without stronger teachers; and (2)Supervised Fine-Tuning (SFT), where weaker models learn from teacher-distilled\ntraces. Extensive experiments demonstrate the effectiveness of this approach.\nInself-play, applying PromptCoT 2.0 toQwen3-30B-A3B-Thinking-2507sets new\nstate-of-the-art results at the 30B scale, with +4.4, +4.8, and +5.3 onAIME24/25 andHMMT25, +6.1 and +5.0 onLiveCodeBenchv5/v6, and +35 Elo onCodeforces. In SFT, trainingQwen2.5-7B-Instructsolely on synthetic prompts\nboosts accuracy to 73.1 (AIME24), 65.6 (AIME25), and 53.4 (LiveCodeBenchv5),\nsurpassing models trained on human or hybrid data. Analyses further confirm\nthat PromptCoT 2.0 yields fundamentally harder and distributionally distinct\nproblems. These results establishprompt synthesisas a new axis for scaling\nreasoning and position PromptCoT 2.0 as a scalable foundation for futureopen-source models. The implementation is available at\nhttps://github.com/inclusionAI/PromptCoT. TL;DR üß©Method Upgrade ‚Äì PromptCoT 2.0: We introduce anEM-style rationale‚Äìdriven synthesis loop(concept ‚Üí rationale ‚Üí problem) that generatesharder, more diverse math & code problemsthan previous datasets, without relying on handcrafted heuristics. üìöSFT with Fully Synthetic Data: Training a7B modelon our4.8M synthetic prompts + trajectories‚Äîwithout any human-written problems‚Äîoutperforms OpenMathReasoning and OpenCodeReasoning. üëâ This shows thatpurely synthetic promptscan serve as a stronger and more scalable alternative to the best human-curated corpora. üèÜSelf-Play for Larger Models: Applying PromptCoT 2.0 toQwen3-30B-A3B-Thinking-2507via self-play achievesnew SOTA at the 30B scale, with results competitive toGemini 2.5 ProandOpenAI o3, while activating only3B parameters. ‚ö°Open Resources for the Community: We release4.8M prompts + GPT-OSS-120B (medium) responses, where responses aremuch shorterthan those in OpenMathReasoning / OpenCodeReasoning (mainly from DeepSeek-R1). This makes our dataset especially suitable fornext-gen architectures(e.g., diffusion LLMs) andefficient training pipelines. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ¬∑Sign uporlog into comment",
  "metadata": {
    "title": "PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model\n  Reasoning",
    "authors": [
      "Xueliang Zhao"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/inclusionAI/PromptCoT",
    "huggingface_url": "https://huggingface.co/papers/2509.19894",
    "upvote": 33,
    "tags": []
  }
}