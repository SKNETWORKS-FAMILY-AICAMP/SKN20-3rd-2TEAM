{
  "context": "Sequential Diffusion Language Model (SDLM) enhances pre-trained autoregressive language models by adaptively determining generation length and maintaining KV-cache compatibility, achieving high efficiency and throughput. Diffusion language models(DLMs) have strong theoretical efficiency but are\nlimited byfixed-length decodingand incompatibility with key-value (KV)\ncaches.Block diffusionmitigates these issues, yet still enforces a fixed\nblock size and requires expensive training. We introduce Next Sequence\nPrediction (NSP), which unifies next-token and next-block prediction, enabling\nthe model to adaptively determine the generation length at each step. When the\nlength is fixed to 1, NSP reduces to standardnext-token prediction. Building\non NSP, we proposeSequential Diffusion Language Model(SDLM), which can\nretrofit pre-trainedautoregressive language models(ALMs) at minimal cost.\nSpecifically, SDLM performsdiffusion inferencewithin fixed-sizemask blocks,\nbut dynamically decodes consecutive subsequences based onmodel confidence,\nthereby preserving KV-cache compatibility and improvingrobustnessto varying\nuncertainty and semantics across the sequence. Experiments show that SDLM\nmatches or surpasses strong autoregressive baselines using only 3.5M training\nsamples, while achieving 2.1 higherthroughputthanQwen-2.5. Notably, the\nSDLM-32B model delivers even more pronounced efficiency gains, demonstrating\nthe strong scalability potential of our modeling paradigm. Project page and\ncodes: https://github.com/OpenGVLab/SDLM Diffusion language models (DLMs) have strong theoretical efficiency but are limited by fixed-length decoding and incompatibility with key-value (KV) caches. Block diffusion mitigates these issues, yet still enforces a fixed block size and requires expensive training. We introduce Next Sequence Prediction (NSP), which unifies next-token and next-block prediction, enabling the model to adaptively determine the generation length at each step. When the length is fixed to 1, NSP reduces to standard next-token prediction. Building on NSP, we propose Sequential Diffusion Language Model (SDLM), which can retrofit pre-trained autoregressive language models (ALMs) at minimal cost. Specifically, SDLM performs diffusion inference within fixed-size mask blocks, but dynamically decodes consecutive subsequences based on model confidence, thereby preserving KV-cache compatibility and improving robustness to varying uncertainty and semantics across the sequence. Experiments show that SDLM matches or surpasses strong autoregressive baselines using only 3.5M training samples, while achieving 2.1 higher throughput than Qwen-2.5. Notably, the SDLM-32B model delivers even more pronounced efficiency gains, demonstrating the strong scalability potential of our modeling paradigm. Project page and codes:https://github.com/OpenGVLab/SDLM This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "title": "Sequential Diffusion Language Models",
    "authors": [
      "Yangzhou Liu",
      "Yue Cao",
      "Lijun Wu",
      "Changyao Tian"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/OpenGVLab/SDLM",
    "huggingface_url": "https://huggingface.co/papers/2509.24007",
    "upvote": 45,
    "tags": []
  }
}