{
  "context": "Sparse Query Attention (SQA) reduces computational complexity in Transformer models by decreasing the number of Query heads, leading to significant throughput improvements with minimal impact on model quality. TheTransformer architecture, underpinned by theMulti-Head Attention (MHA)mechanism, has become the de facto standard for state-of-the-art models in\nartificial intelligence. However, the quadratic computational complexity of MHA\nwith respect to sequence length presents a significant barrier to scaling,\nparticularly for applications involving long contexts. Prevailing solutions,\nsuch asMulti-Query Attention (MQA)andGrouped-Query Attention (GQA), have\neffectively addressed the memory bandwidth bottleneck that dominates\nautoregressive inference latency by sharing Key and Value projections. While\nhighly successful, these methods do not reduce the fundamental number offloating-point operations (FLOPs)required for theattention score computation,\nwhich remains a critical bottleneck for training and full-sequence processing.\nThis paper introducesSparse Query Attention (SQA), a novel attention\narchitecture that pursues an alternative and complementary optimization path.\nInstead of reducing Key/Value heads, SQA reduces the number of Query heads.\nThis architectural modification directly decreases the computational complexity\nof the attention mechanism by a factor proportional to the reduction in query\nheads, thereby lowering the overall FLOPs. This work presents the theoretical\nfoundation of SQA, its mathematical formulation, and a family of architectural\nvariants. Empirical benchmarks on long sequences (32k-200k tokens) demonstrate\nthat SQA can achieve significant throughput improvements of up to 3x in\ncomputation-bound scenarios such as model pre-training, fine-tuning, and\nencoder-based tasks, with only a minimal impact on model quality in preliminary\nsmallscale experiments. SQA was discovered serendipitously during the\ndevelopment of the upcomingReactive Transformer architecture, suggesting its\npotential as a powerful tool for building more efficient and scalable models The paper introducing Sparse Query Attention (SQA) for more computationally efficient training and prompt phase. The experiments were small scale, because of limited budget, but we will test it in real scale in future work This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "title": "Sparse Query Attention (SQA): A Computationally Efficient Attention\n  Mechanism with Query Heads Reduction",
    "authors": [
      "Adam Filipek"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/RxAI-dev/rxlm/blob/main/python/src/rxlm/transformers/attention.py",
    "huggingface_url": "https://huggingface.co/papers/2510.01817",
    "upvote": 15,
    "tags": []
  }
}