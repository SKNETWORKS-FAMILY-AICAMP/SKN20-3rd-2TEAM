{
    "context": "LucidFlux, a caption-free UIR framework using a diffusion transformer, achieves robust image restoration through adaptive conditioning and SigLIP features without text prompts. Universal image restoration(UIR) aims to recover images degraded by unknown\nmixtures while preserving semantics -- conditions under which discriminative\nrestorers and UNet-based diffusion priors often oversmooth, hallucinate, or\ndrift. We present LucidFlux, a caption-free UIR framework that adapts a largediffusion transformer(Flux.1) without image captions. LucidFlux introduces a\nlightweightdual-branch conditionerthat injects signals from the degraded\ninput and a lightly restored proxy to respectively anchor geometry and suppress\nartifacts. Then, atimestep- and layer-adaptive modulationschedule is designed\nto route these cues across the backbone's hierarchy, in order to yield\ncoarse-to-fine and context-aware updates that protect the global structure\nwhile recovering texture. After that, to avoid the latency and instability of\ntext prompts or MLLM captions, we enforce caption-free semantic alignment viaSigLIP featuresextracted from the proxy. Ascalable curation pipelinefurther\nfilters large-scale data for structure-rich supervision. Across synthetic and\nin-the-wild benchmarks, LucidFlux consistently outperforms strong open-source\nand commercial baselines, and ablation studies verify the necessity of each\ncomponent. LucidFlux shows that, for largeDiTs, when, where, and what to\ncondition on -- rather than adding parameters or relying on text prompts -- is\nthe governing lever for robust and caption-freeuniversal image restorationin\nthe wild. LucidFlux: Caption-Free Universal Image Restoration with Large DiTsTL;DR: We adapt a large diffusion transformer (Flux.1) for UIR without captions. A dual-branch conditioner (degraded input + lightly restored proxy) anchors geometry and suppresses artifacts, while a timestep & layer-adaptive modulation routes guidance across the DiT for coarse-to-fine updates. SigLIP-based alignment preserves semantics without prompts/VLM latency. A 3-stage data curation pipeline yields structure-rich training sets. Across synthetic & in-the-wild benchmarks, LucidFlux outperforms strong open-/closed-source baselines; ablations confirm each component.Why it matters: Moves UIR beyond UNet/ControlNet sprawl—when, where, and what to condition becomes the lever for robust, caption-free restoration. Project page:https://w2genai-lab.github.io/LucidFluxCode:https://github.com/W2GenAI-Lab/LucidFlux  This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ·Sign uporlog into comment",
    "metadata": {
        "title": "LucidFlux: Caption-Free Universal Image Restoration via a Large-Scale\n  Diffusion Transformer",
        "authors": [],
        "publication_year": 2025,
        "github_url": "https://github.com/W2GenAI-Lab/LucidFlux",
        "huggingface_url": "https://huggingface.co/papers/2509.22414",
        "upvote": 20
    }
}