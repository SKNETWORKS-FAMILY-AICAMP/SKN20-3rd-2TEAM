{
  "context": "Visual Jigsaw, a self-supervised reinforcement learning framework, enhances multimodal large language models' visual understanding through a permutation task without additional annotations or generative components. Reinforcement learningbasedpost-traininghas recently emerged as a powerful\nparadigm for enhancing the alignment and reasoning capabilities of multimodal\nlarge language models (MLLMs). While vision-centricpost-trainingis crucial\nfor enhancing MLLMs' intrinsic understanding of visual signals, currentpost-trainingparadigms are predominantly text-centric, where dense visual\ninputs are only leveraged to extract sparse cues for text-based reasoning.\nThere exist a few approaches in this direction, however, they often still rely\non text as an intermediate mediator or introduce additional visual generative\ndesigns. In this work, we introduce Visual Jigsaw, a genericself-supervisedpost-trainingframework designed to strengthenvisual understandingin MLLMs.\nVisual Jigsaw is formulated as a general ordering task: visual inputs are\npartitioned, shuffled, and the model must reconstruct the visual information by\nproducing the correct permutation in natural language. This naturally aligns\nwithreinforcement learning from verifiable rewards(RLVR), requires no\nadditional visual generative components, and derives its supervisory signal\nautomatically without any annotations. We instantiate Visual Jigsaw across\nthree visual modalities, including images, videos, and 3D data. Extensive\nexperiments demonstrate substantial improvements infine-grained perception,temporal reasoning, and3D spatial understanding. Our findings highlight the\npotential ofself-supervisedvision-centric tasks inpost-trainingMLLMs and\naim to inspire further research on vision-centric pretext designs. Project\nPage: https://penghao-wu.github.io/visual_jigsaw/ Project Page:https://penghao-wu.github.io/visual_jigsaw/Github Repo:https://github.com/penghao-wu/visual_jigsaw This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "title": "Visual Jigsaw Post-Training Improves MLLMs",
    "authors": [
      "Haiwen Diao",
      "Bo Li"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/penghao-wu/visual_jigsaw",
    "huggingface_url": "https://huggingface.co/papers/2509.25190",
    "upvote": 36,
    "tags": []
  }
}