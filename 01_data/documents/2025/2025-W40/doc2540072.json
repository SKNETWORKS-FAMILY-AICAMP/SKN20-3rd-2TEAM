{
    "context": "Post-training techniques like supervised fine-tuning and reinforcement learning lead to the emergence of specialized attention heads that support structured reasoning, with different training regimes affecting their evolution and performance. The remarkable capabilities of modern large reasoning models are largely\nunlocked through post-training techniques such assupervised fine-tuningandreinforcement learning. However, the architectural mechanisms behind such\nimprovements remain largely opaque. In this work, we usecircuit analysisto\ndemonstrate that post-training for complex reasoning sparks the emergence of\nnovel, functionally specializedattention heads. These heads collectively\nsupportstructured reasoningand computation. Our comparative analysis acrossQwen familiesandDeepSeek-distilled modelreveals that these emergent heads\nevolve differently under different training regimes. Distillation and SFT\nfoster a cumulative addition of stable reasoning heads. In contrast, group\nrelative policy optimization operates in a dynamic search mode: relatively fewattention headsare iteratively activated, evaluated, and pruned, with their\nsurvival closely tracking fluctuations in the task reward signal. Furthermore,\nwe find that controllablethink on/off modelsdo not possess dedicated thinking\nheads. Instead, turning off explicit reasoning triggers a broader-but less\nefficient-set of compensatory heads. Through ablation and qualitative analyses,\nwe connect these circuit-level dynamics to a crucial performance trade-off:\nstrengthened heads enable sophisticated problem-solving strategies for\ndifficult problems but can also introduceover-thinking failure modes, such as\ncalculation errors or logical loops on simpler tasks. These findings connect\ncircuit-level dynamics to macro-level performance, identifying an inherent\ntension where complex reasoning comes at the cost of elementary computations.\nMore broadly, our work points to future directions for training policy design,\nemphasizing the need to balance the development of effective reasoning\nstrategies with the assurance of reliable, flawless execution. Modern large reasoning models boost performance via post-training methods like supervised fine-tuning and reinforcement learning—but how these gains arise internally has remained a mystery.In our new work, we peel back the hood using circuit analysis to reveal: (1) Post-training triggers the emergence of specialized attention heads that coordinate to carry out structured reasoning.(2) Different training regimes steer different dynamics: SFT/distillation yield stable, cumulative reasoning heads, while policy optimization leads to iterative activation and pruning.(3) Strong reasoning heads boost advanced problem solving—but risk “overthinking” errors on simpler tasks, revealing a tension between complexity and reliability. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ·Sign uporlog into comment",
    "metadata": {
        "title": "Thinking Sparks!: Emergent Attention Heads in Reasoning Models During\n  Post Training",
        "authors": [],
        "publication_year": 2025,
        "github_url": "",
        "huggingface_url": "https://huggingface.co/papers/2509.25758",
        "upvote": 22
    }
}