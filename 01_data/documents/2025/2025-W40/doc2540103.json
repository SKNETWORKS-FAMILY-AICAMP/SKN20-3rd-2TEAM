{
    "context": "A unified Regression Language Model (RLM) predicts numeric outcomes of code executions, including memory footprint, latency, and neural network performance, across multiple languages and hardware platforms. We study code-to-metric regression: predicting numeric outcomes of code\nexecutions, a challenging task due to the open-ended nature of programming\nlanguages. While prior methods have resorted to heavy and domain-specific\nfeature engineering, we show that a single unified Regression Language Model\n(RLM) can simultaneously predict directly from text, (i) the memory footprint\nof code across multiple high-level languages such as Python and C++, (ii) the\nlatency of Triton GPU kernels, and (iii) the accuracy and speed of trained\nneural networks represented in ONNX. In particular, a relatively small 300M\nparameter RLM initialized fromT5Gemma, obtains > 0.9Spearman-rankon\ncompetitive programming submissions from APPS, and a single unified model\nachieves > 0.5 averageSpearman-rankacross 17 separate languages from CodeNet.\nFurthermore, the RLM can obtain the highest averageKendall-Tauof 0.46 on five\nclassicNAS design spacespreviously dominated bygraph neural networks, and\nsimultaneously predict architecture latencies on numerous hardware platforms. We study code-to-metric regression: predicting numeric outcomes of code executions, a challenging task due to the open-ended nature of programming languages. While prior methods have resorted to heavy and domain-specific feature engineering, we show that a single unified Regression Language Model (RLM) can simultaneously predict directly from text, (i) the memory footprint of code across multiple high-level languages such as Python and C++, (ii) the latency of Triton GPU kernels, and (iii) the accuracy and speed of trained neural networks represented in ONNX. In particular, a relatively small 300M parameter RLM initialized from T5Gemma, obtains > 0.9 Spearman-rank on competitive programming submissions from APPS, and a single unified model achieves > 0.5 average Spearman-rank across 17 separate languages from CodeNet. Furthermore, the RLM can obtain the highest average Kendall-Tau of 0.46 on five classic NAS design spaces previously dominated by graph neural networks, and simultaneously predict architecture latencies on numerous hardware platforms. Github repo:https://github.com/google-deepmind/regress-lmDataset:https://huggingface.co/datasets/akhauriyash/Code-Regression This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
    "metadata": {
        "title": "Regression Language Models for Code",
        "authors": [
            "Yash Akhauri"
        ],
        "publication_year": 2025,
        "github_url": "https://github.com/google-deepmind/regress-lm/tree/main",
        "huggingface_url": "https://huggingface.co/papers/2509.26476",
        "upvote": 16
    }
}