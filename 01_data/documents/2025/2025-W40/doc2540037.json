{
  "context": "RLP, an information-driven reinforcement pretraining objective, enhances reasoning models by integrating exploration into pretraining, leading to significant performance improvements across various benchmarks. The dominant paradigm for training large reasoning models starts withpre-trainingusingnext-token predictionloss on vast amounts of data.Reinforcement learning, while powerful in scaling reasoning, is introduced only\nas the very last phase of post-training, preceded by supervised fine-tuning.\nWhile dominant, is this an optimal way of training? In this paper, we present\nRLP, an information-driven reinforcement pretraining objective, that brings the\ncore spirit ofreinforcement learning-- exploration -- to the last phase of\npretraining. The key idea is to treatchain-of-thoughtas an exploratory\naction, with rewards computed based on theinformation gainit provides for\npredicting future tokens. This training objective essentially encourages the\nmodel to think for itself before predicting what comes next, thus teaching an\nindependent thinking behavior earlier in the pretraining. More concretely, the\nreward signal measures the increase inlog-likelihoodof the next token when\nconditioning on both context and a sampled reasoning chain, compared to\nconditioning on context alone. This approach yields a verifier-free dense\nreward signal, allowing for efficient training for the full document stream\nduring pretraining. Specifically, RLP reframesreinforcement learningfor\nreasoning as a pretraining objective on ordinary text, bridging the gap betweennext-token predictionand the emergence of usefulchain-of-thoughtreasoning.\nPretraining with RLP onQwen3-1.7B-Baselifts the overall average across an\neight-benchmark math-and-science suite by 19%. With identical post-training,\nthe gains compound, with the largest improvements on reasoning-heavy tasks such\nasAIME25andMMLU-Pro. Applying RLP to the hybridNemotron-Nano-12B-v2increases the overall average from 42.81% to 61.32% and raises the average onscientific reasoningby 23%, demonstrating scalability across architectures and\nmodel sizes. Motivation.Today’s LLMs learn almost everything with next‑token prediction, then onlyafterpretraining try to teach themselves to reason via SFT and RLHF/RLVR. That split means the base model never practices “thinking before predicting” while it’s learning from raw text. We asked: what if exploration happened during pretraining itself, and we rewarded thoughts that genuinely help predict the next token—without any task‑specific verifiers? What we built.RLPis a pretraining objective that treats a short chain‑of‑thought (CoT) as an action takenbeforepredicting each token. For each position, the model samples a brief thought, then scores the true next token twice: Therewardis the increase in the model’s log‑likelihood of the observed next tokenwiththe thought compared towithoutit. We updateonly the thought tokensusing a clipped, group‑relative advantage objective; wedo notbackprop through the reward scores. The signal isdense(every position gets a reward),verifier‑free(works on ordinary text), andscalesto full documents (no entropy filtering or curated checkers). Why earlier approaches (e.g., RPT) fell short.Prior “reinforcement pretraining” with prefix‑matching rewards (RPT) usually relies on: Key results. Qwen3‑1.7B, base pretraining: After identical post‑training (SFT + RLVR) for all models: Scaling to a 12B hybrid (NeMo‑12B): Domain breadth and data practicality: In short,RLP rewards “useful thoughts” during pretraining. The signal is simple, dense, verifier‑free, and compatible with standard pipelines—yielding stronger base models whose gainssurvive and compoundafter alignment. Paper:https://arxiv.org/pdf/2510.01265Code:https://github.com/NVlabs/RLP The \"code\" has only a license and a PDF in there??Also any reason to not put the artifacts/models on Hugging Face? This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Great ideas in the paper. I have one question about the checkpoint: you start fron qwen base, which has no reasoning or instruction tuning, right? Additionally it seems like you have no supervised step to illicit initial reasoning. I am wondering how the model learns how to use the  tokens. What am I missing? Did you use any elaborate prompt templates? If so, how do they look like? I couldn't find any info on that in the paper ·Sign uporlog into comment",
  "metadata": {
    "title": "RLP: Reinforcement as a Pretraining Objective",
    "authors": [
      "Ali Hatamizadeh",
      "Syeda Nahida Akter"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/NVlabs/RLP",
    "huggingface_url": "https://huggingface.co/papers/2510.01265",
    "upvote": 40,
    "tags": []
  }
}