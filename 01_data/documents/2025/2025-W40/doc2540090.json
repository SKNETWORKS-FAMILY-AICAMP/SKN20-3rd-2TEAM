{
    "context": "BroRL enhances reinforcement learning by increasing rollouts per example, overcoming performance plateaus and achieving state-of-the-art results in large language models. Reinforcement Learning with Verifiable Rewards(RLVR) has emerged as a key\ningredient for unlocking complex reasoning capabilities in large language\nmodels. Recent workProRLhas shown promise in scaling RL by increasing the\nnumber of training steps. However, performance plateaus after thousands of\nsteps, with clear diminishing returns from allocating more computation to\nadditional training. In this work, we investigate a complementary paradigm for\nscaling RL, BroR-Lincreasing the number ofrolloutsper example to hundreds to\nexhaustively Broadenexploration, which yields continuous performance gains\nbeyond the saturation point observed inProRLwhen scaling the number of\ntraining steps. Our approach is motivated by amass balance equationanalysis\nallowing us to characterize the rate of change inprobability massfor correct\nandincorrect tokensduring the reinforcement process. We show that under aone-step RLassumption, sampled rollout tokens always contribute tocorrect-mass expansion, while unsampled tokens outsiderolloutsmay lead to\ngains or losses depending on their distribution and thenet reward balance.\nImportantly, as the number ofrolloutsper example N increases, the effect ofunsampled termsdiminishes, ensuring overallcorrect-mass expansion. To\nvalidate our theoretical analysis, we conduct simulations under more relaxed\nconditions and find that a sufficiently large rollout size N-corresponding to\nampleexploration-guarantees an increase in theprobability massof all correct\ntokens. Empirically, BroRL revives models saturated after 3KProRLtraining\nsteps and demonstrates robust, continuous improvement, achieving\nstate-of-the-art results for the 1.5B model across diverse benchmarks. BroRL: Scaling Reinforcement Learning via Broadened Exploration This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
    "metadata": {
        "title": "BroRL: Scaling Reinforcement Learning via Broadened Exploration",
        "authors": [],
        "publication_year": 2025,
        "github_url": "",
        "huggingface_url": "https://huggingface.co/papers/2510.01180",
        "upvote": 18
    }
}