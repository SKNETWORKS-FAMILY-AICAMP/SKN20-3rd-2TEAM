{
    "context": "Implicit Multimodal Guidance (IMG) enhances multimodal alignment between diffusion-generated images and prompts without additional data or editing, outperforming existing methods. Ensuring precisemultimodal alignmentbetweendiffusion-generated imagesand\ninput prompts has been a long-standing challenge. Earlier works finetunediffusion weightusing high-qualitypreference data, which tends to be limited\nand difficult to scale up. Recentediting-based methodsfurther refine local\nregions of generated images but may compromise overall image quality. In this\nwork, we proposeImplicit Multimodal Guidance(IMG), a novel\nre-generation-basedmultimodal alignmentframework that requires no extra data\nor editing operations. Specifically, given a generated image and its prompt,\nIMG a) utilizes amultimodal large language model(MLLM) to identify\nmisalignments; b) introduces anImplicit Alignerthat manipulates diffusion\nconditioning features to reduce misalignments and enable re-generation; and c)\nformulates the re-alignment goal into a trainable objective, namely Iteratively\nUpdated Preference Objective. Extensive qualitative and quantitative\nevaluations onSDXL,SDXL-DPO, andFLUXshow that IMG outperforms existing\nalignment methods. Furthermore, IMG acts as a flexible plug-and-play adapter,\nseamlessly enhancing prior finetuning-based alignment methods. Our code will be\navailable at https://github.com/SHI-Labs/IMG-Multimodal-Diffusion-Alignment. Ensuring precise multimodal alignment between diffusion-generated images and input prompts has been a long-standing challenge. Earlier works finetune diffusion weight using high-quality preference data, which tends to be limited and difficult to scale up. Recent editing-based methods further refine local regions of generated images but may compromise overall image quality. In this work, we propose Implicit Multimodal Guidance (IMG), a novel re-generation-based multimodal alignment framework that requires no extra data or editing operations. Specifically, given a generated image and its prompt, IMG a) utilizes a multimodal large language model (MLLM) to identify misalignments; b) introduces an Implicit Aligner that manipulates diffusion conditioning features to reduce misalignments and enable re-generation; and c) formulates the re-alignment goal into a trainable objective, namely Iteratively Updated Preference Objective. Extensive qualitative and quantitative evaluations on SDXL, SDXL-DPO, and FLUX show that IMG outperforms existing alignment methods. Furthermore, IMG acts as a flexible plug-and-play adapter, seamlessly enhancing prior finetuning-based alignment methods. Our code will be available athttps://github.com/SHI-Labs/IMG-Multimodal-Diffusion-Alignment. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
    "metadata": {
        "title": "IMG: Calibrating Diffusion Models via Implicit Multimodal Guidance",
        "authors": [
            "Jiayi Guo"
        ],
        "publication_year": 2025,
        "github_url": "https://github.com/SHI-Labs/IMG-Multimodal-Diffusion-Alignment",
        "huggingface_url": "https://huggingface.co/papers/2509.26231",
        "upvote": 17
    }
}