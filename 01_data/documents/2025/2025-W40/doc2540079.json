{
  "context": "Critique Reinforcement Learning (CRL) enhances LLMs by teaching them to generate critiques, leading to improved performance on code generation and logic reasoning tasks compared to standard RL. Reinforcement Learning (RL)has emerged as a popular training paradigm,\nparticularly when paired with reasoning models. While effective, it primarily\nfocuses on generating responses and lacks mechanisms to explicitly foster\ncritique or reflection. Several recent studies, likeCritique-Fine-Tuning (CFT)andCritique-Guided-Distillation (CGD)have shown the benefits of explicitly\nteaching LLMs how to critique. Motivated by them, we propose Critique\nReinforcement Learning (CRL), where the model is tasked with generating a\ncritique for a given (question, solution) pair. The reward is determined solely\nby whether the final judgment label c in {True, False}\nof the generated critique aligns with the ground-truth judgment c^*. Building\non this point, we introduceCritique-Coder, which is trained on a\nhybrid of RL and CRL by substituting 20\\% of the standard RL data with CRL\ndata. We fine-tune multiple models (Critique-Coder) and evaluate them\non different benchmarks to show their advantages over RL-only models. We show\nthatCritique-Coderconsistently outperforms RL-only baselines on all\nthe evaluated benchmarks. Notably, ourCritique-Coder-8B can reach\nover 60\\% onLiveCodeBench(v5), outperforming other reasoning models likeDeepCoder-14BandGPT-o1. Beyond code generation,Critique-Coderalso\ndemonstrates enhanced general reasoning abilities, as evidenced by its better\nperformance on logic reasoning tasks from theBBEH dataset. This indicates that\nthe application of CRL on coding datasets enhances general reasoning and\ncritique abilities, which are transferable across a broad range of tasks.\nHence, we believe that CRL works as a great complement to standard RL for LLM\nreasoning. Reinforcement Learning (RL) has emerged as a popular training paradigm, particularly when paired with reasoning models. While effective, it primarily focuses on generating responses and lacks mechanisms to explicitly foster critique or reflection. Several recent studies, like Critique-Fine-Tuning (CFT) and Critique-Guided-Distillation (CGD) have shown the benefits of explicitly teaching LLMs how to critique. Motivated by them, we propose Critique Reinforcement Learning (CRL), where the model is tasked with generating a critique for a given (question, solution) pair. The reward is determined solely by whether the final judgment label câˆˆ{ğšƒğš›ğšğš,ğ™µğšŠğš•ğšœğš} of the generated critique aligns with the ground-truth judgment câˆ—. Building on this point, we introduce \\textsc{Critique-Coder}, which is trained on a hybrid of RL and CRL by substituting 20% of the standard RL data with CRL data. We fine-tune multiple models (\\textsc{Critique-Coder}) and evaluate them on different benchmarks to show their advantages over RL-only models. We show that \\textsc{Critique-Coder} consistently outperforms RL-only baselines on all the evaluated benchmarks. Notably, our \\textsc{Critique-Coder-8B} can reach over 60% on LiveCodeBench (v5), outperforming other reasoning models like DeepCoder-14B and GPT-o1. Beyond code generation, \\textsc{Critique-Coder} also demonstrates enhanced general reasoning abilities, as evidenced by its better performance on logic reasoning tasks from the BBEH dataset. This indicates that the application of CRL on coding datasets enhances general reasoning and critique abilities, which are transferable across a broad range of tasks. Hence, we believe that CRL works as a great complement to standard RL for LLM reasoning.  Â·Sign uporlog into comment",
  "metadata": {
    "title": "Critique-Coder: Enhancing Coder Models by Critique Reinforcement\n  Learning",
    "authors": [
      "Dongfu Jiang"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2509.22824",
    "upvote": 20,
    "tags": []
  }
}