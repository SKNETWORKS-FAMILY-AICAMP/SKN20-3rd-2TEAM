{
    "context": "CoSpaDi, a training-free compression framework, uses structured sparse dictionary learning to compress large language models with better accuracy and efficiency compared to low-rank methods. Post-training compression of large language models (LLMs) largely relies onlow-rank weight approximation, which represents each column of a weight matrix\nin a shared low-dimensional subspace. While this is a computationally efficient\nstrategy, the imposed structural constraint is rigid and can lead to a\nnoticeable modelaccuracydrop. In this work, we proposeCoSpaDi(Compression\nvia Sparse Dictionary Learning), a novel training-free compression framework\nthat replaces low-rank decomposition with a more flexible structured sparse\nfactorization in which each weight matrix is represented with a dense\ndictionary and acolumn-sparse coefficient matrix. This formulation enables aunion-of-subspaces representation: different columns of the original weight\nmatrix are approximated in distinct subspaces spanned by adaptively selecteddictionary atoms, offering greater expressiveness than a single invariant\nbasis. Crucially,CoSpaDileverages a smallcalibration datasetto optimize the\nfactorization such that the output activations of compressed projection layers\nclosely match those of the original ones, thereby minimizing functional\nreconstruction error rather than mere weight approximation. This data-aware\nstrategy preserves better model fidelity without any fine-tuning under\nreasonable compression ratios. Moreover, the resulting structured sparsity\nallows efficientsparse-dense matrix multiplicationand is compatible withpost-training quantizationfor further memory and latency gains. We evaluateCoSpaDiacross multiple Llama and Qwen models under per-layer and per-group\nsettings at 20-50\\% compression ratios, demonstrating consistent superiority\nover state-of-the-art data-aware low-rank methods both inaccuracyandperplexity. Our results establish structured sparse dictionary learning as a\npowerful alternative to conventional low-rank approaches for efficient LLM\ndeployment. We release a new compression method for LLMs, stay tuned the code will be available soon ! This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
    "metadata": {
        "title": "COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary\n  Learning",
        "authors": [
            "Ammar Ali"
        ],
        "publication_year": 2025,
        "github_url": "",
        "huggingface_url": "https://huggingface.co/papers/2509.22075",
        "upvote": 21
    }
}