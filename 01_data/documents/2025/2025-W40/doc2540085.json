{
    "context": "Muon optimizer outperforms Adam in training LLMs by effectively optimizing associative memory parameters and balancing learning across classes in heavy-tailed data. TheMuon optimizeris consistently faster thanAdamin training Large\nLanguage Models (LLMs), yet the mechanism underlying its success remains\nunclear. This paper demystifies this mechanism through the lens of associative\nmemory. By ablating thetransformer componentsoptimized by Muon, we reveal\nthat theassociative memoryparameters of LLMs, namely the Value and Output\n(VO) attention weights andFeed-Forward Networks (FFNs), are the primary\ncontributors to Muon's superiority. Motivated by thisassociative memoryview,\nwe then explain Muon's superiority on real-world corpora, which are\nintrinsically heavy-tailed: a few classes (tail classes) appear far less\nfrequently than others. The superiority is explained through two key\nproperties: (i) its update rule consistently yields a more isotropic singular\nspectrum thanAdam; and as a result, (ii) onheavy-tailed data, it optimizestail classesmore effectively thanAdam. Beyond empirical evidence, we\ntheoretically confirm these findings by analyzing a one-layer associative\nmemory model underclass-imbalanced data. We prove that Muon consistently\nachievesbalanced learningacross classes regardless of feature embeddings,\nwhereasAdamcan induce large disparities in learning errors depending on\nembedding properties. In summary, our empirical observations and theoretical\nanalyses reveal Muon's core advantage: its update rule aligns with theouter-product structureoflinear associative memories, enabling more balanced\nand effective learning oftail classesin heavy-tailed distributions thanAdam. The Muon optimizer is consistently faster than Adam in training Large Language Models (LLMs), yet the mechanism underlying its success remains unclear. This paper demystifies this mechanism through the lens of associative memory. By ablating the transformer components optimized by Muon, we reveal that the associative memory parameters of LLMs, namely the Value and Output (VO) attention weights and Feed-Forward Networks (FFNs), are the primary contributors to Muon’s superiority. Motivated by this associative memory view, we then explain Muon’s superiority on real-world corpora, which are intrinsically heavy-tailed: a few classes (tail classes) appear far less frequently than others. The superiority is explained through two key properties: (i) its update rule consistently yields a more isotropic singular spectrum than Adam; and as a result, (ii) on heavy-tailed data, it optimizes tail classes more effectively than Adam. Beyond empirical evidence, we theoretically confirm these findings by analyzing a one-layer associative memory model under class-imbalanced data. We prove that Muon consistently achieves balanced learning across classes regardless of feature embeddings, whereas Adam can induce large disparities in learning errors depending on embedding properties. In summary, our empirical observations and theoretical analyses reveal Muon’s core advantage: its update rule aligns with the outer-product structure of linear associative memories, enabling more balanced and effective learning of tail classes in heavy-tailed distributions than Adam. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ·Sign uporlog into comment",
    "metadata": {
        "title": "Muon Outperforms Adam in Tail-End Associative Memory Learning",
        "authors": [
            "Zhuoran Yang"
        ],
        "publication_year": 2025,
        "github_url": "",
        "huggingface_url": "https://huggingface.co/papers/2509.26030",
        "upvote": 19
    }
}