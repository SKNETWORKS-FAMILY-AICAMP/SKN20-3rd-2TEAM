{
    "context": "A new reward model, trained with a large-scale human preference dataset, improves instruction-guided image editing by selecting high-quality training data and achieving state-of-the-art performance on benchmarks. Recently, we have witnessed great progress in image editing with natural\nlanguage instructions. Several closed-source models like GPT-Image-1, Seedream,\nand Google-Nano-Banana have shown highly promising progress. However, the\nopen-source models are still lagging. The main bottleneck is the lack of a\nreliablereward modelto scale up high-quality synthetic training data. To\naddress this critical bottleneck, we built \\mname, trained with our new\nlarge-scalehuman preference dataset, meticulously annotated by trained experts\nfollowing a rigorous protocol containing over 200K preference pairs. \\mname\ndemonstrates superior alignment with human preferences in instruction-guided\nimage editing tasks. Experiments show that \\mname achieves state-of-the-art\nhuman correlation on established benchmarks such asGenAI-Bench,AURORA-Bench,ImagenHub, and our new \\benchname, outperforming a wide range of VLM-as-judge\nmodels. Furthermore, we use \\mname to select a high-quality subset from the\nexisting noisy ShareGPT-4o-Image dataset. We train Step1X-Edit on the selected\nsubset, which shows significant improvement over training on the full set. This\ndemonstrates \\mname's ability to serve as areward modelto scale up\nhigh-quality training data for image editing. Furthermore, its strong alignment\nsuggests potential for advanced applications likereinforcement learning-basedpost-trainingandtest-time scalingof image editing models. \\mname with its\ntraining dataset will be released to help the community build more high-quality\nimage editing training datasets. Recently, we have witnessed great progress in image editing with natural language instructions. Several closed-source models like GPT-Image-1, Seedream, and Google-Nano-Banana have shown highly promising progress. However, the open-source models are still lagging. The main bottleneck is the lack of a reliable reward model to scale up high-quality synthetic training data. To address this critical bottleneck, we built \\mname, trained with our new large-scale human preference dataset, meticulously annotated by trained experts following a rigorous protocol containing over 200K preference pairs. \\mname demonstrates superior alignment with human preferences in instruction-guided image editing tasks. Experiments show that \\mname achieves state-of-the-art human correlation on established benchmarks such as GenAI-Bench, AURORA-Bench, ImagenHub, and our new \\benchname, outperforming a wide range of VLM-as-judge models. Furthermore, we use \\mname to select a high-quality subset from the existing noisy ShareGPT-4o-Image dataset. We train Step1X-Edit on the selected subset, which shows significant improvement over training on the full set. This demonstrates \\mname's ability to serve as a reward model to scale up high-quality training data for image editing. Furthermore, its strong alignment suggests potential for advanced applications like reinforcement learning-based post-training and test-time scaling of image editing models. \\mname with its training dataset will be released to help the community build more high-quality image editing training datasets. We are honored to contribute to EditReward, a human-aligned reward model for instruction-guided image editing. üéâ Built on a large-scale expert-annotated dataset and paired with our new benchmark, we hope our data, model, and benchmark can bring valuable insights to both academia and industry, and help advance the future of image editing research and applications.üñºÔ∏è This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ¬∑Sign uporlog into comment",
    "metadata": {
        "title": "EditReward: A Human-Aligned Reward Model for Instruction-Guided Image\n  Editing",
        "authors": [
            "Keming Wu",
            "Sicong Jiang",
            "Max Ku",
            "Ping Nie",
            "Minghao Liu"
        ],
        "publication_year": 2025,
        "github_url": "https://github.com/TIGER-AI-Lab/EditReward",
        "huggingface_url": "https://huggingface.co/papers/2509.26346",
        "upvote": 18
    }
}