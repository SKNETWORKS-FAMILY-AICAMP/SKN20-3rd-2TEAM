{
  "context": "BDH, a biologically inspired Large Language Model, combines scale-free network architecture with Hebbian learning to achieve Transformer-like performance while maintaining interpretability. The relationship between computing systems and the brain has served as\nmotivation for pioneering theoreticians since John von Neumann and Alan Turing.\nUniform, scale-free biological networks, such as the brain, have powerful\nproperties, including generalizing over time, which is the main barrier for\nMachine Learning on the path to Universal Reasoning Models.\n  We introduce `Dragon Hatchling' (BDH), a newLarge Language Modelarchitecture based on a scale-free biologically inspired network of \\n\nlocally-interacting neuron particles. BDH couples strong theoretical\nfoundations and inherentinterpretabilitywithout sacrificing Transformer-like\nperformance.\n  BDH is a practical, performant state-of-the-art attention-based state space\nsequence learning architecture. In addition to being agraph model, BDH admits\naGPU-friendlyformulation. It exhibitsTransformer-like scaling laws:\nempirically BDH rivals GPT2 performance on language and translation tasks, at\nthe same number of parameters (10M to 1B), for the same training data.\n  BDH can be represented as a brain model. The working memory of BDH during\ninference entirely relies onsynaptic plasticitywithHebbian learningusingspiking neurons. We confirm empirically that specific, individual synapses\nstrengthen connection whenever BDH hears or reasons about a specific concept\nwhile processing language inputs. The neuron interaction network of BDH is a\ngraph of high modularity with heavy-tailed degree distribution. The BDH model\nis biologically plausible, explaining one possible mechanism which human\nneurons could use to achieve speech.\n  BDH is designed forinterpretability. Activation vectors of BDH are sparse\nand positive. We demonstratemonosemanticityin BDH on language tasks.Interpretabilityof state, which goes beyondinterpretabilityof neurons and\nmodel parameters, is an inherent feature of the BDH architecture. Brains are a network of neurons, with very specific connection patterns. Transformers use dense matrix multiplications which hide this network structure. What happens when an LLM is designed to be a brain-like, scale-free information transportation network? This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ‚ÄúExcited to explore the BDH architecture as part of the Pathway hackathon!‚Äù Can we train tts models using this architecture? I'm not talking about the fancy new llm-based ones, but the smaller ones like Piper Great!! So you removed softmax and added a relu gating after attention? How is this anything new? The modifications aren't even good. They degrade standard attention performance. @xz259There are about 5 main differences, which should not be tried in isolation - or they will indeed not work (and cannot work in isolation, as explained in the theory part of the paper). See also table in comments below. From the authors: For readers looking for a synthetic (less rigorous) comparison to the Transformer, here is a summary table of differences.  Fascinating. I'll be studying this intensely over the next week. High dimensionality and connectivity are indeed important links. I explored this model:https://github.com/takzen/BDH-Official    I may have missed it, but it seems like you only used letter-level tokenization when testing BDH. Did you test using word-level tokenization? You're spot on ‚Äì for this whole experiment, we stuck strictly to character-level tokenization.  Honestly, it was a deliberate choice. We wanted to keep things as simple as possible to really isolate and test the BDH architecture itself. By feeding it raw characters, we gave it the toughest possible challenge: it had to figure out everything from scratch, including what a 'word' even is. The fact that it managed to learn spelling, grammar, and style just from individual letters is, for us, the most powerful proof that the architecture is doing something special. Testing it with a proper BPE tokenizer is definitely the next logical step, and I'd be really curious to see how it performs then. Thanks again for the great question! I find the interpretability the most intriguing although I think with techniques like Anthropic's autoencoder that seems to recover concepts that becomes a little less impressive. I created an AI overview of the paper in podcast form if you are looking for something on your commute.https://open.spotify.com/episode/63nni2dlHaVH3qntKD9qy5?si=b818df90a9694135 Intriguing workÔºÅAfter reading abstract, I can't help waiting to see how to use Hebbian rule, spiking neurons, or excitatory and inhibitory circuits to build NN and train. It seems that when compressing the BDH by encoder, the BDH-GPU evolved toward  transformers.  I'm not sure if my understanding is correct, is that each local graph in the BDH is seen as the particals which are compressed as a node in BDH-GPU?BTW, if training BDH with Hebbian rule, is the performance also comparable?Looking forward to seeing the graph-based BDH's code! Our work speaks mostly of short-term reasoning and memory dynamics (\"fast weights\"), less directly about what pre-training method to use to set the \"slow weights\". For this reason, we recommend first pretraining a BDH-GPU model the way it is trained now, then, analyzing its activation vectors, parameter matrices, and attention state $\\sigma$, using equations (6), (7), (8) from the paper to directly see the relevant graph characteristics. The Hebbian rule is then visible as a short-term attention mechanism.We plan to provide some Jupyter notebooks in the Github repo - representing the experiments on synapses performed in the paper.Beyond this, exploring fast-to-slow memory transfer methods for BDH is a really nice research area. arXiv explained breakdown of this paper üëâhttps://arxivexplained.com/papers/the-dragon-hatchling-the-missing-link-between-the-transformer-and-models-of-the-brain I'm excited to share the final results of my research adapting Pathway's \"Baby Dragon Hatchling\" (BDH) architecture for computer vision. The outcome exceeded all expectations.My key discovery was an optimized configuration (E-BDH) that was ~35x faster to train than my initial implementation. To test its full potential, I ran a comprehensive 30-epoch benchmark on CIFAR-10.The results are phenomenal:My E-BDH model achieved a final test accuracy of 79.54%!To ensure a fair comparison, I also trained a standard ViT-Tiny baseline for 30 epochs under identical conditions. Even after the full training run, the E-BDH architecture maintained its superiority: E-BDH (3.2M params): 79.54%ViT-Tiny (5.7M params): ~74% This demonstrates a persistent ~5.5 percentage point lead, proving that the E-BDH model's advantage is not just about convergence speed, but about reaching a higher performance ceiling with fewer parameters.The full framework, code, and results are open-source on my GitHub:https://github.com/takzen/vision-bdh This is a path breaking paper & architecture. Great work!! Fascinating work, thanks. I‚Äôm currently studying the official implementation on GitHub but it doesn‚Äôt seem to closely correspond to terms in Equation 8 and Figure 6. Do I fail to see it? Is it result of code optimization? Or perhaps it‚Äôs an equivalent but different variant of the model? As you suggest - all of the variants/flavors should lead to the same qualitative conclusions in terms of scaling, interpretability, etc. The code listing provided on the last page of the paper should correspond to Eq. 8 / Fig 6 in the way described in Appendix E, i.e., up to placement of layer norms, while also describing a mechanism of heads. The github code is a slightly more efficient (optimized) variant of the code from Appendix E, which makes a good starting point for forks. ¬∑Sign uporlog into comment",
  "metadata": {
    "title": "The Dragon Hatchling: The Missing Link between the Transformer and\n  Models of the Brain",
    "authors": [
      "Adrian Kosowski",
      "Przemys≈Çaw Uzna≈Ñski",
      "Jan Chorowski",
      "Micha≈Ç Bartoszkiewicz"
    ],
    "publication_year": 2025,
    "github_url": "https://github.com/pathwaycom/bdh",
    "huggingface_url": "https://huggingface.co/papers/2509.26507",
    "upvote": 535,
    "tags": []
  }
}