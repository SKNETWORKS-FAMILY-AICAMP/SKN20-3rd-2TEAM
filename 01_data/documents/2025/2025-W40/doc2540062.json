{
    "context": "Restoring fine-tuning to a breadth-first pipeline with mini-batch optimization and localized tuning parameters improves its effectiveness for model editing, outperforming state-of-the-art methods. Fine-tuning, a foundational method for adapting large language models, has\nlong been considered ineffective formodel editing. Here, we challenge this\nbelief, arguing that the reported failure arises not from the inherent\nlimitation offine-tuningitself, but from adapting it to the sequential nature\nof the editing task, a single-passdepth-first pipelinethat optimizes each\nsample to convergence before moving on. While intuitive, this depth-first\npipeline coupled with sample-wise updating over-optimizes each edit and induces\ninterference across edits. Our controlled experiments reveal that simply\nrestoringfine-tuningto the standard breadth-first (i.e., epoch-based)\npipeline withmini-batch optimizationsubstantially improves its effectiveness\nformodel editing. Moreover,fine-tuningin editing also suffers from\nsuboptimal tuning parameter locations inherited from prior methods. Through\nsystematic analysis of tuning locations, we deriveLocFT-BF, a simple and\neffectivelocalized editingmethod built on the restoredfine-tuningframework.\nExtensive experiments across diverseLLMsand datasets demonstrate thatLocFT-BFoutperforms state-of-the-art methods by large margins. Notably, to our\nknowledge, it is the first to sustain 100K edits and 72B-parameter models,10 x\nbeyond prior practice, without sacrificing general capabilities. By clarifying\na long-standing misconception and introducing a principled localized tuning\nstrategy, we advancefine-tuningfrom an underestimated baseline to a leading\nmethod formodel editing, establishing a solid foundation for future research. This paper revives fine-tuning for model editing, proving that by simply fixing its flawed implementation and applying a localized parameter update strategy, the simplest method becomes state-of-the-art, handling an unprecedented 100K sequential edits on models up to 72B parameters. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
    "metadata": {
        "title": "Fine-tuning Done Right in Model Editing",
        "authors": [],
        "publication_year": 2025,
        "github_url": "",
        "huggingface_url": "https://huggingface.co/papers/2509.22072",
        "upvote": 28
    }
}