{
    "context": "SparseD is a novel sparse attention method for diffusion language models that addresses the high inference latency by pre-computing head-specific sparse patterns and switching to sparse attention in later denoising steps. Whilediffusion language models(DLMs) offer a promising alternative toautoregressive models(ARs), existing open-source DLMs suffer from high\ninference latency. This bottleneck is mainly due to theattention's quadratic\ncomplexity with respect tocontext lengthin computing allquery-key pairs.\nIntuitively, to reduce this complexity, a natural strategy is to restrictattentionto sparse patterns that retain only the most relevant connections.\nSuch approaches are well-established in ARs, whereattentionfollows fixed and\nclearly defined sparse patterns. However, in DLMs, we observe distinct sparsity\nbehaviors: (1)attentionpatterns vary across heads, (2)attentionpatterns in\neach head remain highly similar acrossdenoising steps, and (3) early denoising\nsteps are critical for generation. These findings rendersparse attentionmethods designed for ARs largely incompatible with DLMs, as they fail to\ncapturehead-specific structuresand risk degrading generation when applied in\nearlydenoising steps. To address these challenges, we proposeSparseD, a novelsparse attentionmethod for DLMs. Leveraging the observations,SparseDonly\nrequires pre-computing head-specific sparse patterns one time, and reuses them\nacross all steps. This prevents recomputing sparse patterns at each denoising\nstep. Meanwhile,SparseDuses fullattentionin the early steps, then switches\ntosparse attentionlater to maintaingeneration quality. Together, these\nestablishSparseDas a practical and efficient solution for deploying DLMs in\nlong-context applications. Experimental results demonstrate thatSparseDachieves lossless acceleration, delivering up to 1.50times speedup overFlashAttentionat a 64kcontext lengthwith 1,024denoising steps. ðŸ“„ Arxiv:https://arxiv.org/abs/2509.24014ðŸ’» Code:https://github.com/INV-WZQ/SparseDðŸš€ SparseD is a novel sparse attention method for diffusion language models (DLMs), delivering near-lossless acceleration in performance. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
    "metadata": {
        "title": "SparseD: Sparse Attention for Diffusion Language Models",
        "authors": [
            "Zeqing Wang",
            "Xingyi Yang"
        ],
        "publication_year": 2025,
        "github_url": "https://github.com/INV-WZQ/SparseD",
        "huggingface_url": "https://huggingface.co/papers/2509.24014",
        "upvote": 30
    }
}