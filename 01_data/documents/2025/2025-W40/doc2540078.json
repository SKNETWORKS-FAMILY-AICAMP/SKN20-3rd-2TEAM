{
    "context": "Reinforcement learning enables large language models to acquire new compositional skills by combining existing ones, which transfer to different tasks and improve reasoning behaviors. Does RL teach LLMs genuinely new skills, or does it merely activate existing\nones? This question lies at the core of ongoing debates about the role of RL in\nLLM post-training. On one side, strong empirical results can be achieved with\nRL even without preceding supervised finetuning; on the other, critics argue\nthat RL contributes little beyond reweighting existing reasoning strategies.\nThis work provides concrete evidence that LLMs can acquire genuinely new skills\nduring RL by composing existing ones, mirroring one of the central mechanisms\nby which humans acquire new cognitive skills. To mitigate data contamination\nand other confounding factors, and to allow precise control over task\ncomplexity, we develop a synthetic framework for our investigation.\nSpecifically, we define a skill as the ability to infer the output of a string\ntransformation function f(x) given x. When an LLM has already learned f and g\nprior to RL, our experiments reveal that RL enables it to learn unseen\ncompositions of them h(x)=g(f(x)). Further, this compositional ability\ngeneralizes to more difficult problems such as compositions of >2 functions\nunseen during RL training. Surprisingly, our experiments show that\ncompositional skill acquired on a source task transfers to a different target\ntask. This transfer happens even without compositional training on the target,\nrequiring only prior knowledge of the target's atomic skills. Our qualitative\nanalysis shows that RL fundamentally changes thereasoning behaviorsof the\nmodels. In contrast,next-token trainingwith the same data yields none of\nthese findings. Our systematic experiments provide fresh insights into LLM\nlearning, suggesting the value of first building base models with basic skills,\nthen using RL to incentivize advanced, generalizable skills for complex\nproblems. Can RL teach LLMs new skills? We find the key iscomposition. Our work shows that once a model has the necessary atomic skills, properly incentivized RL enables it to learn a generalizable and transferable meta-skill for composing those abilities while SFT cannot. We also clarify \"RL only reranks\" debate: a fine-grained pass@k analysis reveals that while the performance gap may shrink on easy problems, it widens dramatically on difficult ones, proving genuine skill acquisition. Code:https://github.com/PRIME-RL/RL-CompositionalityTweet:https://x.com/lifan__yuan/status/1963662222602723673 This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
    "metadata": {
        "title": "From f(x) and g(x) to f(g(x)): LLMs Learn New Skills in RL by\n  Composing Old Ones",
        "authors": [],
        "publication_year": 2025,
        "github_url": "https://github.com/PRIME-RL/RL-Compositionality",
        "huggingface_url": "https://huggingface.co/papers/2509.25123",
        "upvote": 20
    }
}