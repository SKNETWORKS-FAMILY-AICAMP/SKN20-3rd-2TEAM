{
    "context": "An adaptive exploration budget allocation method for reinforcement learning in Large Language Models improves training efficiency and performance on mathematical reasoning benchmarks. Large Language Models (LLMs) can self-improve throughreinforcement learning,\nwhere they generatetrajectoriesto explore and discover better solutions.\nHowever, this exploration process is computationally expensive, often forcing\ncurrent methods to assign limited exploration budgets to each task. This\nuniform allocation creates problematic edge cases: easy tasks consistently\nsucceed while difficult tasks consistently fail, both producing zero gradients\nduring training updates for the widely used Group Relative Policy Optimization\n(GRPO). We address this problem from the lens ofexploration budget allocation.\nViewing each task's exploration as an \"item\" with a distinct \"value\" and\n\"cost\", we establish a connection to the classicalknapsack problem. This\nformulation allows us to derive an optimal assignment rule that adaptively\ndistributes resources based on the model's current learning status. When\napplied to GRPO, our method increases the effective ratio of non-zero policy\ngradients by 20-40% during training. Acting as a computational \"free lunch\",\nour approach could reallocate exploration budgets from tasks where learning is\nsaturated to those where it is most impactful. This enables significantly\nlarger budgets (e.g., 93 rollouts) for especially challenging problems, which\nwould be computationally prohibitive under a uniform allocation. These\nimprovements translate to meaningful gains on mathematical reasoning\nbenchmarks, with average improvements of 2-4 points and peak gains of 9 points\non specific tasks. Notably, achieving comparable performance with traditional\nhomogeneous allocation would require about 2x thecomputational resources. Knapsack RL: Unlocking Exploration of LLMs via Budget Allocation ðŸŽ’ Explorationin LLM training is important but costly. Insufficient exploration limits the modelâ€™s performance ceiling. Current uniform exploration is both ineffective and inefficient: Our idea: treat exploration as aknapsackproblem.ðŸ‘‰Allocate rollouts where they matter most. Results: Paper:https://www.arxiv.org/abs/2509.25849 This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
    "metadata": {
        "title": "Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget\n  Allocation",
        "authors": [
            "Ziniu Li",
            "Ge Zhang"
        ],
        "publication_year": 2025,
        "github_url": "https://github.com/liziniu/KnapsackRL",
        "huggingface_url": "https://huggingface.co/papers/2509.25849",
        "upvote": 47
    }
}