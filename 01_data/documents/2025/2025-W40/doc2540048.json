{
  "context": "TFPI, a simple adaptation to RLVR, improves performance and reduces token usage by discarding thinking content during training, accelerating RL convergence and achieving higher accuracy with less computational cost. Reinforcement Learning with Verifiable Reward (RLVR)effectively solves\ncomplex tasks but demands extremely long context lengths during training,\nleading to substantial computational costs. While multi-stage training can\npartially mitigate this, starting with overly short contexts often causes\nirreversible performance degradation, ultimately failing to reduce overall\ntraining compute significantly. In this paper, we introduce\n**T**hinking-**F**ree **P**olicy **I**nitialization (**TFPI**), a simple yet\neffective adaptation to RLVR that bridges long Chain-of-Thought (CoT)\ndistillation and standard RLVR. TFPI employs a simple *ThinkFree* operation,\nexplicitly discarding the thinking content via a direct *</think>* append, to\nreducetoken usageduring inference. Training with *ThinkFree*-adapted inputs\nimproves performance and lowers token consumption, even in the original\nslow-thinking mode. Extensive experiments across various benchmarks have shown\nthat TFPI acceleratesRL convergence, achieves a higherperformance ceiling,\nand yields moretoken-efficient reasoning modelswithout specialized rewards or\ncomplex training designs. With TFPI only, we train a 4B model to reach 89.0%\naccuracy onAIME24and 65.5% onLiveCodeBenchusing less than 4K H20 hours. We propose the Thinking-Free Policy Initialization, a stage prior to RL that can accelerate RL convergence to a higher performance ceiling and naturally yield token-efficient reasoning models. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "title": "Thinking-Free Policy Initialization Makes Distilled Reasoning Models\n  More Effective and Efficient Reasoners",
    "authors": [
      "Xin Xu",
      "Kai Yang"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2509.26226",
    "upvote": 32,
    "tags": []
  }
}