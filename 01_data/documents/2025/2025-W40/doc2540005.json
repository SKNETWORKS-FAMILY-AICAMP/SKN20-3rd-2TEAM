{
    "context": "DeepSearch integrates Monte Carlo Tree Search into RLVR training to enhance exploration and credit assignment, achieving state-of-the-art performance with reduced computational cost. AlthoughRLVRhas become an essential component for developing advanced\nreasoning skills in LLMs, contemporary studies have documented training\nplateaus that emerge following thousands of optimization steps, demonstrating\nnotable decreases in performance gains despite increased computational\ninvestment. This limitation stems from the sparse exploration patterns inherent\nin currentRLVRpractices, where models rely on limited rollouts that often\nmiss critical reasoning paths and fail to provide systematic coverage of the\nsolution space. We present DeepSearch, a framework that integrates Monte Carlo\nTree Search directly intoRLVRtraining. In contrast to existing methods that\nrely on tree search only at inference, DeepSearch embeds structured search into\nthetraining loop, enablingsystematic explorationand fine-grained credit\nassignment across reasoning steps. Through training-time exploration,\nDeepSearch addresses the fundamental bottleneck of insufficient exploration,\nwhich leads to diminishing performance improvements over prolonged training\nsteps. Our contributions include: (1) aglobal frontier selectionstrategy that\nprioritizes promising nodes across the search tree, (2) selection withentropy-based guidancethat identifies confident paths for supervision, and (3)adaptive replay buffertraining withsolution cachingfor efficiency.\nExperiments onmathematical reasoning benchmarksshow that DeepSearch achieves\n62.95% average accuracy and establishes a new state-of-the-art for 1.5B\nreasoning models - using 5.7x fewer GPU hours than extended training\napproaches. These results highlight the importance of strategic exploration\nover brute-force scaling and demonstrate the promise of algorithmic innovation\nfor advancingRLVRmethodologies. DeepSearch establishes a new direction for\nscaling reasoning capabilities through systematic search rather than prolonged\ncomputation. ðŸ”¥ Concise & Promotional ðŸš€ DeepSearch-1.5B sets a new SOTA in math reasoning for 1.5B LMs:âœ… 62.95% avg accuracy (+1.25% over prior best)âœ… 5.7Ã— fewer GPU hours than extended trainingKey idea: bring MCTS into training, not just inference, for systematic exploration & better credit assignment. ðŸ‘‰ Paper:https://arxiv.org/pdf/2509.25454ðŸ‘‰ Model:https://huggingface.co/fangwu97/DeepSearch-1.5B ðŸ§  Technical & Insightful We introduce DeepSearch, a framework that integrates Monte Carlo Tree Search into RLVR training.Unlike existing methods that restrict search to inference, DeepSearch systematically explores reasoning paths during trainingâ€”achieving fine-grained credit assignment, efficient supervision, and robust exploration. ðŸ“Š Results: -- 62.95% avg accuracy on math reasoning (SOTA for 1.5B models) -- Outperforms Nemotron-Reasoning-Qwen-1.5B v2 by +1.25% -- Uses 5.7Ã— fewer GPU hours than depth-scaled training This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Does using the replay buffer risk turning the method into off-policy, and do you have ablations isolating its gain? Â·Sign uporlog into comment",
    "metadata": {
        "title": "DeepSearch: Overcome the Bottleneck of Reinforcement Learning with\n  Verifiable Rewards via Monte Carlo Tree Search",
        "authors": [
            "Fang Wu",
            "Weihao Xuan"
        ],
        "publication_year": 2025,
        "github_url": "https://github.com/smiles724/DeepSearch",
        "huggingface_url": "https://huggingface.co/papers/2509.25454",
        "upvote": 139
    }
}