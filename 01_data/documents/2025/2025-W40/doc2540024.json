{
  "context": "TruthRL, a reinforcement learning framework, enhances the truthfulness of large language models by balancing accuracy and abstention, significantly reducing hallucinations and improving performance across benchmarks. Whilelarge language models(LLMs) have demonstrated strong performance on\nfactoid question answering, they are still prone tohallucinationanduntruthful responses, particularly when tasks demand information outside theirparametric knowledge. Indeed,truthfulnessrequires more than accuracy --\nmodels must also recognize uncertainty and abstain when unsure to avoidhallucinations. This presents a fundamental challenge for existing methods:\napproaches that optimize for accuracy often amplifyhallucinations, while those\nthat encourageabstentioncan become overly conservative, sacrificing correct\nanswers. Both extremes ultimately compromisetruthfulness. In this work, we\npresent TruthRL, a generalreinforcement learning(RL) framework that directly\noptimizes thetruthfulnessofLLMs. Specifically, we implement TruthRLusingGRPOwith a simple yet effectiveternary rewardthat distinguishes correct\nanswers,hallucinations, andabstentions. It incentivizes models to reducehallucinations not only by providing correct responses, but also by enablingabstentionwhen uncertain, thereby improvingtruthfulness. Extensive\nexperiments across fourknowledge-intensive benchmarksshow that, compared to\nvanillaRL, TruthRLsignificantly reduceshallucinations by 28.9% and improvestruthfulnessby 21.1%, with consistent gains across various backbone models\n(e.g.,Qwen,Llama) under bothretrievalandnon-retrieval setups. In-depthablation studydemonstrates that vanillaaccuracy-driven methods, such assupervised fine-tuningorRLwith abinary reward, struggle to balance factual\ncorrectness and uncertainty. In contrast, our proposedtruthfulness-driven\nTruthRLachieves strong performance in both accuracy andtruthfulness,\nunderscoring the importance of learning objective design for developing\ntruthfulLLMs. While large language models (LLMs) have demonstrated strong performance on factoid question answering, they are still prone to hallucination and untruthful responses, particularly when tasks demand information outside their parametric knowledge. Indeed, truthfulness requires more than accuracy---models must also recognize uncertainty and abstain when unsure to avoid hallucinations. This presents a fundamental challenge for existing methods: approaches that optimize for accuracy often amplify hallucinations, while those that encourage abstention can become overly conservative, sacrificing correct answers. Both extremes ultimately compromise truthfulness. In this work, we present TruthRL, a general reinforcement learning (RL) framework that directly optimizes the truthfulness of LLMs. Specifically, we implement TruthRL using GRPO with a simple yet effective ternary reward that distinguishes correct answers, hallucinations, and abstentions. It incentivizes models to reduce hallucinations not only by providing correct responses, but also by enabling abstention when uncertain, thereby improving truthfulness. Extensive experiments across four knowledge-intensive benchmarks show that, compared to vanilla RL, TruthRL significantly reduces hallucinations by 28.9% and improves truthfulness by 21.1%, with consistent gains across various backbone models (e.g., Qwen, Llama) under both retrieval and non-retrieval setups. In-depth ablation study demonstrates that vanilla accuracy-driven methods, such as supervised fine-tuning or RL with a binary reward, struggle to balance factual correctness and uncertainty.  In contrast, our proposed truthfulness-driven TruthRL achieves strong performance in both accuracy and truthfulness, underscoring the importance of learning objective design for developing truthful LLMs. We also experiment with more complicated reward designs, such as knowledge-enhanced and reasoning-enhanced variants, and show that a simple ternary reward scheme generally performs better. Moreover, we find the improvement of TruthRL arises from enhancing the capability of LLMs to recognize their knowledge boundary, hence avoiding being overly conservative as the baselines are. Further analysis confirms that TruthRL is robust to hallucination-baiting questions and more confident in producing accurate responses. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend arXiv explained breakdown of this paper ðŸ‘‰https://arxivexplained.com/papers/truthrl-incentivizing-truthful-llms-via-reinforcement-learning Â·Sign uporlog into comment",
  "metadata": {
    "title": "TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning",
    "authors": [
      "Teja Gollapudi"
    ],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2509.25760",
    "upvote": 55,
    "tags": []
  }
}