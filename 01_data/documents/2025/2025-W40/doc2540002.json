{
    "context": "LongLive is a frame-level autoregressive framework for real-time and interactive long video generation, addressing efficiency and quality challenges through causal attention, KV-recache, streaming long tuning, and short window attention. We present LongLive, aframe-level autoregressive(AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due tobidirectional attention.Causal attentionAR models supportKV cachingfor\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration,interactive capabilities, such asstreaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates aKV-recache mechanismthat refreshes cached states\nwith new prompts for smooth, adherent switches;streaming long tuningto enable\nlong video training and to align training and inference (train-long-test-long);\nandshort window attentionpaired with aframe-level attention sink, shorten asframe sink, preservinglong-range consistencywhile enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance onVBenchin both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supportsINT8-quantized inferencewith only marginal\nquality loss.  Paper:https://arxiv.org/abs/2509.22622Code:https://github.com/NVlabs/LongLiveModel:https://huggingface.co/Efficient-Large-Model/LongLive-1.3BDemo Page:https://nvlabs.github.io/LongLiveIntroduction Video:https://www.youtube.com/watch?v=CO1QC7BNvig This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
    "metadata": {
        "title": "LongLive: Real-time Interactive Long Video Generation",
        "authors": [
            "Wei Huang",
            "Muyang Li",
            "Yukang Chen"
        ],
        "publication_year": 2025,
        "github_url": "https://github.com/NVlabs/LongLive",
        "huggingface_url": "https://huggingface.co/papers/2509.22622",
        "upvote": 184
    }
}