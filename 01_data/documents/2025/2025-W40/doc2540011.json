{
    "context": "A method is proposed to enhance long-horizon video generation by using sampled segments from self-generated long videos to guide student models, maintaining quality and consistency without additional supervision or retraining. Diffusion modelshave revolutionized image and video generation, achieving\nunprecedented visual quality. However, their reliance on transformer\narchitectures incurs prohibitively high computational costs, particularly when\nextending generation to long videos. Recent work has explored autoregressive\nformulations for long video generation, typically by distilling from\nshort-horizonbidirectional teachers. Nevertheless, given that teacher models\ncannot synthesize long videos, the extrapolation of student models beyond their\ntraining horizon often leads to pronouncedquality degradation, arising from\nthe compounding of errors within the continuouslatent space. In this paper, we\npropose a simple yet effective approach to mitigatequality degradationin\nlong-horizon video generation without requiring supervision from long-video\nteachers or retraining on long video datasets. Our approach centers on\nexploiting the rich knowledge of teacher models to provide guidance for the\nstudent model through sampled segments drawn from self-generated long videos.\nOur method maintainstemporal consistencywhile scaling video length by up to\n20x beyond teacher's capability, avoiding common issues such as over-exposure\nand error-accumulation without recomputing overlapping frames like previous\nmethods. When scaling up the computation, our method shows the capability of\ngenerating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the\nmaximum span supported by our base model'sposition embeddingand more than 50x\nlonger than that of our baseline model. Experiments on standard benchmarks and\nour proposed improved benchmark demonstrate that our approach substantially\noutperforms baseline methods in both fidelity and consistency. Our long-horizon\nvideos demo can be found at https://self-forcing-plus-plus.github.io/ Please checkout our project page at:https://self-forcing-plus-plus.github.io/, thank you! Great work  üëèüëèüëè This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ¬∑Sign uporlog into comment",
    "metadata": {
        "title": "Self-Forcing++: Towards Minute-Scale High-Quality Video Generation",
        "authors": [
            "Jie Wu",
            "Ming Li",
            "Rui Wang"
        ],
        "publication_year": 2025,
        "github_url": "https://github.com/justincui03/Self-Forcing-Plus-Plus",
        "huggingface_url": "https://huggingface.co/papers/2510.02283",
        "upvote": 95
    }
}