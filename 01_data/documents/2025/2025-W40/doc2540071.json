{
  "context": "UltraHorizon is a new benchmark that evaluates long-horizon and partially observable tasks for autonomous agents, highlighting gaps in their sustained reasoning, planning, memory, and tool use capabilities. Autonomous agents have recently achieved remarkable progress across diverse\ndomains, yet most evaluations focus on short-horizon, fully observable tasks.\nIn contrast, many critical real-world tasks, such as large-scale software\ndevelopment, commercial investment, and scientific discovery, unfold inlong-horizonandpartially observablescenarios where success hinges onsustained reasoning,planning,memory management, andtool use. Existing\nbenchmarks rarely capture theselong-horizonchallenges, leaving a gap in\nsystematic evaluation. To bridge this gap, we introduceUltraHorizona\nnovel benchmark that measures the foundational capabilities essential for\ncomplex real-world challenges. We useexplorationas a unifying task across\nthree distinct environments to validate these core competencies. Agents are\ndesigned inlong-horizondiscovery tasks where they must iteratively uncover\nhidden rules throughsustained reasoning,planning, memory and tools\nmanagement, and interaction with environments. Under the heaviest scale\nsetting,trajectoriesaverage 200k+tokensand 400+ tool\ncalls, whereas in standard configurations they still exceed 35ktokensand involve more than 60tool callson average. Our extensive\nexperiments reveal thatLLM-agentsconsistently underperform in these settings,\nwhereas human participants achieve higher scores, underscoring a persistent gap\nin agents'long-horizonabilities. We also observe that simple scaling fails in\nour task. To better illustrate the failure of agents, we conduct an in-depth\nanalysis of collectedtrajectories. We identify eight types of errors and\nattribute them to two primary causes:in-context lockingand functional\nfundamental capability gaps.\nhttps://github.com/StarDewXXX/UltraHorizon{Our code will be available\nhere.} Autonomous agents excel in short-horizon, fully observable tasks, but many real-world challenges—such as software development, investment, and scientific discovery—require long-horizon reasoning, planning, memory, and tool use. Existing benchmarks overlook these settings. We introduce UltraHorizon, a benchmark for evaluating agents in long-horizon, partially observable environments through exploration tasks. In the largest setting, trajectories exceed200k tokensand400 tool calls. Experiments show that LLM-agents consistently underperform compared to humans, and naive scaling alone does not solve the problem. Analysis reveals eight error types linked toin-context lockingand fundamental capability gaps. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ·Sign uporlog into comment",
  "metadata": {
    "title": "UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon\n  Scenarios",
    "authors": [],
    "publication_year": 2025,
    "github_url": "https://github.com/StarDewXXX/UltraHorizon",
    "huggingface_url": "https://huggingface.co/papers/2509.21766",
    "upvote": 23,
    "tags": []
  }
}