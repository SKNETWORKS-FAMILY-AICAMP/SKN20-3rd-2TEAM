{
    "context": "LLaVA-OneVision-1.5 is a family of large multimodal models that achieves state-of-the-art performance with reduced costs through efficient training and high-quality datasets. We present LLaVA-OneVision-1.5, a novel family ofLarge Multimodal Models(LMMs) that achievestate-of-the-art performancewith significantly reduced\ncomputational and financial costs. Different from the existing works,\nLLaVA-OneVision-1.5 provides an open, efficient, and reproducible framework for\nbuilding high-quality vision-language models entirely from scratch. The\nLLaVA-OneVision-1.5 release comprises three primary components: (1) Large-Scale\nCurated Datasets: We construct an 85M concept-balancedpretraining datasetLLaVA-OneVision-1.5-Mid-Traning and a meticulously curated 26M instruction\ndataset LLaVA-OneVision-1.5-Instruct, collectively encompassing 64B compressedmultimodal tokens. (2)Efficient Training Framework: We develop a complete\nend-to-endefficient training frameworkleveraging an offline parallel data\npacking strategy to facilitate the training of LLaVA-OneVision-1.5 within a\n$16,000 budget. (3)State-of-the-art Performance: Experimental results\ndemonstrate that LLaVA-OneVision1.5 yields exceptionally competitive\nperformance across a broad range ofdownstream tasks. Specifically,\nLLaVA-OneVision-1.5-8B outperformsQwen2.5-VL-7Bon 18 of 27 benchmarks, and\nLLaVA-OneVision-1.5-4B surpassesQwen2.5-VL-3Bon all 27 benchmarks. We\nanticipate releasing LLaVA-OneVision-1.5-RL shortly and encourage the community\nto await further updates.    Outstanding release! Thank you so much Â·Sign uporlog into comment",
    "metadata": {
        "title": "LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal\n  Training",
        "authors": [
            "Xiang An",
            "Kaicheng Yang",
            "Huajie Tan",
            "Xiyao Wang",
            "Zizhen Yan",
            "Bo Li"
        ],
        "publication_year": 2025,
        "github_url": "https://github.com/EvolvingLMMs-Lab/LLaVA-OneVision-1.5",
        "huggingface_url": "https://huggingface.co/papers/2509.23661",
        "upvote": 46
    }
}