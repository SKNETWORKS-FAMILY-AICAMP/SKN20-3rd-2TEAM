{
  "context": "While recent video generation models have achieved significant visual fidelity, they often suffer from the lack of explicit physical controllability and plausibility . To address this, some recent studies attempted to guide the video generation with physics-based rendering . However, these methods face inherent challenges in accurately modeling complex physical properties and effectively control ling the resulting physical behavior over extended temporal sequences. In this work, we introduce PhysChoreo, a novel framework that can generate videos with diverse controllability and physical realism from a single image. Our method consists of two stages: first, it estimates the static initial physical properties of all objects in the image through part-aware physical property reconstruction . Then, through temporally instructed and physically editable simulation, it synthesizes high-quality videos with rich dynamic behaviors and physical realism . Experimental results show that PhysChoreo can generate videos with rich behaviors and physical realism , outperforming state-of-the-art methods on multiple evaluation metrics.",
  "metadata": {
    "paper_name": "PhysChoreo: Physics-Controllable Video Generation with Part-Aware Semantic Grounding",
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2511.20562",
    "upvote": 4,
    "tags": [
      "physical",
      "videos",
      "realism"
    ]
  }
}