doc_id,context,metadata
doc2548001,"The optimization of large language models ( LLMs ) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization , but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise . To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers , particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.","{'paper_name': 'ROOT: Robust Orthogonalized Optimizer for Neural Network Training', 'github_url': 'https://github.com/huawei-noah/noah-research/tree/master/ROOT', 'huggingface_url': 'https://huggingface.co/papers/2511.20626', 'upvote': 166, 'tags': ['robust', 'robustness', 'root']}"
doc2548002,"Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called general agentic memory ( GAM ). GAM follows the principle of ""just-in time (JIT) compilation"" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) Memorizer , which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store . 2) Researcher , which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models ( LLMs ), while also facilitating end-to-end performance optimization through reinforcement learning . In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.","{'paper_name': 'General Agentic Memory Via Deep Research', 'github_url': 'https://github.com/VectorSpaceLab/general-agentic-memory', 'huggingface_url': 'https://huggingface.co/papers/2511.18423', 'upvote': 150, 'tags': ['memory', 'gam', 'information']}"
doc2548003,"Recent advances in LLM-guided evolutionary computation , particularly AlphaEvolve (Novikov et al., 2025; Georgiev et al., 2025), have demonstrated remarkable success in discovering novel mathematical constructions and solving challenging optimization problems. However, the high-level descriptions in published work leave many implementation details unspecified, hindering reproducibility and further research. In this report we present GigaEvo, an extensible open-source framework that enables researchers to study and experiment with hybrid LLM-evolution approaches inspired by AlphaEvolve . Our system provides modular implementations of key components: MAP-Elites quality-diversity algorithms , asynchronous DAG-based evaluation pipelines, LLM-driven mutation operators with insight generation and bidirectional lineage tracking , and flexible multi-island evolutionary strategies . In order to assess reproducibility and validate our implementation we evaluate GigaEvo on challenging problems from the AlphaEvolve paper: Heilbronn triangle placement , circle packing in squares, and high-dimensional kissing numbers . The framework emphasizes modularity, concurrency, and ease of experimentation, enabling rapid prototyping through declarative configuration. We provide detailed descriptions of system architecture, implementation decisions, and experimental methodology to support further research in LLM driven evolutionary methods. The GigaEvo framework and all experimental code are available at https://github.com/AIRI-Institute/gigaevo-core.","{'paper_name': 'GigaEvo: An Open Source Optimization Framework Powered By LLMs And Evolution Algorithms', 'github_url': 'https://github.com/FusionBrainLab/gigaevo-core', 'huggingface_url': 'https://huggingface.co/papers/2511.17592', 'upvote': 117, 'tags': ['llm', 'gigaevo', 'framework']}"
doc2548004,"Multi-agent systems (MAS) extend large language models (LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, we take a step forward by enabling models to collaborate directly within the continuous latent space . We introduce LatentMAS , an end-to-end training-free framework that enables pure latent collaboration among LLM agents. In LatentMAS , each agent first performs auto-regressive latent thoughts generation through last-layer hidden embeddings . A shared latent working memory then preserves and transfers each agent's internal representations, ensuring lossless information exchange. We provide theoretical analyses establishing that LatentMAS attains higher expressiveness and lossless information preservation with substantially lower complexity than vanilla text-based MAS. In addition, empirical evaluations across 9 comprehensive benchmarks spanning math and science reasoning , commonsense understanding , and code generation show that LatentMAS consistently outperforms strong single-model and text-based MAS baselines, achieving up to 14.6% higher accuracy , reducing output token usage by 70.8%-83.7%, and providing 4x-4.3x faster end-to-end inference . These results demonstrate that our new latent collaboration framework enhances system-level reasoning quality while offering substantial efficiency gains without any additional training. Code and data are fully open-sourced at https://github.com/Gen-Verse/ LatentMAS .","{'paper_name': 'Latent Collaboration in Multi-Agent Systems', 'github_url': 'https://github.com/Gen-Verse/LatentMAS', 'huggingface_url': 'https://huggingface.co/papers/2511.20639', 'upvote': 103, 'tags': ['latent', 'latentmas', 'reasoning']}"
doc2548005,"We present Segment Anything Model (SAM) 3, a unified model that detects, segments, and tracks objects in images and videos based on concept prompts , which we define as either short noun phrases (e.g., ""yellow school bus""), image exemplars, or a combination of both. Promptable Concept Segmentation (PCS) takes such prompts and returns segmentation masks and unique identities for all matching object instances. To advance PCS, we build a scalable data engine that produces a high-quality dataset with 4M unique concept labels, including hard negatives, across images and videos. Our model consists of an image-level detector and a memory-based video tracker that share a single backbone. Recognition and localization are decoupled with a presence head , which boosts detection accuracy. SAM 3 doubles the accuracy of existing systems in both image and video PCS, and improves previous SAM capabilities on visual segmentation tasks . We open source SAM 3 along with our new Segment Anything with Concepts (SA-Co) benchmark for promptable concept segmentation .","{'paper_name': 'SAM 3: Segment Anything with Concepts', 'github_url': 'https://github.com/facebookresearch/sam3', 'huggingface_url': 'https://huggingface.co/papers/2511.16719', 'upvote': 99, 'tags': ['segmentation', 'sam', 'concept']}"
doc2548006,"Current research on agentic visual reasoning enables deep multimodal understanding but primarily focuses on image manipulation tools, leaving a gap toward more general-purpose agentic models. In this work, we revisit the geolocalization task, which requires not only nuanced visual grounding but also web search to confirm or refine hypotheses during reasoning. Since existing geolocalization benchmarks fail to meet the need for high-resolution imagery and the localization challenge for deep agentic reasoning, we curate GeoBench , a benchmark that includes photos and panoramas from around the world, along with a subset of satellite images of different cities to rigorously evaluate the geolocalization ability of agentic models. We also propose GeoVista , an agentic model that seamlessly integrates tool invocation within the reasoning loop, including an image-zoom-in tool to magnify regions of interest and a web-search tool to retrieve related web information. We develop a complete training pipeline for it, including a cold-start supervised fine-tuning (SFT) stage to learn reasoning patterns and tool-use priors, followed by a reinforcement learning (RL) stage to further enhance reasoning ability. We adopt a hierarchical reward to leverage multi-level geographical information and improve overall geolocalization performance. Experimental results show that GeoVista surpasses other open-source agentic models on the geolocalization task greatly and achieves performance comparable to closed-source models such as Gemini-2.5-flash and GPT-5 on most metrics.","{'paper_name': 'GeoVista: Web-Augmented Agentic Visual Reasoning for Geolocalization', 'github_url': 'https://github.com/ekonwang/GeoVista', 'huggingface_url': 'https://huggingface.co/papers/2511.15705', 'upvote': 90, 'tags': ['reasoning', 'agentic', 'geolocalization']}"
doc2548007,"Recent advancements in large reasoning models have fueled growing interest in extending such capabilities to multimodal domains. However, despite notable progress in visual reasoning, the lack of transparent and reproducible data curation and training strategies remains a major barrier to scalable research. In this work, we introduce OpenMMReasoner, a fully transparent two-stage recipe for multimodal reasoning spanning supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we construct an 874K-sample cold-start dataset with rigorous step-by-step validation , providing a strong foundation for reasoning capabilities. The subsequent RL stage leverages a 74K-sample dataset across diverse domains to further sharpen and stabilize these abilities, resulting in a more robust and efficient learning process. Extensive evaluations demonstrate that our training recipe not only surpasses strong baselines but also highlights the critical role of data quality and training design in shaping multimodal reasoning performance. Notably, our method achieves a 11.6% improvement over the Qwen2.5-VL-7B-Instruct baseline across nine multimodal reasoning benchmarks , establishing a solid empirical foundation for future large-scale multimodal reasoning research. We open-sourced all our codes, pipeline, and data at https://github.com/EvolvingLMMs-Lab/OpenMMReasoner.","{'paper_name': 'OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe', 'github_url': 'https://github.com/EvolvingLMMs-Lab/OpenMMReasoner', 'huggingface_url': 'https://huggingface.co/papers/2511.16334', 'upvote': 89, 'tags': ['reasoning', 'multimodal', 'multimodal reasoning']}"
doc2548008,"Humans naturally adapt to diverse environments by learning underlying rules across worlds with different dynamics, observations, and reward structures. In contrast, existing agents typically demonstrate improvements via self-evolving within a single domain, implicitly assuming a fixed environment distribution. Cross-environment learning has remained largely unmeasured: there is no standard collection of controllable, heterogeneous environments , nor a unified way to represent how agents learn. We address these gaps in two steps. First, we propose AutoEnv , an automated framework that treats environments as factorizable distributions over transitions, observations, and rewards, enabling low-cost (4.12 USD on average) generation of heterogeneous worlds. Using AutoEnv , we construct AutoEnv-36 , a dataset of 36 environments with 358 validated levels, on which seven language models achieve 12-49% normalized reward , demonstrating the challenge of AutoEnv-36 . Second, we formalize agent learning as a component-centric process driven by three stages of Selection , Optimization , and Evaluation applied to an improvable agent component. Using this formulation, we design eight learning methods and evaluate them on AutoEnv-36 . Empirically, the gain of any single learning method quickly decrease as the number of environments increases, revealing that fixed learning methods do not scale across heterogeneous environments . Environment-adaptive selection of learning methods substantially improves performance but exhibits diminishing returns as the method space expands. These results highlight both the necessity and the current limitations of agent learning for scalable cross-environment generalization, and position AutoEnv and AutoEnv-36 as a testbed for studying cross-environment agent learning. The code is avaiable at https://github.com/FoundationAgents/ AutoEnv .","{'paper_name': 'AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.19304', 'upvote': 88, 'tags': ['learning', 'autoenv', 'environments']}"
doc2548009,"Intrinsic dimension (ID) is an important tool in modern LLM analysis, informing studies of training dynamics, scaling behavior, and dataset structure, yet its textual determinants remain underexplored. We provide the first comprehensive study grounding ID in interpretable text properties through cross-encoder analysis , linguistic features, and sparse autoencoders (SAEs). In this work, we establish three key findings. First, ID is complementary to entropy-based metrics : after controlling for length, the two are uncorrelated, with ID capturing geometric complexity orthogonal to prediction quality. Second, ID exhibits robust genre stratification : scientific prose shows low ID (~8), encyclopedic content medium ID (~9), and creative/opinion writing high ID (~10.5) across all models tested. This reveals that contemporary LLMs find scientific text ""representationally simple"" while fiction requires additional degrees of freedom. Third, using SAEs, we identify causal features : scientific signals ( formal tone , report templates , statistics ) reduce ID; humanized signals ( personalization , emotion , narrative ) increase it. Steering experiments confirm these effects are causal. Thus, for contemporary models, scientific writing appears comparatively ""easy"", whereas fiction, opinion, and affect add representational degrees of freedom. Our multi-faceted analysis provides practical guidance for the proper use of ID and the sound interpretation of ID-based results.","{'paper_name': 'Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.15210', 'upvote': 85, 'tags': ['id', 'scientific', 'analysis']}"
doc2548010,"Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language , where no multimodal benchmarks currently exist, we introduce Mera Multi , an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities ; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage , including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.","{'paper_name': 'Multimodal Evaluation of Russian-language Architectures', 'github_url': 'https://github.com/MERA-Evaluation/MERA_MULTIMODAL/tree/main', 'huggingface_url': 'https://huggingface.co/papers/2511.15552', 'upvote': 76, 'tags': ['multimodal', 'text', 'russian']}"
doc2548011,"Pixel diffusion aims to generate images directly in pixel space in an end-to-end fashion. This approach avoids the limitations of VAE in the two-stage latent diffusion , offering higher model capacity. Existing pixel diffusion models suffer from slow training and inference, as they usually model both high-frequency signals and low-frequency semantics within a single diffusion transformer (DiT) . To pursue a more efficient pixel diffusion paradigm, we propose the frequency-DeCoupled pixel diffusion framework. With the intuition to decouple the generation of high and low frequency components, we leverage a lightweight pixel decoder to generate high-frequency details conditioned on semantic guidance from the DiT. This thus frees the DiT to specialize in modeling low-frequency semantics. In addition, we introduce a frequency-aware flow-matching loss that emphasizes visually salient frequencies while suppressing insignificant ones. Extensive experiments show that DeCo achieves superior performance among pixel diffusion models, attaining FID of 1.62 (256x256) and 2.22 (512x512) on ImageNet , closing the gap with latent diffusion methods. Furthermore, our pretrained text-to-image model achieves a leading overall score of 0.86 on GenEval in system-level comparison. Codes are publicly available at https://github.com/Zehong-Ma/DeCo.","{'paper_name': 'DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation', 'github_url': 'https://github.com/Zehong-Ma/DeCo', 'huggingface_url': 'https://huggingface.co/papers/2511.19365', 'upvote': 62, 'tags': ['diffusion', 'pixel', 'frequency']}"
doc2548012,"Deep research models perform multi-step research to produce long-form, well-attributed answers. However, most open deep research models are trained on easily verifiable short-form QA tasks via reinforcement learning with verifiable rewards (RLVR) , which does not extend to realistic long-form tasks. We address this with Reinforcement Learning with Evolving Rubrics (RLER) , in which we construct and maintain rubrics that co-evolve with the policy model during training; this allows the rubrics to incorporate information that the model has newly explored and to provide discriminative, on-policy feedback. Using RLER, we develop Deep Research Tulu (DR Tulu-8B) , the first open model that is directly trained for open-ended, long-form deep research . Across four long-form deep research benchmarks in science, healthcare and general domains, DR Tulu substantially outperforms existing open deep research models , and matches or exceeds proprietary deep research systems, while being significantly smaller and cheaper per query. To facilitate future research, we release all data, models, and code, including our new MCP-based agent infrastructure for deep research systems.","{'paper_name': 'DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research', 'github_url': 'https://github.com/rlresearch/dr-tulu', 'huggingface_url': 'https://huggingface.co/papers/2511.19399', 'upvote': 53, 'tags': ['research', 'deep research', 'deep']}"
doc2548013,"Computer-Use Agents ( CUA ) are becoming increasingly capable of autonomously operating digital environments through Graphical User Interfaces ( GUI ). Yet, most GUI remain designed primarily for humans--prioritizing aesthetics and usability--forcing agents to adopt human-oriented behaviors that are unnecessary for efficient task execution. At the same time, rapid advances in coding-oriented language models ( Coder ) have transformed automatic GUI design. This raises a fundamental question: Can CUA as judges to assist Coder for automatic GUI design? To investigate, we introduce AUI-Gym , a benchmark for Automatic GUI development spanning 52 applications across diverse domains. Using language models , we synthesize 1560 tasks that simulate real-world scenarios. To ensure task reliability , we further develop a verifier that programmatically checks whether each task is executable within its environment. Building on this, we propose a Coder - CUA in Collaboration framework: the Coder acts as Designer, generating and revising websites, while the CUA serves as Judge, evaluating functionality and refining designs. Success is measured not by visual appearance, but by task solvability and CUA navigation success rate . To turn CUA feedback into usable gui dance, we design a CUA Dashboard that compresses multi-step navigation histories into concise visual summaries, offering interpretable gui dance for iterative redesign. By positioning agents as both designers and judges, our framework shifts interface design toward agent-native efficiency and reliability. Our work takes a step toward shifting agents from passive use toward active participation in digital environments. Our code and dataset are available at https://github.com/showlab/AUI.","{'paper_name': 'Computer-Use Agents as Judges for Generative User Interface', 'github_url': 'https://github.com/showlab/AUI', 'huggingface_url': 'https://huggingface.co/papers/2511.15567', 'upvote': 50, 'tags': ['cua', 'gui', 'task']}"
doc2548014,"Medical image segmentation is fundamental for biomedical discovery. Existing methods lack generalizability and demand extensive, time-consuming manual annotation for new clinical application. Here, we propose MedSAM-3, a text promptable medical segmentation model for medical image and video segmentation. By fine-tuning the Segment Anything Model (SAM) 3 archite ct ure on medical images paired with semantic conceptual labels, our MedSAM-3 enables medical Promptable Concept Segmentation (PCS) , allowing precise targeting of anatomical stru ct ures via open-vocabulary text descriptions rather than solely geometric prompts. We further introduce the MedSAM-3 Agent, a framework that integrates Multimodal Large Language Models (MLLMs) to perform complex reasoning and iterative refinement in an agent-in-the-loop workflow. Comprehensive experiments across diverse medical imaging modalities, including X-ray , MRI , Ultrasound , CT , and video , demonstrate that our approach significantly outperforms existing specialist and foundation models. We will release our code and model at https://github.com/Joey-S-Liu/MedSAM3.","{'paper_name': 'MedSAM3: Delving into Segment Anything with Medical Concepts', 'github_url': 'https://github.com/Joey-S-Liu/MedSAM3', 'huggingface_url': 'https://huggingface.co/papers/2511.19046', 'upvote': 47, 'tags': ['medical', 'segmentation', 'model']}"
doc2548015,"Vision-language agents have achieved remarkable progress in a variety of multimodal reasoning tasks; however, their learning remains constrained by the limitations of human-annotated supervision. Recent self-rewarding approaches attempt to overcome this constraint by allowing models to act as their own critics or reward providers. Yet, purely text-based self-evaluation struggles to verify complex visual reasoning steps and often suffers from evaluation hallucinations. To address these challenges, inspired by recent advances in tool-integrated reasoning , we propose Agent0-VL, a self-evolving vision-language agent that achieves continual improvement with tool-integrated reasoning . Agent0-VL incorporates tool usage not only into reasoning but also into self-evaluation and self-repair , enabling the model to introspect , verify, and refine its reasoning through evidence-grounded analysis . It unifies two synergistic roles within a single LVLM: a Solver that performs multi-turn tool-integrated reasoning , and a Verifier that generates structured feedback and fine-grained self-rewards through tool-grounded critique. These roles interact through a Self-Evolving Reasoning Cycle , where tool-based verification and reinforcement learning jointly align the reasoning and evaluation distributions for stable self-improvement. Through this zero-external-reward evolution, Agent0-VL aligns its reasoning and verification behaviors without any human annotation or external reward models, achieving continual self-improvement. Experiments on geometric problem solving and visual scientific analysis show that Agent0-VL achieves an 12.5% improvement over the base model. Our code is available at https://github.com/aiming-lab/Agent0/Agent0-VL{this https URL}.","{'paper_name': 'Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning', 'github_url': 'https://github.com/aiming-lab/Agent0/tree/main/Agent0-VL', 'huggingface_url': 'https://huggingface.co/papers/2511.19900', 'upvote': 46, 'tags': ['reasoning', 'self', 'tool']}"
doc2548016,"World models serve as core simulators for fields such as agentic AI , embodied AI , and gaming , capable of generating long, physically realistic, and interactive high-quality videos. Moreover, scaling these models could unlock emergent capabilities in visual perception , understanding , and reasoning , paving the way for a new paradigm that moves beyond current LLM-centric vision foundation models. A key breakthrough empowering them is the semi-autoregressive ( block-diffusion ) decoding paradigm, which merges the strengths of diffusion and autoregressive methods by generating video tokens in block-applying diffusion within each block while conditioning on previous ones, resulting in more coherent and stable video sequences. Crucially, it overcomes limitations of standard video diffusion by reintroducing LLM-style KV Cache management , enabling efficient, variable-length, and high-quality generation.
  Therefore, Inferix is specifically designed as a next-generation inference engine to enable immersive world synthesis through optimized semi-autoregressive decoding processes. This dedicated focus on world simulation distinctly sets it apart from systems engineered for high-concurrency scenarios (like vLLM or SGLang ) and from classic video diffusion models (such as xDiTs ). Inferix further enhances its offering with interactive video streaming and profiling , enabling real-time interaction and realistic simulation to accurately model world dynamics. Additionally, it supports efficient benchmarking through seamless integration of LV-Bench , a new fine-grained evaluation benchmark tailored for minute-long video generation scenarios. We hope the community will work together to advance Inferix and foster world model exploration.","{'paper_name': 'Inferix: A Block-Diffusion based Next-Generation Inference Engine for World Simulation', 'github_url': 'https://github.com/alibaba-damo-academy/Inferix', 'huggingface_url': 'https://huggingface.co/papers/2511.20714', 'upvote': 44, 'tags': ['video', 'world', 'diffusion']}"
doc2548017,"Reward feedback learning ( ReFL ) has proven effective for aligning image generation with human preferences. However, its extension to video generation faces significant challenges. Existing video reward models rely on vision-language models designed for pixel-space inputs, confining ReFL optimization to near-complete denoising steps after computationally expensive VAE decoding . This pixel-space approach incurs substantial memory overhead and increased training time, and its late-stage optimization lacks early-stage supervision, refining only visual quality rather than fundamental motion dynamics and structural coherence. In this work, we show that pre-trained video generation models are naturally suited for reward modeling in the noisy latent space , as they are explicitly designed to process noisy latent representations at arbitrary timesteps and inherently preserve temporal information through their sequential modeling capabilities. Accordingly, we propose Process Reward Feedback Learning ~( PRFL ), a framework that conducts preference optimization entirely in latent space , enabling efficient gradient backpropagation throughout the full denoising chain without VAE decoding . Extensive experiments demonstrate that PRFL significantly improves alignment with human preferences, while achieving substantial reductions in memory consumption and training time compared to RGB ReFL .","{'paper_name': 'Video Generation Models Are Good Latent Reward Models', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.21541', 'upvote': 41, 'tags': ['space', 'reward', 'video']}"
doc2548018,"Preserving first-frame identity while ensuring precise motion control is a fundamental challenge in human image animation. The Image-to-Motion Binding process of the dominant Reference-to-Video (R2V) paradigm overlooks critical spatio-temporal misalignments common in real-world applications, leading to failures such as identity drift and visual artifacts. We introduce SteadyDancer, an Image-to-Video (I2V) paradigm-based framework that achieves harmonized and coherent animation and is the first to ensure first-frame preservation robustly. Firstly, we propose a Condition-Reconciliation Mechanism to harmonize the two conflicting conditions, enabling precise control without sacrificing fidelity. Secondly, we design Synergistic Pose Modulation Modules to generate an adaptive and coherent pose representation that is highly compatible with the reference image. Finally, we employ a Staged Decoupled-Objective Training Pipeline that hierarchically optimizes the model for motion fidelity , visual quality , and temporal coherence . Experiments demonstrate that SteadyDancer achieves state-of-the-art performance in both appearance fidelity and motion control, while requiring significantly fewer training resources than comparable methods.","{'paper_name': 'SteadyDancer: Harmonized and Coherent Human Image Animation with First-Frame Preservation', 'github_url': 'https://github.com/MCG-NJU/SteadyDancer', 'huggingface_url': 'https://huggingface.co/papers/2511.19320', 'upvote': 39, 'tags': ['image', 'motion', 'control']}"
doc2548019,"Diffusion transformers have recently delivered strong text-to-image generation around 1K resolution, but we show that extending them to native 4K across diverse aspect ratios exposes a tightly coupled failure mode spanning positional encoding , VAE compression , and optimization. Tackling any of these factors in isolation leaves substantial quality on the table. We therefore take a data-model co-design view and introduce UltraFlux, a Flux-based DiT trained natively at 4K on MultiAspect-4K-1M, a 1M-image 4K corpus with controlled multi-AR coverage, bilingual captions, and rich VLM/IQA metadata for resolution- and AR-aware sampling. On the model side, UltraFlux couples (i) Resonance 2D RoPE with YaRN for training-window-, frequency-, and AR-aware positional encoding at 4K; (ii) a simple, non-adversarial VAE post-training scheme that improves 4K reconstruction fidelity; (iii) an SNR-Aware Huber Wavelet objective that rebalances gradients across timesteps and frequency bands; and (iv) a Stage-wise Aesthetic Curriculum Learning strategy that concentrates high-aesthetic supervision on high-noise steps governed by the model prior. Together, these components yield a stable, detail-preserving 4K DiT that generalizes across wide, square, and tall ARs. On the Aesthetic-Eval at 4096 benchmark and multi-AR 4K settings, UltraFlux consistently outperforms strong open-source baselines across fidelity, aesthetic, and alignment metrics, and-with a LLM prompt refiner-matches or surpasses the proprietary Seedream 4.0 .","{'paper_name': 'UltraFlux: Data-Model Co-Design for High-quality Native 4K Text-to-Image Generation across Diverse Aspect Ratios', 'github_url': 'https://github.com/W2GenAI-Lab/UltraFlux', 'huggingface_url': 'https://huggingface.co/papers/2511.18050', 'upvote': 37, 'tags': ['4k', 'ar', 'aesthetic']}"
doc2548020,"Reinforcement learning (RL) plays an increasingly important role in enhancing the reasoning capabilities of large language models (LLMs), yet stable and performant policy optimization remains challenging. Token-level importance ratios often exhibit high variance-a phenomenon exacerbated in Mixture-of-Experts models -leading to unstable updates. Existing group-based policy optimization methods, such as GSPO and GRPO , alleviate this problem via hard clipping, making it difficult to maintain both stability and effective learning. We propose Soft Adaptive Policy Optimization (SAPO), which replaces hard clipping with a smooth, temperature-controlled gate that adaptively attenuates off-policy updates while preserving useful learning signals. Compared with GSPO and GRPO , SAPO is both sequence-coherent and token-adaptive . Like GSPO , SAPO maintains sequence-level coherence , but its soft gating forms a continuous trust region that avoids the brittle hard clipping band used in GSPO . When a sequence contains a few highly off-policy tokens, GSPO suppresses all gradients for that sequence, whereas SAPO selectively down-weights only the offending tokens and preserves the learning signal from the near-on-policy ones, improving sample efficiency . Relative to GRPO , SAPO replaces hard token-level clipping with smooth, temperature-controlled scaling, enabling more informative and stable updates. Empirical results on mathematical reasoning benchmarks indicate that SAPO exhibits improved training stability and higher Pass@1 performance under comparable training budgets. Moreover, we employ SAPO to train the Qwen3-VL model series , demonstrating that SAPO yields consistent performance gains across diverse tasks and different model sizes. Overall, SAPO provides a more reliable, scalable, and effective optimization strategy for RL training of LLMs.","{'paper_name': 'Soft Adaptive Policy Optimization', 'github_url': 'https://github.com/modelscope/ms-swift/tree/main', 'huggingface_url': 'https://huggingface.co/papers/2511.20347', 'upvote': 33, 'tags': ['sapo', 'policy', 'gspo']}"
doc2548021,"Pre-trained video models learn powerful priors for generating high-quality, temporally coherent content. While these models excel at temporal coherence , their dynamics are often constrained by the continuous nature of their training data. We hypothesize that by injecting the rich and unconstrained content diversity from image data into this coherent temporal framework, we can generate image sets that feature both natural transitions and a far more expansive dynamic range. To this end, we introduce iMontage , a unified framework designed to repurpose a powerful video model into an all-in-one image generator . The framework consumes and produces variable-length image sets , unifying a wide array of image generation and editing tasks. To achieve this, we propose an elegant and minimally invasive adaptation strategy , complemented by a tailored data curation process and training paradigm . This approach allows the model to acquire broad image manipulation capabilities without corrupting its invaluable original motion priors . iMontage excels across several mainstream many-in-many-out tasks, not only maintaining strong cross-image contextual consistency but also generating scenes with extraordinary dynamics that surpass conventional scopes. Find our homepage at: https://kr1sjfu.github.io/ iMontage -web/.","{'paper_name': 'iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation', 'github_url': 'https://github.com/Kr1sJFU/iMontage', 'huggingface_url': 'https://huggingface.co/papers/2511.20635', 'upvote': 31, 'tags': ['image', 'imontage', 'framework']}"
doc2548022,"Recent years have witnessed significant progress in Unified Multimodal Models , yet a fundamental question remains: Does understanding truly inform generation? To investigate this, we introduce UniSandbox , a decoupled evaluation framework paired with controlled, synthetic datasets to avoid data leakage and enable detailed analysis. Our findings reveal a significant understanding-generation gap , which is mainly reflected in two key dimensions: reasoning generation and knowledge transfer . Specifically, for reasoning generation tasks, we observe that explicit Chain-of-Thought (CoT) in the understanding module effectively bridges the gap, and further demonstrate that a self-training approach can successfully internalize this ability, enabling implicit reasoning during generation. Additionally, for knowledge transfer tasks, we find that CoT assists the generative process by helping retrieve newly learned knowledge, and also discover that query-based architectures inherently exhibit latent CoT-like properties that affect this transfer. UniSandbox provides preliminary insights for designing future unified architectures and training strategies that truly bridge the gap between understanding and generation. Code and data are available at https://github.com/PKU-YuanGroup/ UniSandBox","{'paper_name': 'Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward', 'github_url': 'https://github.com/PKU-YuanGroup/UniSandBox', 'huggingface_url': 'https://huggingface.co/papers/2511.20561', 'upvote': 31, 'tags': ['generation', 'understanding', 'transfer']}"
doc2548023,"World models are emerging as a foundational paradigm for scalable, data-efficient embodied AI. In this work, we present GigaWorld-0 , a unified world model framework designed explicitly as a data engine for Vision-Language-Action (VLA) learning. GigaWorld-0 integrates two synergistic components: GigaWorld-0-Video , which leverages large-scale video generation to produce diverse, texture-rich, and temporally coherent embodied sequences under fine-grained control of appearance, camera viewpoint, and action semantics; and GigaWorld-0-3D , which combines 3D generative modeling , 3D Gaussian Splatting reconstruction, physically differentiable system identification , and executable motion planning to ensure geometric consistency and physical realism. Their joint optimization enables the scalable synthesis of embodied interaction data that is visually compelling, spatially coherent, physically plausible, and instruction-aligned. Training at scale is made feasible through our efficient GigaTrain framework, which exploits FP8-precision and sparse attention to drastically reduce memory and compute requirements. We conduct comprehensive evaluations showing that GigaWorld-0 generates high-quality, diverse, and controllable data across multiple dimensions. Critically, VLA model (e.g., GigaBrain-0 ) trained on GigaWorld-0 -generated data achieve strong real-world performance , significantly improving generalization and task success on physical robots without any real-world interaction during training.","{'paper_name': 'GigaWorld-0: World Models as Data Engine to Empower Embodied AI', 'github_url': 'https://github.com/open-gigaai/giga-world-0', 'huggingface_url': 'https://huggingface.co/papers/2511.19861', 'upvote': 29, 'tags': ['gigaworld', 'data', 'world']}"
doc2548024,"Large-scale video generative models have recently demonstrated strong visual capabilities, enabling the prediction of future frames that adhere to the logical and physical cues in the current observation. In this work, we investigate whether such capabilities can be harnessed for controllable image-to-video generation by interpreting visual signals embedded within the frames as instructions, a paradigm we term In-Video Instruction . In contrast to prompt-based control , which provides textual descriptions that are inherently global and coarse, In-Video Instruction encodes user guidance directly into the visual domain through elements such as overlaid text , arrows , or trajectories . This enables explicit, spatial-aware , and unambiguous correspondences between visual subjects and their intended actions by assigning distinct instructions to different objects. Extensive experiments on three state-of-the-art generators, including Veo 3.1 , Kling 2.5 , and Wan 2.2 , show that video models can reliably interpret and execute such visually embedded instructions, particularly in complex multi-object scenarios .","{'paper_name': 'In-Video Instructions: Visual Signals as Generative Control', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.19401', 'upvote': 28, 'tags': ['video', 'visual', 'instructions']}"
doc2548025,"While modern diffusion models excel at generating high-quality and diverse images, they still struggle with high-fidelity compositional and multimodal control , particularly when users simultaneously specify text prompts , subject references , spatial arrangements , pose constraints , and layout annotations . We introduce Canvas-to-Image, a unified framework that consolidates these heterogeneous controls into a single canvas interface, enabling users to generate images that faithfully reflect their intent. Our key idea is to encode diverse control signals into a single composite canvas image that the model can directly interpret for integrated visual-spatial reasoning . We further curate a suite of multi-task datasets and propose a Multi-Task Canvas Training strategy that optimizes the diffusion model to jointly understand and integrate heterogeneous controls into text-to-image generation within a unified learning paradigm. This joint training enables Canvas-to-Image to reason across multiple control modalities rather than relying on task-specific heuristics, and it generalizes well to multi-control scenarios during inference. Extensive experiments show that Canvas-to-Image significantly outperforms state-of-the-art methods in identity preservation and control adherence across challenging benchmarks, including multi-person composition , pose-controlled composition , layout-constrained generation , and multi-control generation .","{'paper_name': 'Canvas-to-Image: Compositional Image Generation with Multimodal Controls', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.21691', 'upvote': 27, 'tags': ['control', 'canvas', 'image']}"
doc2548026,"Normalizing flows ( NFs ) are end-to-end likelihood-based generative models for continuous data, and have recently regained attention with encouraging progress on image generation. Yet in the video generation domain, where spatiotemporal complexity and computational cost are substantially higher, state-of-the-art systems almost exclusively rely on diffusion-based models. In this work, we revisit this design space by presenting STARFlow-V, a normalizing flow-based video generator with substantial benefits such as end-to-end learning, robust causal prediction, and native likelihood estimation. Building upon the recently proposed STARFlow, STARFlow-V operates in the spatiotemporal latent space with a global-local architecture which restricts causal dependencies to a global latent space while preserving rich local within-frame interactions. This eases error accumulation over time, a common pitfall of standard autoregressive diffusion model generation. Additionally, we propose flow-score matching , which equips the model with a light-weight causal denoiser to improve the video generation consistency in an autoregressive fashion. To improve the sampling efficiency, STARFlow-V employs a video-aware Jacobi iteration scheme that recasts inner updates as parallelizable iterations without breaking causality. Thanks to the invertible structure, the same model can natively support text-to-video , image-to-video as well as video-to-video generation tasks. Empirically, STARFlow-V achieves strong visual fidelity and temporal consistency with practical sampling throughput relative to diffusion-based baselines. These results present the first evidence, to our knowledge, that NFs are capable of high-quality autoregressive video generation , establishing them as a promising research direction for building world models . Code and generated samples are available at https://github.com/apple/ml-starflow.","{'paper_name': 'STARFlow-V: End-to-End Video Generative Modeling with Normalizing Flow', 'github_url': 'https://github.com/apple/ml-starflow', 'huggingface_url': 'https://huggingface.co/papers/2511.20462', 'upvote': 26, 'tags': ['video', 'starflow', 'generation']}"
doc2548027,"A reliable reward function is essential for reinforcement learning (RL) in image generation. Most current RL approaches depend on pre-trained preference models that output scalar rewards to approximate human preferences. However, these rewards often fail to capture human perception and are vulnerable to reward hacking , where higher scores do not correspond to better images. To address this, we introduce Adv-GRPO , an RL framework with an adversarial reward that iteratively updates both the reward model and the generator . The reward model is supervised using reference images as positive samples and can largely avoid being hacked. Unlike KL regularization that constrains parameter updates, our learned reward directly guides the generator through its visual outputs, leading to higher-quality images. Moreover, while optimizing existing reward function s can alleviate reward hacking , their inherent biases remain. For instance, PickScore may degrade image quality , whereas OCR-based rewards often reduce aesthetic fidelity. To address this, we take the image itself as a reward, using reference images and vision foundation models (e.g., DINO ) to provide rich visual rewards. These dense visual signals, instead of a single scalar, lead to consistent gains across image quality , aesthetics , and task-specific metrics . Finally, we show that combining reference samples with foundation-model rewards enables distribution transfer and flexible style customization . In human evaluation, our method outperforms Flow-GRPO and SD3, achieving 70.0% and 72.4% win rates in image quality and aesthetics , respectively. Code and models have been released.","{'paper_name': 'The Image as Its Own Reward: Reinforcement Learning with Adversarial Reward for Image Generation', 'github_url': 'https://github.com/showlab/Adv-GRPO', 'huggingface_url': 'https://huggingface.co/papers/2511.20256', 'upvote': 26, 'tags': ['reward', 'rewards', 'image']}"
doc2548028,"The quadratic complexity of full attention limits efficient long-context processing in large language models (LLMs). Sparse attention mitigates this cost by restricting each query to attend to a subset of previous tokens; however, training-free approaches often lead to severe performance degradation. Native sparse-attention methods (e.g., NSA , MoBA ) alleviate this issue, yet exhibit a critical paradox: they produce lower attention sparsity than full-attention models, despite aiming to approximate full attention , which may constrain their effectiveness. We attribute this paradox to gradient update deficiency : low-ranked key-value pairs excluded during sparse training receive neither forward contribution nor backward gradients, and thus never learn proper suppression. To overcome this limitation, we propose SSA ( Sparse Sparse Attention ), a unified training framework that considers both sparse and full attention and enforces bidirectional alignment at every layer. This design preserves gradient flow to all tokens while explicitly encouraging sparse-attention outputs to align with their full-attention counterparts, thereby promoting stronger sparsity. As a result, SSA achieves state-of-the-art performance under both sparse and full attention inference across multiple commonsense benchmarks. Furthermore, SSA enables models to adapt smoothly to varying sparsity budgets; performance improves consistently as more tokens are allowed to attend, supporting flexible compute-performance trade-offs at inference time. Finally, we show that native sparse-attention training surprisingly improves long-context extrapolation by mitigating the over-allocation of attention values in sink areas , with SSA demonstrating the strongest extrapolation capability.","{'paper_name': 'SSA: Sparse Sparse Attention by Aligning Full and Sparse Attention Outputs in Feature Space', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.20102', 'upvote': 25, 'tags': ['attention', 'sparse', 'sparse attention']}"
doc2548029,"Vision-Language Models (VLMs) excel at reasoning in linguistic space but struggle with perceptual understanding that requires dense visual perception, e.g., spatial reasoning and geometric awareness. This limitation stems from the fact that current VLMs have limited mechanisms to capture dense visual information across spatial dimensions . We introduce Chain-of-Visual-Thought ( COVT ), a framework that enables VLMs to reason not only in words but also through continuous visual tokens -compact latent representations that encode rich perceptual cues. Within a small budget of roughly 20 tokens, COVT distills knowledge from lightweight vision experts , capturing complementary properties such as 2D appearance , 3D geometry , spatial layout , and edge structure . During training, the VLM with COVT autoregressively predicts these visual tokens to reconstruct dense supervision signals (e.g., depth , segmentation , edges , and DINO features ). At inference, the model reasons directly in the continuous visual token space, preserving efficiency while optionally decoding dense predictions for interpretability. Evaluated across more than ten diverse perception benchmarks, including CV-Bench , MMVP , RealWorldQA , MMStar , WorldMedQA , and HRBench , integrating COVT into strong VLMs such as Qwen2.5-VL and LLaVA consistently improves performance by 3% to 16% and demonstrates that compact continuous visual thinking enables more precise, grounded, and interpretable multimodal intelligence .","{'paper_name': 'Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.19418', 'upvote': 25, 'tags': ['visual', 'vlms', 'covt']}"
doc2548030,"We introduce RynnVLA-002, a unified Vision-Language-Action (VLA) and world model . The world model leverages action and visual inputs to predict future image states , learning the underlying physics of the environment to refine action generation . Conversely, the VLA model produces subsequent actions from image observations, enhancing visual understanding and supporting the world model 's image generation. The unified framework of RynnVLA-002 enables joint learning of environmental dynamics and action planning . Our experiments show that RynnVLA-002 surpasses individual VLA and world model s, demonstrating their mutual enhancement. We evaluate RynnVLA-002 in both simulation and real-world robot tasks. RynnVLA-002 achieves 97.4% success rate on the LIBERO simulation benchmark without pretraining, while in real-world LeRobot experiments , its integrated world model boosts the overall success rate by 50%.","{'paper_name': 'RynnVLA-002: A Unified Vision-Language-Action and World Model', 'github_url': 'https://github.com/alibaba-damo-academy/RynnVLA-002', 'huggingface_url': 'https://huggingface.co/papers/2511.17502', 'upvote': 24, 'tags': ['world', 'model', 'rynnvla']}"
doc2548031,"Scaling test-time computation improves performance across different tasks on large language models (LLMs), which has also been extended to tool-augmented agents. For these agents, scaling involves not only ""thinking"" in tokens but also ""acting"" via tool calls. The number of tool calls directly bounds the agent's interaction with the external environment. However, we find that simply granting agents a larger tool-call budget fails to improve performance, as they lack ""budget awareness"" and quickly hit a performance ceiling. To address this, we study how to scale such agents effectively under explicit tool-call budgets, focusing on web search agents. We first introduce the Budget Tracker , a lightweight plug-in that provides the agent with continuous budget awareness, enabling simple yet effective scaling. We further develop BATS ( Budget Aware Test-time Scaling ), an advanced framework that leverages this awareness to dynamically adapt its planning and verification strategy, deciding whether to ""dig deeper"" on a promising lead or ""pivot"" to new paths based on remaining resources. To analyze cost-performance scaling in a controlled manner, we formalize a unified cost metric that jointly accounts for token and tool consumption . We provide the first systematic study on budget-constrained agents, showing that budget-aware methods produce more favorable scaling curves and push the cost-performance Pareto frontier . Our work offers empirical insights toward a more transparent and principled understanding of scaling in tool-augmented agents.","{'paper_name': 'Budget-Aware Tool-Use Enables Effective Agent Scaling', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.17006', 'upvote': 24, 'tags': ['tool', 'scaling', 'agents']}"
doc2548032,"Recent advancements in LLM-powered agents have demonstrated significant potential in generating human-like responses; however, they continue to face challenges in maintaining long-term interactions within complex environments, primarily due to limitations in contextual consistency and dynamic personalization . Existing memory systems often depend on semantic grouping prior to retrieval, which can overlook semantically irrelevant yet critical user information and introduce retrieval noise. In this report, we propose the initial design of O-Mem, a novel memory framework based on active user profiling that dynamically extracts and updates user characteristics and event records from their proactive interactions with agents. O-Mem supports hierarchical retrieval of persona attributes and topic-related context , enabling more adaptive and coherent personalized responses. O-Mem achieves 51.67% on the public LoCoMo benchmark , a nearly 3% improvement upon LangMem ,the previous state-of-the-art, and it achieves 62.99% on PERSONAMEM , a 3.5% improvement upon A-Mem ,the previous state-of-the-art. O-Mem also boosts token and interaction response time efficiency compared to previous memory framework s. Our work opens up promising directions for developing efficient and human-like personalized AI assistants in the future.","{'paper_name': 'O-Mem: Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.13593', 'upvote': 24, 'tags': ['mem', 'user', 'previous']}"
doc2548033,"We present HunyuanVideo 1.5, a lightweight yet powerful open-source video generation model that achieves state-of-the-art visual quality and motion coherence with only 8.3 billion parameters, enabling efficient inference on consumer-grade GPUs. This achievement is built upon several key components, including meticulous data curation, an advanced DiT architecture featuring selective and sliding tile attention ( SSTA ), enhanced bilingual understanding through glyph-aware text encoding , progressive pre-training and post-training , and an efficient video super-resolution network . Leveraging these designs, we developed a unified framework capable of high-quality text-to-video and image-to-video generation across multiple durations and resolutions.Extensive experiments demonstrate that this compact and proficient model establishes a new state-of-the-art among open-source video generation models. By releasing the code and model weights, we provide the community with a high-performance foundation that lowers the barrier to video creation and research, making advanced video generation accessible to a broader audience. All open-source assets are publicly available at https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5.","{'paper_name': 'HunyuanVideo 1.5 Technical Report', 'github_url': 'https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5', 'huggingface_url': 'https://huggingface.co/papers/2511.18870', 'upvote': 21, 'tags': ['video', 'video generation', 'generation']}"
doc2548034,"Understanding text-rich videos requires reading small, transient textual cues that often demand repeated inspection. Yet most video QA models rely on single-pass perception over fixed frames , leading to hallucinations and failures on fine-grained evidence. Inspired by how humans pause, zoom, and re-read critical regions, we introduce Video-R4 (Reinforcing Text-Rich Video Reasoning with Visual Rumination ), a video reasoning LMM that performs visual rumination : iteratively selecting frames , zooming into informative regions, re-encoding retrieved pixels, and updating its reasoning state . We construct two datasets with executable rumination trajectories: Video-R4-CoT-17k for supervised practice and Video-R4-RL-30k for reinforcement learning. We propose a multi-stage rumination learning framework that progressively finetunes a 7B LMM to learn atomic and mixing visual operations via SFT and GRPO-based RL . Video-R4 -7B achieves state-of-the-art results on M4-ViteVQA and further generalizes to multi-page document QA , slides QA , and generic video QA , demonstrating that iterative rumination is an effective paradigm for pixel-grounded multimodal reasoning.","{'paper_name': 'Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination', 'github_url': 'https://github.com/yunlong10/Video-R4', 'huggingface_url': 'https://huggingface.co/papers/2511.17490', 'upvote': 21, 'tags': ['video', 'rumination', 'video r4']}"
doc2548035,"The synthesis of synchronized audio-visual content is a key challenge in generative AI, with open-source models facing challenges in robust audio-video alignment. Our analysis reveals that this issue is rooted in three fundamental challenges of the joint diffusion process : (1) Correspondence Drift , where concurrently evolving noisy latents impede stable learning of alignment; (2) inefficient global attention mechanisms that fail to capture fine-grained temporal cues ; and (3) the intra-modal bias of conventional Classifier-Free Guidance (CFG), which enhances conditionality but not cross-modal synchronization. To overcome these challenges, we introduce Harmony, a novel framework that mechanistically enforces audio-visual synchronization. We first propose a Cross-Task Synergy training paradigm to mitigate drift by leveraging strong supervisory signals from audio-driven video and video-driven audio generation tasks. Then, we design a Global-Local Decoupled Interaction Module for efficient and precise temporal-style alignment. Finally, we present a novel Synchronization-Enhanced CFG (SyncCFG) that explicitly isolates and amplifies the alignment signal during inference. Extensive experiments demonstrate that Harmony establishes a new state-of-the-art, significantly outperforming existing methods in both generation fidelity and, critically, in achieving fine-grained audio-visual synchronization.","{'paper_name': 'Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy', 'github_url': 'https://github.com/sjtuplayer/Harmony', 'huggingface_url': 'https://huggingface.co/papers/2511.21579', 'upvote': 20, 'tags': ['audio', 'synchronization', 'alignment']}"
doc2548036,"We introduce Nemotron-Parse-1.1, a lightweight document parsing and OCR model that advances the capabilities of its predecessor, Nemoretriever-Parse-1.0. Nemotron-Parse-1.1 delivers improved capabilities across general OCR , markdown formatting , structured table parsing , and text extraction from pictures, charts, and diagrams. It also supports a longer output sequence length for visually dense documents. As with its predecessor, it extracts bounding boxes of text segments, as well as corresponding semantic classes . Nemotron-Parse-1.1 follows an encoder-decoder architecture with 885M parameters, including a compact 256M-parameter language decoder. It achieves competitive accuracy on public benchmarks making it a strong lightweight OCR solution. We release the model weights publicly on Huggingface , as well as an optimized NIM container , along with a subset of the training data as part of the broader Nemotron-VLM-v2 dataset. Additionally, we release Nemotron-Parse-1.1-TC which operates on a reduced vision token length, offering a 20% speed improvement with minimal quality degradation.","{'paper_name': 'NVIDIA Nemotron Parse 1.1', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.20478', 'upvote': 20, 'tags': ['nemotron', 'parse', 'nemotron parse']}"
doc2548037,"We introduce WorldGen, a system that enables the automatic creation of large-scale, interactive 3D worlds directly from text prompts. Our approach transforms natural language descriptions into traversable, fully textured environments that can be immediately explored or edited within standard game engines. By combining LLM-driven scene layout reasoning , procedural generation , diffusion-based 3D generation , and object-aware scene decomposition , WorldGen bridges the gap between creative intent and functional virtual spaces, allowing creators to design coherent, navigable worlds without manual modeling or specialized 3D expertise. The system is fully modular and supports fine-grained control over layout, scale, and style, producing worlds that are geometrically consistent, visually rich, and efficient to render in real time. This work represents a step towards accessible, generative world-building at scale, advancing the frontier of 3D generative AI for applications in gaming, simulation, and immersive social environments.","{'paper_name': 'WorldGen: From Text to Traversable and Interactive 3D Worlds', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.16825', 'upvote': 20, 'tags': ['3d', 'worlds', 'scale']}"
doc2548038,"Unified Multimodal Models (UMMs) have shown impressive performance in both understanding and generation with a single architecture. However, UMMs still exhibit a fundamental inconsistency: understanding favors compact embeddings , whereas generation favors reconstruction-rich representations . This structural trade-off produces misaligned decision boundaries , degraded cross-modal coherence , and heightened vulnerability under distributional and adversarial shifts . In this paper, we present UniGame, a self-adversarial post-training framework that directly targets the inconsistencies. By applying a lightweight perturber at the shared token interface, UniGame enables the generation branch to actively seek and challenge fragile understanding, turning the model itself into its own adversary. Experiments demonstrate that UniGame significantly improves the consistency (+4.6%). Moreover, it also achieves substantial improvements in understanding (+3.6%), generation (+0.02), out-of-distribution and adversarial robustness (+4.8% and +6.2% on NaturalBench and AdVQA). The framework is architecture-agnostic, introduces less than 1% additional parameters, and is complementary to existing post-training methods. These results position adversarial self-play as a general and effective principle for enhancing the coherence, stability, and unified competence of future multimodal foundation models . The official code is available at: https://github.com/AIFrontierLab/UniGame","{'paper_name': 'UniGame: Turning a Unified Multimodal Model Into Its Own Adversary', 'github_url': 'https://github.com/AIFrontierLab/UniGame', 'huggingface_url': 'https://huggingface.co/papers/2511.19413', 'upvote': 19, 'tags': ['unigame', 'understanding', 'generation']}"
doc2548039,"This paper presents HunyuanOCR, a commercial-grade, open-source, and lightweight (1B parameters) Vision-Language Model (VLM) dedicated to OCR tasks. The architecture comprises a Native Vision Transformer (ViT) and a lightweight LLM connected via an MLP adapter . HunyuanOCR demonstrates superior performance, outperforming commercial APIs, traditional pipelines, and larger models (e.g., Qwen3-VL-4B). Specifically, it surpasses current public solutions in perception tasks ( Text Spotting , Parsing ) and excels in semantic tasks (IE, Text Image Translation ), securing first place in the ICDAR 2025 DIMT Challenge (Small Model Track). Furthermore, it achieves state-of-the-art (SOTA) results on OCRBench among VLMs with fewer than 3B parameters.
  HunyuanOCR achieves breakthroughs in three key aspects: 1) Unifying Versatility and Efficiency: We implement comprehensive support for core capabilities including spotting, parsing , IE, VQA , and translation within a lightweight framework. This addresses the limitations of narrow ""OCR expert models"" and inefficient ""General VLMs"". 2) Streamlined End-to-End Architecture : Adopting a pure end-to-end paradigm eliminates dependencies on pre-processing modules (e.g., layout analysis). This fundamentally resolves error propagation common in traditional pipelines and simplifies system deployment. 3) Data-Driven and RL Strategies: We confirm the critical role of high-quality data and, for the first time in the industry, demonstrate that Reinforcement Learning (RL) strategies yield significant performance gains in OCR tasks.
  HunyuanOCR is officially open-sourced on HuggingFace. We also provide a high-performance deployment solution based on vLLM, placing its production efficiency in the top tier. We hope this model will advance frontier research and provide a solid foundation for industrial applications.","{'paper_name': 'HunyuanOCR Technical Report', 'github_url': 'https://github.com/Tencent-Hunyuan/HunyuanOCR', 'huggingface_url': 'https://huggingface.co/papers/2511.19575', 'upvote': 19, 'tags': ['tasks', 'hunyuanocr', 'end']}"
doc2548040,"Radiology plays an integral role in modern medicine, yet rising imaging volumes have far outpaced workforce growth. Foundation models offer a path toward assisting with the full spectrum of radiology tasks, but existing medical models remain limited: they process volumetric CT and MRI as low-fidelity 2D slices, discard critical grayscale contrast information, and lack evaluation frameworks that reflect real clinical practice. We introduce Pillar-0, a radiology foundation model pretrained on 42,990 abdomen-pelvis CTs, 86,411 chest CTs, 14,348 head CTs, and 11,543 breast MRI s from a large academic center, together with RATE , a scalable framework that extracts structured labels for 366 radiologic findings with near-perfect accuracy using LLMs. Across internal test sets of 14,230 abdomen-pelvis CTs, 10,646 chest CTs, 4,906 head CTs, and 1,585 breast MRI s, Pillar-0 establishes a new performance frontier, achieving mean AUROC s of 86.4, 88.0, 90.1, and 82.9, outperforming MedGemma (Google), MedImageInsight (Microsoft), Lingshu (Alibaba), and Merlin (Stanford) by 7.8-15.8 AUROC points and ranking best in 87.2\% (319/366) tasks. Pillar-0 similarly outperforms all baselines in an external validation on the Stanford Abdominal CT dataset , including Merlin (82.2 vs 80.6 AUROC ). Pillar-0 extends to tasks beyond its pretraining, such as long-horizon lung cancer risk prediction , where it improves upon the state-of-the-art Sybil by 3.0 C-index points on NLST, and generalizes with gains of 5.9 (MGH) and 1.9 (CGMH). In brain hemorrhage detection , Pillar-0 obtained a >95 AUROC when using only 1/20th of the data of the next most sample efficient baseline. Pillar-0 and RATE together provide an open , clinically rigorous foundation for building high-performance radiology systems, enabling applications that were previously infeasible due to computational, data, and evaluation constraints.","{'paper_name': 'Pillar-0: A New Frontier for Radiology Foundation Models', 'github_url': 'https://github.com/YalaLab/pillar-pretrain', 'huggingface_url': 'https://huggingface.co/papers/2511.17803', 'upvote': 19, 'tags': ['cts', 'pillar', 'radiology']}"
doc2548041,"Recent interactive video world model methods generate scene evolution conditioned on user instructions. Although they achieve impressive results, two key limitations remain. First, they fail to fully exploit the correspondence between instruction-driven scene motion and the underlying 3D geometry, which results in structural instability under viewpoint changes. Second, they easily forget historical information during multi-step interaction, resulting in error accumulation and progressive drift in scene semantics and structure. To address these issues, we propose MagicWorld, an interactive video world model that integrates 3D geometric priors and historical retrieval . MagicWorld starts from a single scene image, employs user actions to drive dynamic scene evolution , and autoregressively synthesizes continuous scenes. We introduce the Action-Guided 3D Geometry Module ( AG3D ), which constructs a point cloud from the first frame of each interaction and the corresponding action, providing explicit geometric constraints for viewpoint transitions and thereby improving structural consistency. We further propose History Cache Retrieval ( HCR ) mechanism, which retrieves relevant historical frames during generation and injects them as conditioning signals, helping the model utilize past scene information and mitigate error accumulation. Experimental results demonstrate that MagicWorld achieves notable improvements in scene stability and continuity across interaction iterations.","{'paper_name': 'MagicWorld: Interactive Geometry-driven Video World Exploration', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.18886', 'upvote': 17, 'tags': ['scene', 'model', 'results']}"
doc2548042,"Multi-agent systems perform well on general reasoning tasks. However, the lack of training in specialized areas hinders their accuracy. Current training methods train a unified large language model (LLM) for all agents in the system. This may limit the performances due to different distributions underlying for different agents. Therefore, training multi-agent systems with distinct LLMs should be the next step to solve. However, this approach introduces optimization challenges. For example, agents operate at different frequencies, rollouts involve varying sub-agent invocations, and agents are often deployed across separate servers, disrupting end-to-end gradient flow. To address these issues, we propose M-GRPO, a hierarchical extension of Group Relative Policy Optimization designed for vertical Multi-agent systems with a main agent (planner) and multiple sub-agents (multi-turn tool executors). M-GRPO computes group-relative advantages for both main and sub-agents, maintaining hierarchical credit assignment . It also introduces a trajectory-alignment scheme that generates fixed-size batches despite variable sub-agent invocations. We deploy a decoupled training pipeline in which agents run on separate servers and exchange minimal statistics via a shared store . This enables scalable training without cross-server backpropagation . In experiments on real-world benchmarks (e.g., GAIA, XBench-DeepSearch, and WebWalkerQA), M-GRPO consistently outperforms both single-agent GRPO and multi-agent GRPO with frozen sub-agents, demonstrating improved stability and sample efficiency. These results show that aligning heterogeneous trajectories and decoupling optimization across specialized agents enhances tool-augmented reasoning tasks .","{'paper_name': 'Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.13288', 'upvote': 17, 'tags': ['agents', 'agent', 'sub']}"
doc2548043,"Diffusion Transformers have demonstrated remarkable capabilities in visual synthesis , yet they often struggle with high-level semantic reasoning and long-horizon planning . This limitation frequently leads to visual hallucinations and mis-alignments with user instructions, especially in scenarios involving complex scene understanding, human-object interactions, multi-stage actions, and in-context motion reasoning. To address these challenges, we propose Plan-X, a framework that explicitly enforces high-level semantic planning to instruct video generation process. At its core lies a Semantic Planner , a learnable multimodal language model that reasons over the user's intent from both text prompts and visual context, and autoregressively generates a sequence of text-grounded spatio-temporal semantic tokens . These semantic tokens, complementary to high-level text prompt guidance, serve as structured ""semantic sketches"" over time for the video diffusion model , which has its strength at synthesizing high-fidelity visual details. Plan-X effectively integrates the strength of language models in multimodal in-context reasoning and planning, together with the strength of diffusion models in photorealistic video synthesis . Extensive experiments demonstrate that our framework substantially reduces visual hallucinations and enables fine-grained, instruction-aligned video generation consistent with multimodal context.","{'paper_name': 'Plan-X: Instruct Video Generation via Semantic Planning', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.17986', 'upvote': 16, 'tags': ['semantic', 'visual', 'video']}"
doc2548044,"We present M^3-Bench, the first benchmark for evaluating multimodal tool use under the Model Context Protocol . The benchmark targets realistic, multi-hop and multi-threaded workflows that require visual grounding and textual reasoning , cross-tool dependencies , and persistence of intermediate resources across steps. We introduce a similarity-driven alignment that serializes each tool call, embeds signatures with a sentence encoder , and performs similarity-bucketed Hungarian matching to obtain auditable one-to-one correspondences. On top of this alignment, we report interpretable metrics that decouple semantic fidelity from workflow consistency . The benchmark spans 28 servers with 231 tools, and provides standardized trajectories curated through an Executor & Judge pipeline with human verification; an auxiliary four large language models (LLMs) judge ensemble reports end-task Task Completion and information grounding. Evaluations of representative state-of-the-art Multimodal LLMs (MLLMs) reveal persistent gaps in multimodal MCP tool use, particularly in argument fidelity and structure consistency , underscoring the need for methods that jointly reason over images, text, and tool graphs . Our Benchmark's anonymous repository is at https://github.com/EtaYang10th/Open-M3-Bench","{'paper_name': 'M3-Bench: Multi-Modal, Multi-Hop, Multi-Threaded Tool-Using MLLM Agent Benchmark', 'github_url': 'https://github.com/EtaYang10th/Open-M3-Bench', 'huggingface_url': 'https://huggingface.co/papers/2511.17729', 'upvote': 16, 'tags': ['tool', 'benchmark', 'multimodal']}"
doc2548045,"We investigate how well large language models ( LLMs ) generalize across different task difficulties , a key question for effective data curation and evaluation . Existing research is mixed regarding whether training on easier or harder data leads to better results, and whether those gains come on easier or harder test data. We address this question by conducting a systematic evaluation of LLMs ' generalization across models, datasets, and fine-grained groups of example difficulty. We rank examples in six datasets using the outputs of thousands of different LLMs and Item Response Theory ( IRT ), a well-established difficulty metric in educational testing. Unlike prior work, our difficulty ratings are therefore determined solely by the abilities of many different LLMs , excluding human opinions of difficulty. With a more objective, larger-scale, and finer-grained analysis, we show that cross-difficulty generalization is often limited; training on either easy or hard data cannot achieve consistent improvements across the full range of difficulties. These results show the importance of having a range of difficulties in both training and evaluation data for LLMs , and that taking shortcuts with respect to difficulty is risky.","{'paper_name': ""Revisiting Generalization Across Difficulty Levels: It's Not So Easy"", 'github_url': 'https://github.com/BatsResearch/Cross-Difficulty', 'huggingface_url': 'https://huggingface.co/papers/2511.21692', 'upvote': 15, 'tags': ['difficulty', 'llms', 'data']}"
doc2548046,"Despite advances, video diffusion transformers still struggle to generalize beyond their training length, a challenge we term video length extrapolation . We identify two failure modes: model-specific periodic content repetition and a universal quality degradation. Prior works attempt to solve repetition via positional encodings , overlooking quality degradation and achieving only limited extrapolation. In this paper, we revisit this challenge from a more fundamental view: attention maps , which directly govern how context influences outputs. We identify that both failure modes arise from a unified cause: attention dispersion , where tokens beyond the training window dilute learned attention patterns. This leads to quality degradation and repetition emerges as a special case when this dispersion becomes structured into periodic attention patterns, induced by harmonic properties of positional encodings . Building on this insight, we propose UltraViCo , a training-free, plug-and-play method that suppresses attention for tokens beyond the training window via a constant decay factor. By jointly addressing both failure modes, we outperform a broad set of baselines largely across models and extrapolation ratios, pushing the extrapolation limit from 2x to 4x. Remarkably, it improves Dynamic Degree and Imaging Quality by 233% and 40.5% over the previous best method at 4x extrapolation. Furthermore, our method generalizes seamlessly to downstream tasks such as controllable video synthesis and editing .","{'paper_name': 'UltraViCo: Breaking Extrapolation Limits in Video Diffusion Transformers', 'github_url': 'https://github.com/thu-ml/DiT-Extrapolation', 'huggingface_url': 'https://huggingface.co/papers/2511.20123', 'upvote': 16, 'tags': ['attention', 'extrapolation', 'training']}"
doc2548047,"Step-by-step painting tutorials are vital for learning artistic techniques, but existing video resources (e.g., YouTube) lack interactivity and personalization. While recent generative models have advanced artistic image synthesis, they struggle to generalize across media and often show temporal or structural inconsistencies, hindering faithful reproduction of human creative workflows. To address this, we propose a unified framework for multi-media painting process generation with a semantics-driven style control mechanism that embeds multiple media into a diffusion models conditional space and uses cross-medium style augmentation . This enables consistent texture evolution and process transfer across styles. A reverse-painting training strategy further ensures smooth, human-aligned generation. We also build a large-scale dataset of real painting processes and evaluate cross-media consistency, temporal coherence, and final-image fidelity, achieving strong results on LPIPS , DINO , and CLIP metrics . Finally, our Perceptual Distance Profile (PDP) curve quantitatively models the creative sequence, i.e., composition, color blocking, and detail refinement, mirroring human artistic progression.","{'paper_name': 'Loomis Painter: Reconstructing the Painting Process', 'github_url': 'https://github.com/Markus-Pobitzer/wlp', 'huggingface_url': 'https://huggingface.co/papers/2511.17344', 'upvote': 15, 'tags': ['media', 'painting', 'human']}"
doc2548048,"This study presents PARROT ( Persuasion and Agreement Robustness Rating of Output Truth ), a robustness focused framework designed to measure the degradation in accuracy that occurs under social pressure exerted on users through authority and persuasion in large language models ( LLMs ) the phenomenon of sycophancy (excessive conformity). PARROT (i) isolates causal effects by comparing the neutral version of the same question with an authoritatively false version using a double-blind evaluation , (ii) quantifies confidence shifts toward the correct and imposed false responses using log-likelihood-based calibration tracking, and (iii) systematically classifies failure modes (e.g., robust correct, sycophantic agreement, reinforced error, stubborn error, self-correction, etc.) using an eight-state behavioral taxonomy . We evaluated 22 models using 1,302 MMLU-style multiple-choice questions across 13 domains and domain-specific authority templates. Findings show marked heterogeneity: advanced models (e.g., GPT-5, GPT-4.1, Claude Sonnet 4.5) exhibit low "" follow rates "" (leq 11%, GPT-5: 4\%) and minimal accuracy loss, while older/smaller models show severe epistemic collapse (GPT-4: 80\%, Qwen 2.5-1.5B: 94\%). The danger is not limited to response changes; weak models reduce confidence in the correct response while increasing confidence in the imposed incorrect response. While international law and global knowledge at the domain level exhibit high fragility, elementary mathematics is relatively resilient. Consequently, we argue that the goal of "" resistance to overfitting pressure "" should be addressed as a primary objective alongside accuracy, harm avoidance, and privacy for safe deployment in the real world.","{'paper_name': 'Parrot: Persuasion and Agreement Robustness Rating of Output Truth -- A Sycophancy Robustness Benchmark for LLMs', 'github_url': 'https://github.com/YusufCelebii/PARROT', 'huggingface_url': 'https://huggingface.co/papers/2511.17220', 'upvote': 15, 'tags': ['models', 'using', 'gpt']}"
doc2548049,"Despite the remarkable success of Vision-Language Models (VLMs), their performance on a range of complex visual tasks is often hindered by a "" visual processing bottleneck "": a propensity to lose grounding in visual evidence and exhibit a deficit in contextualized visual experience during prolonged generation. Drawing inspiration from human cognitive memory theory, which distinguishes short-term visually-dominant memory and long-term semantically-dominant memory , we propose VisMem, a cognitively-aligned framework that equips VLMs with dynamic latent vision memories , a short-term module for fine-grained perceptual retention and a long-term module for abstract semantic consolidation . These memories are seamlessly invoked during inference, allowing VLMs to maintain both perceptual fidelity and semantic consistency across thinking and generation. Extensive experiments across diverse visual benchmarks for understanding, reasoning, and generation reveal that VisMem delivers a significant average performance boost of 11.8% relative to the vanilla model and outperforms all counterparts, establishing a new paradigm for latent-space memory enhancement . The code will be available: https://github.com/YU-deep/VisMem.git.","{'paper_name': 'VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models', 'github_url': 'https://github.com/YU-deep/VisMem.git', 'huggingface_url': 'https://huggingface.co/papers/2511.11007', 'upvote': 15, 'tags': ['visual', 'term', 'memory']}"
doc2548050,"""Thinking with images"" has emerged as an effective paradigm for advancing visual reasoning, extending beyond text-only chains of thought by injecting visual evidence into intermediate reasoning steps. However, existing methods fall short of human-like abstract visual thinking, as their flexibility is fundamentally limited by external tools. In this work, we introduce Monet, a training framework that enables multimodal large language models ( MLLMs ) to reason directly within the latent visual space by generating continuous embeddings that function as intermediate visual thoughts . We identify two core challenges in training MLLMs for latent visual reasoning: high computational cost in latent-vision alignment and insufficient supervision over latent embeddings, and address them with a three-stage distillation-based supervised fine-tuning ( SFT ) pipeline. We further reveal a limitation of applying GRPO to latent reasoning: it primarily enhances text-based reasoning rather than latent reasoning. To overcome this, we propose VLPO ( Visual-latent Policy Optimization ), a reinforcement learning method that explicitly incorporates latent embeddings into policy gradient updates . To support SFT , we construct Monet- SFT -125K, a high-quality text-image interleaved CoT dataset containing 125K real-world, chart, OCR, and geometry CoTs. Our model, Monet-7B, shows consistent gains across real-world perception and reasoning benchmarks and exhibits strong out-of-distribution generalization on challenging abstract visual reasoning tasks. We also empirically analyze the role of each training component and discuss our early unsuccessful attempts, providing insights for future developments in visual latent reasoning. Our model, data, and code are available at https://github.com/NOVAglow646/Monet.","{'paper_name': 'Monet: Reasoning in Latent Visual Space Beyond Images and Language', 'github_url': 'https://github.com/NOVAglow646/Monet', 'huggingface_url': 'https://huggingface.co/papers/2511.21395', 'upvote': 14, 'tags': ['reasoning', 'visual', 'latent']}"
doc2548051,"Embodied cognition argues that intelligence arises from sensorimotor interaction rather than passive observation. It raises an intriguing question: do modern vision-language models ( VLMs ), trained largely in a disembodied manner, exhibit signs of embodied cognition ? We introduce ENACT, a benchmark that casts evaluation of embodied cognition as world modeling from egocentric interaction in a visual question answering ( VQA ) format. Framed as a partially observable Markov decision process ( POMDP ) whose actions are scene graph changes, ENACT comprises two complementary sequence reordering tasks: forward world modeling (reorder shuffled observations given actions) and inverse world modeling (reorder shuffled actions given observations). While conceptually simple, solving these tasks implicitly demands capabilities central to embodied cognition - affordance recognition , action-effect reasoning , embodied awareness , and interactive, long-horizon memory from partially observable egocentric input, while avoiding low-level image synthesis that could confound the evaluation. We provide a scalable pipeline that synthesizes QA pairs from robotics simulation ( BEHAVIOR ) and evaluates models on 8,972 QA pairs spanning long-horizon home-scale activities. Experiments reveal a performance gap between frontier VLMs and humans that widens with interaction horizon. Models consistently perform better on the inverse task than the forward one and exhibit anthropocentric biases , including a preference for right-handed actions and degradation when camera intrinsics or viewpoints deviate from human vision. Website at https://enact-embodied-cognition.github.io/.","{'paper_name': 'ENACT: Evaluating Embodied Cognition with World Modeling of Egocentric Interaction', 'github_url': 'https://github.com/mll-lab-nu/ENACT', 'huggingface_url': 'https://huggingface.co/papers/2511.20937', 'upvote': 13, 'tags': ['embodied', 'embodied cognition', 'cognition']}"
doc2548052,"Generative models have excelled in RGB synthesis, but real-world applications require RGBA manipulation . This has led to a fragmented landscape: specialized, single-task models handle alpha but lack versatility, while unified multi-task frameworks are confined to the RGB domain. To bridge this critical gap, we propose OmniAlpha , the first unified, multi-task generative framework for sequence-to-sequence RGBA image generation and editing. Its architecture features MSRoPE-BiL , a novel RoPE method with a bi-directionally extendable layer axis for its Diffusion Transformer (DiT) backbone, enabling the concurrent processing of multiple input and target RGBA layers. To power this framework, we introduce AlphaLayers , a new dataset of 1,000 high-quality, multi-layer triplets, built via a novel automated synthesis and filter pipeline. Jointly training OmniAlpha on this dataset across a comprehensive suite of 21 diverse tasks, extensive experiments demonstrate that our unified approach consistently outperforms strong, specialized baselines. Most notably, OmniAlpha achieves a dramatic 84.8% relative reduction in SAD for mask-free matting on AIM-500 and wins over 90% of human preferences in layer-conditioned completion . Our work proves that a unified, multi-task model can learn a superior shared representation for RGBA, paving the way for more powerful, layer-aware generative systems.","{'paper_name': 'OmniAlpha: A Sequence-to-Sequence Framework for Unified Multi-Task RGBA Generation', 'github_url': 'https://github.com/Longin-Yu/OmniAlpha', 'huggingface_url': 'https://huggingface.co/papers/2511.20211', 'upvote': 12, 'tags': ['unified', 'task', 'multi']}"
doc2548053,"Recent advances in Vision-Language-Action (VLA) models demonstrate that visual signals can effectively complement sparse action supervisions. However, letting VLA directly predict high-dimensional visual states can distribute model capacity and incur prohibitive training cost, while compressing visual states into more compact supervisory signals inevitably incurs information bottlenecks. Moreover, existing methods often suffer from poor comprehension and reasoning capabilities due to the neglect of language supervision. This paper introduces Mantis, a novel framework featuring a Disentangled Visual Foresight (DVF) to tackle these issues. Specifically, Mantis decouples visual foresight prediction from the backbone with the combination of meta queries and a diffusion Transformer (DiT) head. With the current visual state provided to the DiT via a residual connection , a simple next-state prediction objective enables the meta queries to automatically capture the latent actions that delineate the visual trajectory, and hence boost the learning of explicit actions. The disentanglement reduces the burden of the VLA backbone, enabling it to maintain comprehension and reasoning capabilities through language supervision. Empirically, pretrained on human manipulation videos, robot demonstrations, and image-text pairs, Mantis achieves a 96.7% success rate on LIBERO benchmark after fine-tuning, surpassing powerful baselines while exhibiting high convergence speed. Real-world evaluations show that Mantis outperforms π_{0.5}, a leading open-source VLA model, particularly in instruction-following capability , generalization to unseen instructions, and reasoning ability . Code and weights are released to support the open-source community.","{'paper_name': 'Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight', 'github_url': 'https://github.com/zhijie-group/Mantis', 'huggingface_url': 'https://huggingface.co/papers/2511.16175', 'upvote': 12, 'tags': ['visual', 'vla', 'mantis']}"
doc2548054,"We present ReDirector, a novel camera-controlled video retake generation method for dynamically captured variable-length videos. In particular, we rectify a common misuse of RoPE in previous works by aligning the spatiotemporal positions of the input video and the target retake. Moreover, we introduce Rotary Camera Encoding ( RoCE ), a camera-conditioned RoPE phase shift that captures and integrates multi-view relationships within and across the input and target videos. By integrating camera conditions into RoPE , our method generalizes to out-of-distribution camera trajectories and video lengths, yielding improved dynamic object localization and static background preservation . Extensive experiments further demonstrate significant improvements in camera controllability , geometric consistency , and video quality across various trajectories and lengths.","{'paper_name': 'ReDirector: Creating Any-Length Video Retakes with Rotary Camera Encoding', 'github_url': 'https://github.com/byeongjun-park/ReDirector', 'huggingface_url': 'https://huggingface.co/papers/2511.19827', 'upvote': 11, 'tags': ['camera', 'video', 'rope']}"
doc2548055,"We propose Terminal Velocity Matching (TVM), a generalization of flow matching that enables high- fid elity one- and few-step generative modeling. TVM models the transition between any two diffusion timesteps and regularizes its behavior at its terminal time rather than at the initial time. We prove that TVM provides an upper bound on the 2-Wasserstein distance between data and model distributions when the model is Lipschitz continuous . However, since Diffusion Transformers lack this property, we introduce minimal architectural changes that achieve stable, single-stage training. To make TVM efficient in practice, we develop a fused attention kernel that supports backward passes on Jacobian-Vector Products , which scale well with transformer architectures. On ImageNet-256x256 , TVM achieves 3.29 FID with a single function evaluation (NFE) and 1.99 FID with 4 NFEs. It similarly achieves 4.32 1-NFE FID and 2.94 4-NFE FID on ImageNet-512x512 , representing state-of-the-art performance for one/few-step models from scratch.","{'paper_name': 'Terminal Velocity Matching', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.19797', 'upvote': 11, 'tags': ['tvm', 'fid', 'nfe']}"
doc2548056,"We address the task of multi-view image editing from sparse input views, where the inputs can be seen as a mix of images capturing the scene from different viewpoints. The goal is to modify the scene according to a textual instruction while preserving consistency across all views. Existing methods, based on per-scene neural fields or temporal attention mechanisms , struggle in this setting, often producing artifacts and incoherent edits. We propose InstructMix2Mix (I-Mix2Mix), a framework that distills the editing capabilities of a 2D diffusion model into a pretrained multi-view diffusion model , leveraging its data-driven 3D prior for cross-view consistency. A key contribution is replacing the conventional neural field consolidator in Score Distillation Sampling ( SDS ) with a multi-view diffusion student, which requires novel adaptations: incremental student updates across timesteps, a specialized teacher noise scheduler to prevent degeneration, and an attention modification that enhances cross-view coherence without additional cost. Experiments demonstrate that I-Mix2Mix significantly improves multi-view consistency while maintaining high per-frame edit quality.","{'paper_name': 'InstructMix2Mix: Consistent Sparse-View Editing Through Multi-View Model Personalization', 'github_url': 'https://github.com/DanielGilo/instruct-mix2mix/tree/main', 'huggingface_url': 'https://huggingface.co/papers/2511.14899', 'upvote': 11, 'tags': ['view', 'multi view', 'multi']}"
doc2548057,"Large multimodal models ( LMMs ) are increasingly adopted as judges in multimodal evaluation systems due to their strong instruction following and consistency with human preferences . However, their ability to follow diverse, fine-grained evaluation criteria remains underexplored. We develop Multi-Crit, a benchmark for evaluating multimodal judges on their capacity to follow pluralistic criteria and produce reliable criterion-level judgments . Covering both open-ended generation and verifiable reasoning tasks , Multi-Crit is built through a rigorous data curation pipeline that gathers challenging response pairs with multi-criterion human annotations . It further introduces three novel metrics for systematically assessing pluralistic adherence, criterion-switching flexibility, and the ability to recognize criterion-level preference conflicts. Comprehensive analysis of 25 LMMs reveals that 1) proprietary models still struggle to maintain consistent adherence to pluralistic criteria --especially in open-ended evaluation; 2) open-source models lag further behind in flexibly following diverse criteria; and 3) critic fine-tuning with holistic judgment signals enhances visual grounding but fails to generalize to pluralistic criterion-level judgment. Additional analyses on reasoning fine-tuning , test-time scaling , and boundary consistency between open-source and proprietary models further probe the limits of current multimodal judges. As a pioneering study, Multi-Crit lays the foundation for building reliable and steerable multimodal AI evaluation.","{'paper_name': 'Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following', 'github_url': 'https://github.com/tyxiong23/Multi-Crit', 'huggingface_url': 'https://huggingface.co/papers/2511.21662', 'upvote': 10, 'tags': ['multimodal', 'criterion', 'pluralistic']}"
doc2548058,"We present One4D, a unified framework for 4D generation and reconstruction that produces dynamic 4D content as synchronized RGB frames and pointmaps . By consistently handling varying sparsities of conditioning frames through a Unified Masked Conditioning (UMC) mechanism, One4D can seamlessly transition between 4D generation from a single image, 4D reconstruction from a full video, and mixed generation and reconstruction from sparse frames. Our framework adapts a powerful video generation model for joint RGB and pointmap generation, with carefully designed network architectures. The commonly used diffusion finetuning strategies for depthmap or pointmap reconstruction often fail on joint RGB and pointmap generation, quickly degrading the base video model. To address this challenge, we introduce Decoupled LoRA Control (DLC) , which employs two modality-specific LoRA adapters to form decoupled computation branches for RGB frames and pointmaps , connected by lightweight, zero-initialized control links that gradually learn mutual pixel-level consistency. Trained on a mixture of synthetic and real 4D datasets under modest computational budgets, One4D produces high-quality RGB frames and accurate pointmaps across both generation and reconstruction tasks. This work represents a step toward general, high-quality geometry-based 4D world modeling using video diffusion models . Project page: https://mizhenxing.github.io/One4D","{'paper_name': 'One4D: Unified 4D Generation and Reconstruction via Decoupled LoRA Control', 'github_url': 'https://github.com/MiZhenxing/One4D', 'huggingface_url': 'https://huggingface.co/papers/2511.18922', 'upvote': 10, 'tags': ['generation', '4d', 'reconstruction']}"
doc2548059,"Multiple-choice question answering (MCQA) has been a popular format for evaluating and reinforcement fine-tuning (RFT) of modern multimodal language models . Its constrained output format allows for simplified, deterministic automatic verification . However, we find that the options may leak exploitable signals, which makes the accuracy metrics unreliable for indicating real capabilities and encourages explicit or implicit answer guessing behaviors during RFT. We propose ReVeL (Rewrite and Verify by LLM), a framework that rewrites multiple-choice questions into open-form questions while keeping answers verifiable whenever possible. The framework categorizes questions according to different answer types, apply different rewriting and verification schemes, respectively. When applied for RFT, we converted 20k MCQA examples and use GRPO to finetune Qwen2.5-VL models. Models trained on ReVeL - OpenQA match MCQA accuracy on multiple-choice benchmarks and improve OpenQA accuracy by about six percentage points, indicating better data efficiency and more robust reward signals than MCQA-based training. When used for evaluation, ReVeL also reveals up to 20 percentage points of score inflation in MCQA benchmarks (relative to OpenQA ), improves judging accuracy , and reduces both cost and latency. We will release code and data publicly.","{'paper_name': 'Beyond Multiple Choice: Verifiable OpenQA for Robust Vision-Language RFT', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.17405', 'upvote': 10, 'tags': ['mcqa', 'accuracy', 'questions']}"
doc2548060,"MLLMs exhibit strong reasoning on isolated queries, yet they operate de novo -- solving each problem independently and often repeating the same mistakes. Existing memory-augmented agents mainly store past trajectories for reuse. However, trajectory-based memory suffers from brevity bias , gradually losing essential domain knowledge. More critically, even in truly multimodal problem-solving settings, it records only a single-modality trace of past behavior, failing to preserve how visual attention and logical reasoning jointly contributed to the solution. This is fundamentally misaligned with human cognition: semantic memory is both multimodal and integrated, preserving visual and abstract knowledge through coordinated but distinct representational streams. We thus introduce ViLoMem , a dual-stream memory framework that constructs compact, schema-based memory . It separately encodes visual distraction patterns and logical reasoning errors , enabling MLLMs to learn from their successful and failed experiences. Following a grow-and-refine principle, the system incrementally accumulates and updates multimodal semantic knowledge -- preserving stable, generalizable strategies while avoiding catastrophic forgetting . Across six multimodal benchmarks, ViLoMem consistently improves pass@1 accuracy and substantially reduces repeated visual and logical errors. Ablations confirm the necessity of dual-stream memory with explicit distraction--hallucination separation, demonstrating the value of error-aware multimodal memory for lifelong and cross-domain agentic learning. Our project page will be available at https://weihao-bo.github.io/ViLoMeo-page.","{'paper_name': 'Agentic Learner with Grow-and-Refine Multimodal Semantic Memory', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.21678', 'upvote': 9, 'tags': ['memory', 'multimodal', 'visual']}"
doc2548061,"Instruction-guided image editing offers an intuitive way for users to edit images with natural language. However, diffusion-based editing models often struggle to accurately interpret complex user instructions, especially those involving compositional relationships, contextual cues, or referring expressions, leading to edits that drift semantically or fail to reflect the intended changes. We tackle this problem by proposing MIRA (Multimodal Iterative Reasoning Agent), a lightweight, plug-and-play multimodal reasoning agent that performs editing through an iterative perception-reasoning-action loop , effectively simulating multi-turn human-model interaction processes. Instead of issuing a single prompt or static plan, MIRA predicts atomic edit instructions step by step, using visual feedback to make its decisions. Our 150K multimodal tool-use dataset , MIRA-Editing , combined with a two-stage SFT + GRPO training pipeline , enables MIRA to perform reasoning and editing over complex editing instructions. When paired with open-source image editing models such as Flux.1-Kontext , Step1X-Edit , and Qwen-Image-Edit , MIRA significantly improves both semantic consistency and perceptual quality , achieving performance comparable to or exceeding proprietary systems such as GPT-Image and Nano-Banana .","{'paper_name': 'MIRA: Multimodal Iterative Reasoning Agent for Image Editing', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.21087', 'upvote': 9, 'tags': ['editing', 'mira', 'image']}"
doc2548062,"While recent vision-language models ( VLMs ) demonstrate strong image understanding, their ability to ""think with images"", i.e., to reason through multi-step visual interactions, remains limited. We introduce VISTA-Gym , a scalable training environment for incentivizing tool-integrated visual reasoning capabilities in VLMs . VISTA-Gym unifies diverse real-world multimodal reasoning tasks (7 tasks from 13 datasets in total) with a standardized interface for visual tools (e.g., grounding, parsing), executable interaction loops , verifiable feedback signals , and efficient trajectory logging , enabling visual agentic reinforcement learning at scale. While recent VLMs exhibit strong text-only reasoning, both proprietary and open-source models still struggle with tool selection , invocation, and coordination. With VISTA-Gym , we train VISTA-R1 to interleave tool-use with agentic reasoning via multi-turn trajectory sampling and end-to-end reinforcement learning . Extensive experiments across 11 public reasoning-intensive VQA benchmarks show that VISTA-R1-8B outperforms state-of-the-art baselines with similar sizes by 9.51%-18.72%, demonstrating VISTA-Gym as an effective training ground to unlock the tool-integrated reasoning capabilities for VLMs .","{'paper_name': 'Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.19773', 'upvote': 9, 'tags': ['reasoning', 'vista', 'vlms']}"
doc2548063,"Language understanding entails not just extracting the surface-level meaning of the linguistic input, but constructing rich mental models of the situation it describes. Here we propose that because processing within the brain's core language system is fundamentally limited, deeply understanding language requires exporting information from the language system to other brain regions that compute perceptual and motor representations, construct mental models, and store our world knowledge and autobiographical memories. We review the existing evidence for this hypothesis, and argue that recent progress in cognitive neuroscience provides both the conceptual foundation and the methods to directly test it, thus opening up a new strategy to reveal what it means, cognitively and neurally, to understand language.","{'paper_name': 'What does it mean to understand language?', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.19757', 'upvote': 9, 'tags': ['language', 'understanding', 'mental models']}"
doc2548064,"Scaling up multimodal models has enabled remarkable advances in visual understanding and reasoning, but practical demands call for smaller, efficient systems. In this work, we conduct a principled analysis of downscaling intelligence in multimodal models , examining how reduced large language model (LLM) capacity affects multimodal capabilities. Our initial findings reveal an interesting trend: LLM downscaling disproportionately affects visual capabilities , rather than abilities inherited from the LLM. We then examine whether this drop mainly reflects the expected decline in visual reasoning or a more fundamental loss of perceptual abilities. Isolating the effect of LLM downscaling on perception, we find performance still drops sharply, often matching or exceeding the impact on reasoning. To address this bottleneck, we introduce visual extraction tuning , which explicitly trains the model to extract instruction-relevant visual details consistently across tasks. With these extracted visual details, we then apply step-by-step reasoning to generate answers. Together, these components form our Extract+Think approach , setting a new standard for efficiency and performance in this space.","{'paper_name': 'Downscaling Intelligence: Exploring Perception and Reasoning Bottlenecks in Small Multimodal Models', 'github_url': 'https://github.com/markendo/downscaling_intelligence', 'huggingface_url': 'https://huggingface.co/papers/2511.17487', 'upvote': 9, 'tags': ['visual', 'reasoning', 'llm']}"
doc2548065,"Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G^2VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding . G^2VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning . Our unified design is highly scalable for spatial understanding : it trains on abundant multi-view image and video data , while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations. Experimental results demonstrate G^2VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks. By unifying a semantically strong VLM with low-level 3D vision tasks, we hope G^2VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.","{'paper_name': 'G^2VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning', 'github_url': 'https://github.com/InternRobotics/G2VLM', 'huggingface_url': 'https://huggingface.co/papers/2511.21688', 'upvote': 8, 'tags': ['spatial', '3d', 'tasks']}"
doc2548066,"Generating realistic 3D cities is fundamental to world models, virtual reality, and game development, where an ideal urban scene must satisfy both stylistic diversity, fine-grained, and controllability. However, existing methods struggle to balance the creative flexibility offered by text-based generation with the object-level editability enabled by explicit structural representations. We introduce MajutsuCity, a natural language-driven and aesthetically adaptive framework for synthesizing structurally consistent and stylistically diverse 3D urban scenes . MajutsuCity represents a city as a composition of controllable layouts , assets , and materials , and operates through a four-stage pipeline. To extend controllability beyond initial generation, we further integrate MajutsuAgent, an interactive language-grounded editing agent } that supports five object-level operations. To support photorealistic and customizable scene synthesis, we also construct MajutsuDataset, a high-quality multimodal dataset} containing 2D semantic layouts and height maps , diverse 3D building assets , and curated PBR materials and skyboxes , each accompanied by detailed annotations . Meanwhile, we develop a practical set of evaluation metrics , covering key dimensions such as structural consistency , scene complexity , material fidelity , and lighting atmosphere . Extensive experiments demonstrate MajutsuCity reduces layout FID by 83.7% compared with CityDreamer and by 20.1% over CityCraft . Our method ranks first across all AQS and RDR scores , outperforming existing methods by a clear margin. These results confirm MajutsuCity as a new state-of-the-art in geometric fidelity , stylistic adaptability , and semantic controllability for 3D city generation. We expect our framework can inspire new avenues of research in 3D city generation. Our dataset and code will be released at https://github.com/LongHZ140516/MajutsuCity.","{'paper_name': 'MajutsuCity: Language-driven Aesthetic-adaptive City Generation with Controllable 3D Assets and Layouts', 'github_url': 'https://github.com/LongHZ140516/MajutsuCity', 'huggingface_url': 'https://huggingface.co/papers/2511.20415', 'upvote': 8, 'tags': ['3d', 'majutsucity', 'generation']}"
doc2548067,"Progress in computer use agents (CUAs) has been constrained by the absence of large and high-quality datasets that capture how humans interact with a computer. While LLMs have thrived on abundant textual data, no comparable corpus exists for CUA trajectories . To address these gaps, we introduce FaraGen , a novel synthetic data generation system for multi-step web tasks . FaraGen can propose diverse tasks from frequently used websites, generate multiple solution attempts, and filter successful trajectories using multiple verifiers . It achieves high throughput, yield, and diversity for multi-step web tasks , producing verified trajectories at approximately $1 each. We use this data to train Fara-7B , a native CUA model that perceives the computer using only screenshots , executes actions via predicted coordinates , and is small enough to run on-device. We find that Fara-7B outperforms other CUA models of comparable size on benchmarks like WebVoyager , Online-Mind2Web , and WebTailBench -- our novel benchmark that better captures under-represented web tasks in pre-existing benchmarks. Furthermore, Fara-7B is competitive with much larger frontier models, illustrating key benefits of scalable data generation systems in advancing small efficient agentic models. We are making Fara-7B open-weight on Microsoft Foundry and HuggingFace, and we are releasing WebTailBench .","{'paper_name': 'Fara-7B: An Efficient Agentic Model for Computer Use', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.19663', 'upvote': 8, 'tags': ['fara', 'fara 7b', 'tasks']}"
doc2548068,"We propose a fully data-driven approach to designing mutual information (MI) estimators. Since any MI estimator is a function of the observed sample from two random variables, we parameterize this function with a neural network ( MIST ) and train it end-to-end to predict MI values. Training is performed on a large meta-dataset of 625,000 synthetic joint distributions with known ground-truth MI. To handle variable sample sizes and dimensions, we employ a two-dimensional attention scheme ensuring permutation invariance across input samples. To quantify uncertainty, we optimize a quantile regression loss, enabling the estimator to approximate the sampling distribution of MI rather than return a single point estimate. This research program departs from prior work by taking a fully empirical route, trading universal theoretical guarantees for flexibility and efficiency. Empirically, the learned estimators largely outperform classical baselines across sample sizes and dimensions, including on joint distributions unseen during training. The resulting quantile-based intervals are well-calibrated and more reliable than bootstrap-based confidence intervals , while inference is orders of magnitude faster than existing neural baselines. Beyond immediate empirical gains, this framework yields trainable, fully differentiable estimators that can be embedded into larger learning pipelines. Moreover, exploiting MI's invariance to invertible transformations, meta-dataset s can be adapted to arbitrary data modalities via normalizing flows , enabling flexible training for diverse target meta-distributions.","{'paper_name': 'MIST: Mutual Information Via Supervised Training', 'github_url': 'https://github.com/grgera/mist', 'huggingface_url': 'https://huggingface.co/papers/2511.18945', 'upvote': 8, 'tags': ['mi', 'sample', 'meta']}"
doc2548069,"Retrieval-augmented generation ( RAG ) enhances large language models (LLMs) with external knowledge but still suffers from long contexts and disjoint retrieval-generation optimization . In this work, we propose CLaRa ( Continuous Latent Reasoning ), a unified framework that performs embedding-based compression and joint optimization in a shared continuous space . To obtain semantically rich and retrievable compressed vectors, we introduce SCP , a key-preserving data synthesis framework using QA and paraphrase supervision . CLaRa then trains the reranker and generator end-to-end via a single language modeling loss, with gradients flowing through both modules using a differentiable top-k estimator . Theoretically, this unified optimization aligns retrieval relevance with answer quality. Experiments across multiple QA benchmarks show that CLaRa achieves state-of-the-art compression and reranking performance, often surpassing text-based fine-tuned baselines.","{'paper_name': 'CLaRa: Bridging Retrieval and Generation with Continuous Latent Reasoning', 'github_url': 'https://github.com/apple/ml-clara', 'huggingface_url': 'https://huggingface.co/papers/2511.18659', 'upvote': 8, 'tags': ['retrieval', 'optimization', 'clara']}"
doc2548070,"Large language models (LLMs) solve complex problems yet fail on simpler variants, suggesting they achieve correct outputs through mechanisms fundamentally different from human reasoning. To understand this gap, we synthesize cognitive science research into a taxonomy of 28 cognitive elements spanning reasoning invariants , meta-cognitive controls , representations for organizing reasoning & knowledge, and transformation operations . We introduce a fine-grained evaluation framework and conduct the first large-scale empirical analysis of 192K traces from 18 models across text, vision, and audio, complemented by 54 human think-aloud traces , which we make publicly available. We find that models under-utilize cognitive elements correlated with success, narrowing to rigid sequential processing on ill-structured problems where diverse representations and meta-cognitive monitoring are critical. Human traces show more abstraction and conceptual processing , while models default to surface-level enumeration . Meta-analysis of 1.6K LLM reasoning papers reveals the research community concentrates on easily quantifiable elements (sequential organization: 55%, decomposition: 60%) but neglecting meta-cognitive controls ( self-awareness : 16%) that correlate with success. Models possess behavioral repertoires associated with success but fail to deploy them spontaneously. Leveraging these patterns, we develop test-time reasoning guidance that automatically scaffold successful structures, improving performance by up to 66.7% on complex problems. By establishing a shared vocabulary between cognitive science and LLM research, our framework enables systematic diagnosis of reasoning failures and principled development of models that reason through robust cognitive mechanisms rather than spurious shortcuts , while providing tools to test theories of human cognition at scale.","{'paper_name': 'Cognitive Foundations for Reasoning and Their Manifestation in LLMs', 'github_url': 'https://github.com/pkargupta/cognitive_foundations/', 'huggingface_url': 'https://huggingface.co/papers/2511.16660', 'upvote': 8, 'tags': ['cognitive', 'reasoning', 'models']}"
doc2548071,"This work presents Controllable Layer Decomposition ( CLD ), a method for achieving fine-grained and controllable multi-layer separation of raster images. In practical workflows, designers typically generate and edit each RGBA layer independently before compositing them into a final raster image. However, this process is irreversible: once composited, layer-level editing is no longer possible. Existing methods commonly rely on image matting and inpainting , but remain limited in controllability and segmentation precision. To address these challenges, we propose two key modules: LayerDecompose-DiT ( LD-DiT ), which decouples image elements into distinct layers and enables fine-grained control; and Multi-Layer Conditional Adapter ( MLCA ), which injects target image information into multi-layer tokens to achieve precise conditional generation . To enable a comprehensive evaluation, we build a new benchmark and introduce tailored evaluation metrics . Experimental results show that CLD consistently outperforms existing methods in both decomposition quality and controllability. Furthermore, the separated layers produced by CLD can be directly manipulated in commonly used design tools such as PowerPoint, highlighting its practical value and applicability in real-world creative workflows.","{'paper_name': 'Controllable Layer Decomposition for Reversible Multi-Layer Image Generation', 'github_url': 'https://github.com/monkek123King/CLD', 'huggingface_url': 'https://huggingface.co/papers/2511.16249', 'upvote': 8, 'tags': ['layer', 'image', 'multi layer']}"
doc2548072,"Abstract reasoning from minimal examples remains a core unsolved problem for frontier foundation models such as GPT-5 and Grok 4 . These models still fail to infer structured transformation rules from a handful of examples, which is a key hallmark of human intelligence. The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) provides a rigorous testbed for this capability, demanding conceptual rule induction and transfer to novel tasks. Most existing methods treat ARC-AGI as a purely textual reasoning task, overlooking the fact that humans rely heavily on visual abstraction when solving such puzzles. However, our pilot experiments reveal a paradox: naively rendering ARC-AGI grids as images degrades performance due to imprecise rule execution. This leads to our central hypothesis that vision and language possess complementary strengths across distinct reasoning stages: vision supports global pattern abstraction and verification, whereas language specializes in symbolic rule formulation and precise execution. Building on this insight, we introduce two synergistic strategies: (1) Vision-Language Synergy Reasoning (VLSR) , which decomposes ARC-AGI into modality-aligned subtasks; and (2) Modality-Switch Self-Correction (MSSC) , which leverages vision to verify text-based reasoning for intrinsic error correction . Extensive experiments demonstrate that our approach yields up to a 4.33% improvement over text-only baselines across diverse flagship models and multiple ARC-AGI tasks. Our findings suggest that unifying visual abstraction with linguistic reasoning is a crucial step toward achieving generalizable, human-like intelligence in future foundation models. Source code will be released soon.","{'paper_name': 'Think Visually, Reason Textually: Vision-Language Synergy in ARC', 'github_url': 'https://github.com/InternLM/ARC-VL', 'huggingface_url': 'https://huggingface.co/papers/2511.15703', 'upvote': 8, 'tags': ['reasoning', 'arc agi', 'agi']}"
doc2548073,"Modeling genomic sequences faces two unsolved challenges: the information density varies widely across different regions, while there is no clearly defined minimum vocabulary unit. Relying on either four primitive bases or independently designed DNA tokenizers, existing approaches with naive masked language modeling pre-training often fail to adapt to the varying complexities of genomic sequences. Leveraging Token Merging techniques, this paper introduces a hierarchical architecture that jointly optimizes a dynamic genomic tokenizer and latent Transformers with context-aware pre-training tasks. As for network structures, the tokenization module automatically chunks adjacent bases into words by stacking multiple layers of the differentiable token merging blocks with local-window constraints, then a Latent Encoder captures the global context of these merged words by full-attention blocks . Symmetrically employing a Latent Decoder and a Local Decoder , MergeDNA learns with two pre-training tasks: Merged Token Reconstruction simultaneously trains the dynamic tokenization module and adaptively filters important tokens, while Adaptive Masked Token Modeling learns to predict these filtered tokens to capture informative contents. Extensive experiments show that MergeDNA achieves superior performance on three popular DNA benchmarks and several multi-omics tasks with fine-tuning or zero-shot evaluation, outperforming typical tokenization methods and large-scale DNA foundation models.","{'paper_name': 'MergeDNA: Context-aware Genome Modeling with Dynamic Tokenization through Token Merging', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.14806', 'upvote': 8, 'tags': ['token', 'training', 'tokenization']}"
doc2548074,"This paper studies Visual Question-Visual Answering ( VQ-VA ): generating an image, rather than text, in response to a visual question -- an ability that has recently emerged in proprietary systems such as NanoBanana and GPT-Image . To also bring this capability to open-source models, we introduce VQ-VA World, a data-centric framework built around an agentic pipeline for large-scale, targeted data construction. Leveraging web-scale deployment , this pipeline crawls a massive amount of ~1.8M high-quality, interleaved image-text samples for model training. For evaluation, we further release IntelligentBench , a human-curated benchmark that systematically assesses VQ-VA along the aspects of world knowledge , design knowledge , and reasoning . Training with VQ-VA World data yields strong empirical gains: it helps LightFusion attain 53.06 on IntelligentBench , substantially surpassing the best prior open-source baselines (i.e., 7.78 from vanilla LightFusion ; 1.94 from UniWorld-V1 ), and significantly narrowing the gap toward leading proprietary systems (e.g., 81.67 from NanoBanana ; 82.64 from GPT-Image ). By releasing the full suite of model weights, datasets, and pipelines, we hope to stimulate future research on VQ-VA .","{'paper_name': 'VQ-VA World: Towards High-Quality Visual Question-Visual Answering', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.20573', 'upvote': 7, 'tags': ['vq', 'vq va', 'va']}"
doc2548075,"Block-causal video generation faces a stark speed-quality trade-off: small 1.3B models manage only 16 FPS while large 14B models crawl at 4.5 FPS, forcing users to choose between responsiveness and quality. Block Cascading significantly mitigates this trade-off through training-free parallelization . Our key insight: future video blocks do not need fully denoised current blocks to begin generation. By starting block generation with partially denoised context from predecessors, we transform sequential pipelines into parallel cascades where multiple blocks denoise simultaneously. With 5 GPUs exploiting temporal parallelism , we achieve ~2x acceleration across all model scales: 1.3B models accelerate from 16 to 30 FPS, 14B models from 4.5 to 12.5 FPS. Beyond inference speed, Block Cascading eliminates overhead from KV-recaching (of ~200ms) during context switches for interactive generation . Extensive evaluations validated against multiple block-causal pipelines demonstrate no significant loss in generation quality when switching from block-causal to Block Cascading pipelines for inference. Project Page: https://hmrishavbandy.github.io/block_cascading_page/","{'paper_name': 'Block Cascading: Training Free Acceleration of Block-Causal Video Models', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.20426', 'upvote': 7, 'tags': ['block', 'generation', 'fps']}"
doc2548076,"Task scheduling is critical for embodied AI, enabling agents to follow natural language instructions and execute actions efficiently in 3D physical worlds. However, existing datasets often simplify task planning by ignoring operations research (OR) knowledge and 3D spatial grounding. In this work, we propose Operations Research knowledge-based 3D Grounded Task Scheduling ( ORS3D ), a new task that requires the synergy of language understanding, 3D grounding , and efficiency optimization . Unlike prior settings, ORS3D demands that agents minimize total completion time by leveraging parallelizable subtasks , e.g., cleaning the sink while the microwave operates. To facilitate research on ORS3D , we construct ORS3D -60K, a large-scale dataset comprising 60K composite tasks across 4K real-world scenes. Furthermore, we propose GRANT , an embodied multi-modal large language model equipped with a simple yet effective scheduling token mechanism to generate efficient task schedules and grounded actions. Extensive experiments on ORS3D -60K validate the effectiveness of GRANT across language understanding, 3D grounding , and scheduling efficiency. The code is available at https://github.com/H-EmbodVis/ GRANT","{'paper_name': 'Cook and Clean Together: Teaching Embodied Agents for Parallel Task Execution', 'github_url': 'https://github.com/H-EmbodVis/GRANT', 'huggingface_url': 'https://huggingface.co/papers/2511.19430', 'upvote': 7, 'tags': ['task', 'ors3d', '3d']}"
doc2548077,"Vision-language-action (VLA) models show potential for general robotic tasks, but remain challenging in spatiotemporally coherent manipulation, which requires fine-grained representations. Typically, existing methods embed 3D positions into visual representations to enhance the spatial precision of actions. However, these methods struggle to achieve temporally coherent control over action execution. In this work, we propose VLA-4D, a general VLA model with 4D awareness for spatiotemporally coherent robotic manipulation . Our model is guided by two key designs: 1) 4D-aware visual representation . We extract visual features, embed 1D time into 3D positions for 4D embeddings, and fuse them into a unified visual representation via a cross-attention mechanism . 2) Spatiotemporal action representation . We extend conventional spatial action representations with temporal information to enable the spatiotemporal planning , and align the multimodal representations into the LLM for spatiotemporal action prediction. Within this unified framework, the designed visual and action representations jointly make robotic manipulation spatially-smooth and temporally-coherent. In addition, we extend the VLA dataset with temporal action annotations for fine-tuning our model. Extensive experiments have been conducted to verify the superiority of our method across different tasks of robotic manipulation.","{'paper_name': 'VLA-4D: Embedding 4D Awareness into Vision-Language-Action Models for SpatioTemporally Coherent Robotic Manipulation', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.17199', 'upvote': 7, 'tags': ['action', 'visual', 'representations']}"
doc2548078,"While web data quality is crucial for large language model s, most curation efforts focus on filtering and deduplication,treating HTML-to-text extraction as a fixed pre-processing step. Existing web corpora rely on heuristic-based extractors like Trafilatura, which struggle to preserve document structure and frequently corrupt structured elements such as formulas, codes, and tables. We hypothesize that improving extraction quality can be as impactful as aggressive filtering strategies for downstream performance. We introduce MinerU-HTML, a novel extraction pipeline that reformulates content extraction as a sequence labeling problem solved by a 0.6B-parameter language model . Unlike text-density heuristics, MinerU-HTML leverages semantic understanding and employs a two-stage formatting pipeline that explicitly categorizes semantic elements before converting to Markdown . Crucially, its model-based approach is inherently scalable, whereas heuristic methods offer limited improvement pathways. On MainWebBench, our benchmark of 7,887 annotated web pages, MinerU-HTML achieves 81.8\% ROUGE-N F1 compared to Trafilatura's 63.6\%, with exceptional structured element preservation (90.9\% for code blocks, 94.0\% for formulas). Using MinerU-HTML, we construct AICC ( AI-ready Common Crawl ), a 7.3-trillion token multilingual corpus from two Common Crawl snapshots. In controlled pretraining experiments where AICC and Trafilatura-extracted TfCC undergo identical filtering, models trained on AICC (62B tokens) achieve 50.8\% average accuracy across 13 benchmarks , outperforming TfCC by 1.08pp-providing direct evidence that extraction quality significantly impacts model capabilities. AICC also surpasses RefinedWeb and FineWeb on key benchmarks . We publicly release MainWebBench, MinerU-HTML, and AICC, demonstrating that HTML extraction is a critical, often underestimated component of web corpus construction.","{'paper_name': 'AICC: Parse HTML Finer, Make Models Better -- A 7.3T AI-Ready Corpus Built by a Model-Based HTML Parser', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.16397', 'upvote': 7, 'tags': ['html', 'extraction', 'mineru']}"
doc2548079,"Information-seeking is a core capability for AI agents, requiring them to gather and reason over tool-generated information across long trajectories. However, such multi-step information-seeking tasks remain challenging for agents backed by language models. While process reward models ( PRMs ) can guide agents by ranking candidate steps at test-time, existing PRMs , designed for short reasoning with binary judgment, cannot capture richer dimensions of information-seeking steps, such as tool interactions and reasoning over tool outputs, nor handle the rapidly growing context in long-horizon tasks. To address these limitations, we introduce PRInTS, a generative PRM trained with dual capabilities: (1) dense scoring based on the PRM's reasoning across multiple step quality dimensions (e.g., interpretation of tool outputs, tool call informativeness ) and (2) trajectory summarization that compresses the growing context while preserving essential information for step evaluation. Extensive evaluations across FRAMES , GAIA (levels 1-3), and WebWalkerQA (easy-hard) benchmarks on multiple models, along with ablations, reveal that best-of-n sampling with PRInTS enhances information-seeking abilities of open-source models as well as specialized agents , matching or surpassing the performance of frontier models with a much smaller backbone agent and outperforming other strong reward modeling baselines.","{'paper_name': 'PRInTS: Reward Modeling for Long-Horizon Information Seeking', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.19314', 'upvote': 6, 'tags': ['information', 'tool', 'models']}"
doc2548080,"Realistic 3D city generation is fundamental to a wide range of applications, including virtual reality and digital twins. However, most existing methods rely on training a single diffusion model , which limits their ability to generate personalized and boundless city-scale scenes. In this paper, we present Yo'City, a novel agentic framework that enables user-customized and infinitely expandable 3D city generation by leveraging the reasoning and compositional capabilities of off-the-shelf large models. Specifically, Yo'City first conceptualize the city through a top-down planning strategy that defines a hierarchical "" City-District-Grid "" structure. The Global Planner determines the overall layout and potential functional districts, while the Local Designer further refines each district with detailed grid-level descriptions. Subsequently, the grid-level 3D generation is achieved through a "" produce-refine-evaluate "" isometric image synthesis loop, followed by image-to-3D generation . To simulate continuous city evolution, Yo'City further introduces a user-interactive, relationship-guided expansion mechanism, which performs scene graph -based distance- and semantics-aware layout optimization , ensuring spatially coherent city growth. To comprehensively evaluate our method, we construct a diverse benchmark dataset and design six multi-dimensional metrics that assess generation quality from the perspectives of semantics, geometry, texture, and layout. Extensive experiments demonstrate that Yo'City consistently outperforms existing state-of-the-art methods across all evaluation aspects.","{'paper_name': ""Yo'City: Personalized and Boundless 3D Realistic City Scene Generation via Self-Critic Expansion"", 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.18734', 'upvote': 6, 'tags': ['city', 'generation', 'yo city']}"
doc2548081,"Visual Autoregressive (VAR) models have recently garnered significant attention for their innovative next-scale prediction paradigm, offering notable advantages in both inference efficiency and image quality compared to traditional multi-step autoregressive (AR) and diffusion models . However, despite their efficiency, VAR models often suffer from the diversity collapse i.e., a reduction in output variability, analogous to that observed in few-step distilled diffusion models . In this paper, we introduce DiverseVAR , a simple yet effective approach that restores the generative diversity of VAR models without requiring any additional training. Our analysis reveals the pivotal component of the feature map as a key factor governing diversity formation at early scales. By suppressing the pivotal component in the model input and amplifying it in the model output, DiverseVAR effectively unlocks the inherent generative potential of VAR models while preserving high-fidelity synthesis . Empirical results demonstrate that our approach substantially enhances generative diversity with only neglectable performance influences. Our code will be publicly released at https://github.com/wangtong627/ DiverseVAR .","{'paper_name': 'Diversity Has Always Been There in Your Visual Autoregressive Models', 'github_url': 'https://github.com/wangtong627/DiverseVAR', 'huggingface_url': 'https://huggingface.co/papers/2511.17074', 'upvote': 6, 'tags': ['models', 'var models', 'var']}"
doc2548082,"With the rapid development of Large Language Models ( LLMs ), AI agents have demonstrated increasing proficiency in scientific tasks, ranging from hypothesis generation and experimental design to manuscript writing. Such agent systems are commonly referred to as "" AI Scientists ."" However, existing AI Scientists predominantly formulate scientific discovery as a standalone search or optimization problem , overlooking the fact that scientific research is inherently a social and collaborative endeavor. Real-world science relies on a complex scientific infrastructure composed of collaborative mechanisms , contribution attribution , peer review , and structured scientific knowledge networks . Due to the lack of modeling for these critical dimensions, current systems struggle to establish a genuine research ecosystem or interact deeply with the human scientific community. To bridge this gap, we introduce OmniScientist , a framework that explicitly encodes the underlying mechanisms of human research into the AI scientific workflow. OmniScientist not only achieves end-to-end automation across data foundation , literature review , research ideation , experiment automation , scientific writing , and peer review , but also provides comprehensive infrastructural support by simulating the human scientific system, comprising: (1) a structured knowledge system built upon citation networks and conceptual correlations ; (2) a collaborative research protocol ( OSP ), which enables seamless multi-agent collaboration and human researcher participation; and (3) an open evaluation platform ( ScienceArena ) based on blind pairwise user voting and Elo rankings . This infrastructure empowers agents to not only comprehend and leverage human knowledge systems but also to collaborate and co-evolve, fostering a sustainable and scalable innovation ecosystem.","{'paper_name': 'OmniScientist: Toward a Co-evolving Ecosystem of Human and AI Scientists', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.16931', 'upvote': 6, 'tags': ['scientific', 'research', 'human']}"
doc2548083,"We present Upsample Anything, a lightweight test-time optimization (TTO) framework that restores low-resolution features to high-resolution, pixel-wise outputs without any training. Although Vision Foundation Models demonstrate strong generalization across diverse downstream tasks, their representations are typically downsampled by 14x/16x (e.g., ViT), which limits their direct use in pixel-level applications. Existing feature upsampling approaches depend on dataset-specific retraining or heavy implicit optimization, restricting scalability and generalization. Upsample Anything addresses these issues through a simple per-image optimization that learns an anisotropic Gaussian kernel combining spatial and range cues, effectively bridging Gaussian Splatting and Joint Bilateral Upsampling . The learned kernel acts as a universal, edge-aware operator that transfers seamlessly across architectures and modalities, enabling precise high-resolution reconstruction of features, depth, or probability maps. It runs in only approx0.419 s per 224x224 image and achieves state-of-the-art performance on semantic segmentation , depth estimation , and both depth and probability map upsampling . Project page: https://seominseok0429.github.io/Upsample-Anything/{https://seominseok0429.github.io/Upsample-Anything/}","{'paper_name': 'Upsample Anything: A Simple and Hard to Beat Baseline for Feature Upsampling', 'github_url': 'https://github.com/seominseok0429/Upsample-Anything-A-Simple-and-Hard-to-Beat-Baseline-for-Feature-Upsampling', 'huggingface_url': 'https://huggingface.co/papers/2511.16301', 'upvote': 6, 'tags': ['upsample', 'upsampling', 'optimization']}"
doc2548084,"Peer review is a cornerstone of scientific publishing, including at premier machine learning conferences such as ICLR . As submission volumes increase, understanding the nature and dynamics of the review process is crucial for improving its efficiency, effectiveness, and the quality of published papers. We present a large-scale analysis of the ICLR 2024 and 2025 peer review processes, focusing on before- and after-rebuttal scores and reviewer-author interactions. We examine review scores , author-reviewer engagement , temporal patterns in review submissions, and co-reviewer influence effects. Combining quantitative analyses with LLM-based categorization of review texts and rebuttal discussions, we identify common strengths and weaknesses for each rating group, as well as trends in rebuttal strategies that are most strongly associated with score changes . Our findings show that initial scores and the ratings of co-reviewers are the strongest predictors of score changes during the rebuttal, pointing to a degree of reviewer influence . Rebuttals play a valuable role in improving outcomes for borderline papers, where thoughtful author responses can meaningfully shift reviewer perspectives. More broadly, our study offers evidence-based insights to improve the peer review process, guiding authors on effective rebuttal strategies and helping the community design fairer and more efficient review processes. Our code and score changes data are available at https://github.com/papercopilot/ iclr -insights.","{'paper_name': 'Insights from the ICLR Peer Review and Rebuttal Process', 'github_url': 'https://github.com/papercopilot/iclr-insights.', 'huggingface_url': 'https://huggingface.co/papers/2511.15462', 'upvote': 6, 'tags': ['review', 'reviewer', 'rebuttal']}"
doc2548085,"Vision Language Models ( VLMs ) perform well on standard video tasks but struggle with physics-driven reasoning involving motion dynamics and spatial interactions . This limitation reduces their ability to interpret real or AI-generated content (AIGC) videos and to generate physically consistent content. We present an approach that addresses this gap by translating physical-world context cues into interpretable representations aligned with VLMs ' perception, comprehension , and reasoning. We introduce MASS-Bench , a comprehensive benchmark consisting of 4,350 real-world and AIGC videos and 8,361 free-form video question-answering pairs focused on physics-related comprehension tasks, with detailed annotations including visual detections , sub-segment grounding , and full-sequence 3D motion tracking of entities. We further present MASS , a model-agnostic method that injects spatial-temporal signals into the VLM language space via depth-based 3D encoding and visual grounding , coupled with a motion tracker for object dynamics . To strengthen cross-modal alignment and reasoning, we apply reinforcement fine-tuning . Experiments and ablations show that our refined VLMs outperform comparable and larger baselines, as well as prior state-of-the-art models, by 8.7% and 6.0%, achieving performance comparable to close-source SoTA VLMs such as Gemini-2.5-Flash on physics reasoning and comprehension . These results validate the effectiveness of our approach.","{'paper_name': 'MASS: Motion-Aware Spatial-Temporal Grounding for Physics Reasoning and Comprehension in Vision-Language Models', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.18373', 'upvote': 5, 'tags': ['vlms', 'reasoning', 'motion']}"
doc2548086,"Grounding natural-language instructions into continuous control for quadruped robots remains a fundamental challenge in vision language action. Existing methods struggle to bridge high-level semantic reasoning and low-level actuation, leading to unstable grounding and weak generalization in the real world. To address these issues, we present MobileVLA-R1 , a unified vision-language-action framework that enables explicit reasoning and continuous control for quadruped robots. We construct MobileVLA-CoT, a large-scale dataset of multi-granularity chain-of-thought (CoT) for embodied trajectories, providing structured reasoning supervision for alignment. Built upon this foundation, we introduce a two-stage training paradigm that combines supervised CoT alignment with GRPO reinforcement learning to enhance reasoning consistency, control stability, and long-horizon execution. Extensive evaluations on VLN and VLA tasks demonstrate superior performance over strong baselines, with approximately a 5% improvement. Real-world deployment on a quadruped robot validates robust performance in complex environments. Code: https://github.com/AIGeeksGroup/ MobileVLA-R1 . Website: https://aigeeksgroup.github.io/ MobileVLA-R1 .","{'paper_name': 'MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots', 'github_url': 'https://github.com/AIGeeksGroup/MobileVLA-R1', 'huggingface_url': 'https://huggingface.co/papers/2511.17889', 'upvote': 5, 'tags': ['mobilevla', 'reasoning', 'r1']}"
doc2548087,"While recent video generation models have achieved significant visual fidelity, they often suffer from the lack of explicit physical controllability and plausibility . To address this, some recent studies attempted to guide the video generation with physics-based rendering . However, these methods face inherent challenges in accurately modeling complex physical properties and effectively control ling the resulting physical behavior over extended temporal sequences. In this work, we introduce PhysChoreo, a novel framework that can generate videos with diverse controllability and physical realism from a single image. Our method consists of two stages: first, it estimates the static initial physical properties of all objects in the image through part-aware physical property reconstruction . Then, through temporally instructed and physically editable simulation, it synthesizes high-quality videos with rich dynamic behaviors and physical realism . Experimental results show that PhysChoreo can generate videos with rich behaviors and physical realism , outperforming state-of-the-art methods on multiple evaluation metrics.","{'paper_name': 'PhysChoreo: Physics-Controllable Video Generation with Part-Aware Semantic Grounding', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.20562', 'upvote': 4, 'tags': ['physical', 'videos', 'realism']}"
doc2548088,"State-of-the-art flow models achieve remarkable quality but require slow, iterative sampling . To accelerate this, flow maps can be distilled from pre-trained teachers, a procedure that conventionally requires sampling from an external dataset. We argue that this data-dependency introduces a fundamental risk of Teacher-Data Mismatch , as a static dataset may provide an incomplete or even misaligned representation of the teacher's full generative capabilities. This leads us to question whether this reliance on data is truly necessary for successful flow map distillation . In this work, we explore a data-free alternative that samples only from the prior distribution , a distribution the teacher is guaranteed to follow by construction, thereby circumventing the mismatch risk entirely. To demonstrate the practical viability of this philosophy, we introduce a principled framework that learns to predict the teacher's sampling path while actively correcting for its own compounding errors to ensure high fid elity. Our approach surpasses all data-based counterparts and establishes a new state-of-the-art by a significant margin. Specifically, distilling from SiT-XL/2+REPA, our method reaches an impressive FID of 1.45 on ImageNet 256x256, and 1.49 on ImageNet 512x512, both with only 1 sampling step. We hope our work establishes a more robust paradigm for accelerating generative models and motivates the broader adoption of flow map distillation without data.","{'paper_name': 'Flow Map Distillation Without Data', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.19428', 'upvote': 4, 'tags': ['data', 'teacher', 'flow']}"
doc2548089,"Long-horizon robotic manipulation remains challenging for Vision-Language-Action (VLA) models despite recent progress in zero-shot generalization and simulation-to-real-world transfer. Current VLA models suffer from stage hallucination, where agents exploit coarse evaluation signals to shortcut multi-step tasks, reporting high progress without truly completing them. We present EvoVLA, a self-supervised VLA framework that addresses this issue through three complementary components: Stage-Aligned Reward (SAR), which uses triplet contrastive learning with Gemini-generated hard negatives to prevent visual shortcuts; Pose-Based Object Exploration (POE), which grounds curiosity in relative object-gripper pose instead of raw pixels; and Long-Horizon Memory , which uses selective context retention and gated fusion to stabilize intrinsic shaping during extended rollouts. Extensive evaluations on Discoverse-L , a long-horizon manipulation benchmark with three multi-stage tasks, show that EvoVLA improves average task success by 10.2 percentage points over the strongest baseline ( OpenVLA-OFT ), reaching 69.2 percent. EvoVLA also achieves one-and-a-half times better sample efficiency and reduces stage hallucination from 38.5 percent to 14.8 percent. Real-world deployment on physical robots reaches an average success rate of 54.6 percent across four manipulation tasks, outperforming OpenVLA-OFT by 11 points, demonstrating effective sim-to-real transfer and strong generalization. Code: https://github.com/AIGeeksGroup/EvoVLA. Website: https://aigeeksgroup.github.io/EvoVLA.","{'paper_name': 'EvoVLA: Self-Evolving Vision-Language-Action Model', 'github_url': 'https://github.com/AIGeeksGroup/EvoVLA', 'huggingface_url': 'https://huggingface.co/papers/2511.16166', 'upvote': 4, 'tags': ['evovla', 'stage', 'percent']}"
doc2548090,"Vision-Language-Action (VLA) policies excel in aligning language, perception, and robot control. However, most VLAs are trained purely by imitation , which overfits to demonstrations, and is brittle under distribution shift. Reinforcement learning (RL) directly optimizes task reward and thus addresses this misalignment, but real-robot interaction is expensive and conventional simulators are hard to engineer and transfer. We address both data efficiency and optimization stability in VLA post-training via a learned world model and an RL procedure tailored to flow-based action heads. Specifically, we introduce Prophet , a unified action-to-video robot actuation pretrained across large-scale, heterogeneous robot data to learn reusable action-outcome dynamics. It is able to few-shot adapt to new robots, objects, and environments, yielding a rollout-ready simulator . Upon Prophet , we reinforce action policies with Flow-action-GRPO (FA-GRPO), which adapts Flow-GRPO to operate on VLA actions, and with FlowScale , a stepwise reweighting that rescales per-step gradients in the flow head. Together, Prophet , FA-GRPO, and FlowScale constitute ProphRL , a practical, data- and compute-efficient path to VLA post-training. Experiments show 5-17% success gains on public benchmarks and 24-30% gains on real robots across different VLA variants.","{'paper_name': 'Reinforcing Action Policies by Prophesying', 'github_url': 'https://github.com/LogosRoboticsGroup/ProphRL', 'huggingface_url': 'https://huggingface.co/papers/2511.20633', 'upvote': 3, 'tags': ['action', 'vla', 'grpo']}"
doc2548091,"Diffusion-based editing enables realistic modification of local image regions, making AI-generated content harder to detect. Existing AIGC detection benchmarks focus on classifying entire images, overlooking the localization of diffusion-based edits. We introduce DiffSeg30k , a publicly available dataset of 30k diffusion-edited images with pixel-level annotations , designed to support fine-grained detection. DiffSeg30k features: 1) In-the-wild images --we collect images or image prompts from COCO to reflect real-world content diversity; 2) Diverse diffusion models --local edits using eight SOTA diffusion models ; 3) Multi-turn editing --each image undergoes up to three sequential edits to mimic real-world sequential editing; and 4) Realistic editing scenarios--a vision-language model (VLM)-based pipeline automatically identifies meaningful regions and generates context-aware prompts covering additions, removals, and attribute changes. DiffSeg30k shifts AIGC detection from binary classification to semantic segmentation , enabling simultaneous localization of edits and identification of the editing models. We benchmark three baseline segmentation approaches, revealing significant challenges in semantic segmentation tasks, particularly concerning robustness to image distortions . Experiments also reveal that segmentation models, despite being trained for pixel-level localization, emerge as highly reliable whole-image classifiers of diffusion edits, outperforming established forgery classifiers while showing great potential in cross-generator generalization . We believe DiffSeg30k will advance research in fine-grained localization of AI-generated content by demonstrating the promise and limitations of segmentation-based methods. DiffSeg30k is released at: https://huggingface.co/datasets/Chaos2629/ Diffseg30k","{'paper_name': 'DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.19111', 'upvote': 3, 'tags': ['diffseg30k', 'diffusion', 'segmentation']}"
doc2548092,"While recent world models generate highly realistic videos, their ability to perform robot path planning remains unclear and unquantified. We introduce Target-Bench , the first benchmark specifically designed to evaluate world models on mapless path planning toward semantic targets in real-world environments. Target-Bench provides 450 robot-collected video sequences spanning 45 semantic categories with SLAM-based ground truth trajectories . Our evaluation pipeline recovers camera motion from generated videos and measures planning performance using five complementary metrics that quantify target-reaching capability, trajectory accuracy , and directional consistency . We evaluate state-of-the-art models including Sora 2 , Veo 3.1 , and the Wan series . The best off-the-shelf model (Wan2.2-Flash) achieves only 0.299 overall score, revealing significant limitations in current world models for robotic planning tasks. We show that fine-tuning an open-source 5B-parameter model on only 325 scenarios from our dataset achieves 0.345 overall score -- an improvement of more than 400% over its base version (0.066) and 15% higher than the best off-the-shelf model. We will open-source the code and dataset.","{'paper_name': 'Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?', 'github_url': 'https://github.com/TUM-AVS/target-bench', 'huggingface_url': 'https://huggingface.co/papers/2511.17792', 'upvote': 3, 'tags': ['world', 'planning', 'models']}"
doc2548093,"We present Sphinx, a synthetic environment for visual perception and reasoning that targets core cognitive primitives . Sphinx procedurally generates puzzles using motifs , tiles , charts , icons , and geometric primitives , each paired with verifiable ground-truth solutions , enabling both precise evaluation and large-scale dataset construction . The benchmark covers 25 task types spanning symmetry detection , geometric transformations , spatial reasoning , chart interpretation , and sequence prediction . Evaluating recent large vision-language models (LVLMs) shows that even state-of-the-art GPT-5 attains only 51.1% accuracy, well below human performance. Finally, we demonstrate that reinforcement learning with verifiable rewards (RLVR) substantially improves model accuracy on these tasks and yields gains on external visual reasoning benchmarks, highlighting its promise for advancing multimodal reasoning .","{'paper_name': 'SPHINX: A Synthetic Environment for Visual Perception and Reasoning', 'github_url': 'https://github.com/xashru/sphinx', 'huggingface_url': 'https://huggingface.co/papers/2511.20814', 'upvote': 2, 'tags': ['reasoning', 'visual', 'verifiable']}"
doc2548094,"While recent text-to-video (T2V) diffusion models have achieved impressive quality and prompt alignment, they often produce low-diversity outputs when sampling multiple videos from a single text prompt. We tackle this challenge by formulating it as a set-level policy optimization problem, with the goal of training a policy that can cover the diverse range of plausible outcomes for a given prompt. To address this, we introduce DPP-GRPO, a novel framework for diverse video generation that combines Determinantal Point Processes (DPPs) and Group Relative Policy Optimization (GRPO) theories to enforce explicit reward on diverse generations. Our objective turns diversity into an explicit signal by imposing diminishing returns on redundant samples (via DPP) while supplies groupwise feedback over candidate sets (via GRPO). Our framework is plug-and-play and model-agnostic, and encourages diverse generations across visual appearance , camera motions , and scene structure without sacrificing prompt fidelity or perceptual quality. We implement our method on WAN and CogVideoX, and show that our method consistently improves video diversity on state-of-the-art benchmarks such as VBench , VideoScore , and human preference studies. Moreover, we release our code and a new benchmark dataset of 30,000 diverse prompts to support future research.","{'paper_name': 'Diverse Video Generation with Determinantal Point Process-Guided Policy Optimization', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.20647', 'upvote': 2, 'tags': ['diverse', 'prompt', 'grpo']}"
doc2548095,"Timestep distillation is an effective approach for improving the generation efficiency of diffusion models . The Consistency Model (CM), as a trajectory-based framework , demonstrates significant potential due to its strong theoretical foundation and high-quality few-step generation. Nevertheless, current continuous-time consistency distillation methods still rely heavily on training data and computational resources, hindering their deployment in resource-constrained scenarios and limiting their scalability to diverse domains. To address this issue, we propose Trajectory-Backward Consistency Model (TBCM), which eliminates the dependence on external training data by extracting latent representations directly from the teacher model's generation trajectory. Unlike conventional methods that require VAE encoding and large-scale datasets, our self-contained distillation paradigm significantly improves both efficiency and simplicity. Moreover, the trajectory-extracted samples naturally bridge the distribution gap between training and inference, thereby enabling more effective knowledge transfer. Empirically, TBCM achieves 6.52 FID and 28.08 CLIP scores on MJHQ-30k under one-step generation , while reducing training time by approximately 40% compared to Sana-Sprint and saving a substantial amount of GPU memory, demonstrating superior efficiency without sacrificing quality. We further reveal the diffusion-generation space discrepancy in continuous-time consistency distillation and analyze how sampling strategies affect distillation performance, offering insights for future distillation research. GitHub Link: https://github.com/hustvl/TBCM.","{'paper_name': 'Image-Free Timestep Distillation via Continuous-Time Consistency with Trajectory-Sampled Pairs', 'github_url': 'https://github.com/hustvl/TBCM', 'huggingface_url': 'https://huggingface.co/papers/2511.20410', 'upvote': 2, 'tags': ['distillation', 'generation', 'trajectory']}"
doc2548096,"Vision Foundation Models (VFMs) extract spatially downsampled representations , posing challenges for pixel-level tasks. Existing upsampling approaches face a fundamental trade-off: classical filters are fast and broadly applicable but rely on fixed forms, while modern upsamplers achieve superior accuracy through learnable, VFM-specific forms at the cost of retraining for each VFM. We introduce Neighborhood Attention Filtering (NAF) , which bridges this gap by learning adaptive spatial-and-content weights through Cross-Scale Neighborhood Attention and Rotary Position Embeddings (RoPE) , guided solely by the high-resolution input image. NAF operates zero-shot: it upsamples features from any VFM without retraining, making it the first VFM-agnostic architecture to outperform VFM-specific upsamplers and achieve state-of-the-art performance across multiple downstream tasks. It maintains high efficiency, scaling to 2K feature maps and reconstructing intermediate-resolution maps at 18 FPS. Beyond feature upsampling , NAF demonstrates strong performance on image restoration , highlighting its versatility. Code and checkpoints are available at https://github.com/valeoai/NAF.","{'paper_name': 'NAF: Zero-Shot Feature Upsampling via Neighborhood Attention Filtering', 'github_url': 'https://github.com/valeoai/NAF', 'huggingface_url': 'https://huggingface.co/papers/2511.18452', 'upvote': 2, 'tags': ['vfm', 'naf', 'upsampling']}"
doc2548097,"Recent advancements in multimodal large language models (MLLMs) and video agent systems have significantly improved general video understanding. However, when applied to scientific video understanding and educating, a domain that demands external professional knowledge integration and rigorous step-wise reasoning, existing approaches often struggle. To bridge this gap, we propose SciEducator, the first iterative self-evolving multi-agent system for scientific video comprehension and education. Rooted in the classical Deming Cycle from management science, our design reformulates its Plan-Do-Study-Act philosophy into a self-evolving reasoning and feedback mechanism, which facilitates the interpretation of intricate scientific activities in videos. Moreover, SciEducator can produce multimodal educational content tailored to specific scientific processes, including textual instructions , visual guides , audio narrations , and interactive references . To support evaluation, we construct SciVBench , a benchmark consisting of 500 expert-verified and literature-grounded science QA pairs across five categories, covering physical, chemical, and everyday phenomena . Extensive experiments demonstrate that SciEducator substantially outperforms leading closed-source MLLMs (e.g., Gemini, GPT-4o) and state-of-the-art video agents on the benchmark, establishing a new paradigm for the community.","{'paper_name': 'SciEducator: Scientific Video Understanding and Educating via Deming-Cycle Multi-Agent System', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.17943', 'upvote': 2, 'tags': ['video', 'scientific', 'scieducator']}"
doc2548098,"Recent video generation approaches increasingly rely on planning intermediate control signals such as object trajectories to improve temporal coherence and motion fidelity . However, these methods mostly employ single-shot plans that are typically limited to simple motions, or iterative refinement which requires multiple calls to the video generator, incuring high computational cost. To overcome these limitations, we propose SketchVerify, a training-free , sketch-verification-based planning framework that improves motion planning quality with more dynamically coherent trajectories (i.e., physically plausible and instruction-consistent motions ) prior to full video generation by introducing a test-time sampling and verification loop . Given a prompt and a reference image , our method predicts multiple candidate motion plans and ranks them using a vision-language verifier that jointly evaluates semantic alignment with the instruction and physical plausibility . To efficiently score candidate motion plans , we render each trajectory as a lightweight video sketch by compositing objects over a static background, which bypasses the need for expensive, repeated diffusion-based synthesis while achieving comparable performance. We iteratively refine the motion plan until a satisfactory one is identified, which is then passed to the trajectory-conditioned generator for final synthesis. Experiments on WorldModelBench and PhyWorldBench demonstrate that our method significantly improves motion quality , physical realism , and long-term consistency compared to competitive baselines while being substantially more efficient. Our ablation study further shows that scaling up the number of trajectory candidates consistently enhances overall performance.","{'paper_name': 'Planning with Sketch-Guided Verification for Physics-Aware Video Generation', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.17450', 'upvote': 2, 'tags': ['motion', 'video', 'plans']}"
doc2548099,"Generative models for structure-based drug design are often limited to a specific modality, restricting their broader applicability. To address this challenge, we introduce FuncBind, a framework based on computer vision to generate target-conditioned , all-atom molecules across atomic systems . FuncBind uses neural fields to represent molecules as continuous atomic densities and employs score-based generative models with modern architectures adapted from the computer vision literature. This modality-agnostic representation allows a single unified model to be trained on diverse atomic systems , from small to large molecules, and handle variable atom/residue counts, including non-canonical amino acids. FuncBind achieves competitive in silico performance in generating small molecules, macrocyclic peptides , and antibody complementarity-determining region loops , conditioned on target structures. FuncBind also generated in vitro novel antibody binders via de novo redesign of the complementarity-determining region H3 loop of two chosen co-crystal structures. As a final contribution, we introduce a new dataset and benchmark for structure-conditioned macrocyclic peptide generation. The code is available at https://github.com/prescient-design/funcbind.","{'paper_name': 'Unified all-atom molecule generation with neural fields', 'github_url': 'https://github.com/prescient-design/funcbind/', 'huggingface_url': 'https://huggingface.co/papers/2511.15906', 'upvote': 2, 'tags': ['funcbind', 'molecules', 'atomic']}"
doc2548100,"Training prohibited item detection models requires a large amount of X-ray security images, but collecting and annotating these images is time-consuming and laborious. To address data insufficiency, X-ray security image synthesis methods composite images to scale up datasets. However, previous methods primarily follow a two-stage pipeline, where they implement labor-intensive foreground extraction in the first stage and then composite images in the second stage. Such a pipeline introduces inevitable extra labor cost and is not efficient. In this paper, we propose a one-stage X-ray security image synthesis pipeline (Xsyn) based on text-to-image generation , which incorporates two effective strategies to improve the usability of synthetic images. The Cross-Attention Refinement (CAR) strategy leverages the cross-attention map from the diffusion model to refine the bounding box annotation . The Background Occlusion Modeling (BOM) strategy explicitly models background occlusion in the latent space to enhance imaging complexity. To the best of our knowledge, compared with previous methods, Xsyn is the first to achieve high-quality X-ray security image synthesis without extra labor cost. Experiments demonstrate that our method outperforms all previous methods with 1.2% mAP improvement, and the synthetic images generated by our method are beneficial to improve prohibited item detection performance across various X-ray security datasets and detectors. Code is available at https://github.com/pILLOW-1/Xsyn/.","{'paper_name': 'Taming Generative Synthetic Data for X-ray Prohibited Item Detection', 'github_url': 'https://github.com/pILLOW-1/Xsyn/', 'huggingface_url': 'https://huggingface.co/papers/2511.15299', 'upvote': 2, 'tags': ['images', 'ray', 'ray security']}"
doc2548101,"What data should a vision-language model be trained on? To answer this question, many data curation efforts center on the quality of a dataset. However, most of these existing methods are (i) offline, i.e. they produce a static dataset from a set of predetermined filtering criteria, and (ii) concept-agnostic, i.e. they use model-based filters which induce additional data biases. In this work, we go beyond such offline, concept-agnostic methods and advocate for more flexible, task-adaptive online concept-based curation. Our first contribution is DataConcept , a collection of 128M web-crawled image-text pairs annotated with fine-grained details about their concept composition. Building on DataConcept , we introduce Concept-Aware Batch Sampling ( CABS ), a simple yet effective batch sampling framework that flexibly constructs batches on-the-fly based on specific target distributions. We propose two variants: (i) Diversity Maximization ( CABS -DM) to curate batches with a broad coverage of available concepts, and (ii) Frequency Maximization ( CABS -FM) to curate batches with high object multiplicity. Through extensive evaluations across 28 benchmarks, we demonstrate that our CABS method significantly benefits CLIP / SigLIP model classes and yields highly performant models. Overall, CABS represents a strong open-source alternative to proprietary online data curation algorithms, enabling practitioners to define custom concept distributions that optimize for specific downstream tasks.","{'paper_name': 'Concept-Aware Batch Sampling Improves Language-Image Pretraining', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.20643', 'upvote': 1, 'tags': ['concept', 'cabs', 'data']}"
doc2548102,"Obtaining the precise 3D motion of a table tennis ball from standard monocular videos is a challenging problem, as existing methods trained on synthetic data struggle to generalize to the noisy, imperfect ball and table detections of the real world. This is primarily due to the inherent lack of 3D ground truth trajectories and spin annotations for real-world video. To overcome this, we propose a novel two-stage pipeline that divides the problem into a front-end perception task and a back-end 2D-to-3D uplifting task. This separation allows us to train the front-end components with abundant 2D supervision from our newly created TTHQ dataset , while the back-end uplifting network is trained exclusively on physically-correct synthetic data . We specifically re-engineer the uplifting model to be robust to common real-world artifacts, such as missing detections and varying frame rates. By integrating a ball detector and a table keypoint detector , our approach transforms a proof-of-concept uplifting method into a practical, robust, and high-performing end-to-end application for 3D table tennis trajectory and spin analysis.","{'paper_name': 'Uplifting Table Tennis: A Robust, Real-World Application for 3D Trajectory and Spin Estimation', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.20250', 'upvote': 1, 'tags': ['end', 'uplifting', '3d']}"
doc2548103,"Large language models (LLMs) are widely used for factual tasks such as ""What treats asthma?"" or ""What is the capital of Latvia?"". However, it remains unclear how stably LLMs encode distinctions between true, false, and neither-true-nor-false content in their internal probabilistic representations. We introduce representational stability as the robustness of an LLM's veracity representations to perturbations in the operational definition of truth. We assess representational stability by (i) training a linear probe on an LLM's activations to separate true from not-true statements and (ii) measuring how its learned decision boundary shifts under controlled label changes. Using activations from sixteen open-source models and three factual domains, we compare two types of neither statements. The first are fact-like assertions about entities we believe to be absent from any training data. We call these unfamiliar neither statements . The second are nonfactual claims drawn from well-known fictional contexts. We call these familiar neither statements . The unfamiliar statements induce the largest boundary shifts, producing up to 40% flipped truth judgements in fragile domains (such as word definitions), while familiar fictional statements remain more coherently clustered and yield smaller changes (leq 8.2%). These results suggest that representational stability stems more from epistemic familiarity than from linguistic form. More broadly, our approach provides a diagnostic for auditing and training LLMs to preserve coherent truth assignments under semantic uncertainty , rather than optimizing for output accuracy alone.","{'paper_name': 'Representational Stability of Truth in Large Language Models', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.19166', 'upvote': 1, 'tags': ['statements', 'true', 'truth']}"
doc2548104,"Reinforcement Learning from Human Feedback (RLHF) is widely used for aligning large language models, yet practitioners face a persistent puzzle: improving safety often reduces fairness, scaling to diverse populations becomes computationally intractable, and making systems robust often amplifies majority biases. We formalize this tension as the Alignment Trilemma : no RLHF system can simultaneously achieve (i) epsilon-representativeness across diverse human values, (ii) polynomial tractability in sample and compute complexity, and (iii) delta-robustness against adversarial perturbations and distribution shift. Through a complexity-theoretic analysis integrating statistical learning theory and robust optimization , we prove that achieving both representativeness (epsilon <= 0.01) and robustness (delta <= 0.001) for global-scale populations requires Omega(2^{d_context}) operations, which is super-polynomial in the context dimensionality. We show that current RLHF implementations resolve this trilemma by sacrificing representativeness: they collect only 10^3--10^4 samples from homogeneous annotator pools while 10^7--10^8 samples are needed for true global representation. Our framework provides a unified explanation for documented RLHF pathologies including preference collapse , sycophancy , and systematic bias amplification . We conclude with concrete directions for navigating these fundamental trade-offs through strategic relaxations of alignment requirements.","{'paper_name': 'Position: The Complexity of Perfect AI Alignment -- Formalizing the RLHF Trilemma', 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.19504', 'upvote': 1, 'tags': ['rlhf', '10', 'representativeness']}"
doc2548105,"Large Language Models (LLMs) demonstrate partial forecasting competence across social, political, and economic events. Yet, their predictive ability varies sharply with domain structure and prompt framing . We investigate how forecasting performance varies with different model families on real-world questions about events that happened beyond the model cutoff date . We analyze how context , question type , and external knowledge affect accuracy and calibration , and how adding factual news context modifies belief formation and failure modes . Our results show that forecasting ability is highly variable as it depends on what, and how, we ask.","{'paper_name': ""Future Is Unevenly Distributed: Forecasting Ability of LLMs Depends on What We're Asking"", 'github_url': '', 'huggingface_url': 'https://huggingface.co/papers/2511.18394', 'upvote': 1, 'tags': ['forecasting', 'varies', 'events']}"
