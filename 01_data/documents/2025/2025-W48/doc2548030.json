{
  "context": "We introduce RynnVLA-002, a unified Vision-Language-Action (VLA) and world model . The world model leverages action and visual inputs to predict future image states , learning the underlying physics of the environment to refine action generation . Conversely, the VLA model produces subsequent actions from image observations, enhancing visual understanding and supporting the world model 's image generation. The unified framework of RynnVLA-002 enables joint learning of environmental dynamics and action planning . Our experiments show that RynnVLA-002 surpasses individual VLA and world model s, demonstrating their mutual enhancement. We evaluate RynnVLA-002 in both simulation and real-world robot tasks. RynnVLA-002 achieves 97.4% success rate on the LIBERO simulation benchmark without pretraining, while in real-world LeRobot experiments , its integrated world model boosts the overall success rate by 50%.",
  "metadata": {
    "paper_name": "RynnVLA-002: A Unified Vision-Language-Action and World Model",
    "github_url": "https://github.com/alibaba-damo-academy/RynnVLA-002",
    "huggingface_url": "https://huggingface.co/papers/2511.17502",
    "upvote": 24,
    "tags": [
      "world",
      "model",
      "rynnvla"
    ]
  }
}