{
  "context": "Neural information retrieval systems excel in high-resource languages but remain underexplored for morphologically rich, lower-resource languages such as Turkish.Dense bi-encoderscurrently dominate Turkish IR, yetlate-interaction models-- which retaintoken-level representationsforfine-grained matching-- have not been systematically evaluated. We introduceTurkColBERT, the first comprehensive benchmark comparing dense encoders andlate-interaction modelsfor Turkish retrieval. Our two-stage adaptation pipeline fine-tunes English and multilingual encoders onTurkish NLI/STS tasks, then converts them intoColBERT-style retrieversusingPyLatetrained onMS MARCO-TR. We evaluate 10 models across fiveTurkish BEIR datasetscovering scientific, financial, and argumentative domains. Results show strongparameter efficiency: the 1.0M-parametercolbert-hash-nano-tris 600times smaller than the 600Mturkish-e5-largedense encoder while preserving over 71\\% of itsaverage mAP.Late-interaction modelsthat are 3--5times smaller than dense encoders significantly outperform them;ColmmBERT-base-TRyields up to +13.8\\% mAP on domain-specific tasks. For production-readiness, we compareindexing algorithms:MUVERA+Rerankis 3.33times faster thanPLAIDand offers +1.7\\% relative mAP gain. This enables low-latency retrieval, withColmmBERT-base-TRachieving 0.54 msquery timesunderMUVERA. We release all checkpoints, configs, and evaluation scripts. Limitations include reliance on moderately sized datasets (leq50K documents) and translated benchmarks, which may not fully reflect real-world Turkish retrieval conditions; larger-scaleMUVERAevaluations remain necessary.",
  "metadata": {
    "paper_name": "TurkColBERT: A Benchmark of Dense and Late-Interaction Models for Turkish Information Retrieval",
    "github_url": null,
    "huggingface_url": "https://huggingface.co/papers/2511.16528",
    "upvote": "15",
    "tags": [
      "retrieval",
      "turkish",
      "dense"
    ]
  }
}