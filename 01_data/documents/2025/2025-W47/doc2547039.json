{
  "context": "Evaluating therobustnessofLarge Vision-Language Models(LVLMs) is essential for their continued development and responsible deployment in real-world applications. However, existingrobustnessbenchmarks typically focus onhallucinationor misleading textual inputs, while largely overlooking the equally critical challenge posed bymisleading visual inputsin assessing visual understanding. To fill this important gap, we introduceMVI-Bench, the first comprehensive benchmark specially designed for evaluating howMisleading Visual Inputsundermine therobustnessofLVLMs. Grounded in fundamentalvisual primitives, the design ofMVI-Benchcenters on three hierarchical levels ofmisleading visual inputs:Visual Concept,Visual Attribute, andVisual Relationship. Using this taxonomy, we curate six representative categories and compile 1,248 expertly annotatedVQAinstances. To facilitate fine-grainedrobustnessevaluation, we further introduceMVI-Sensitivity, a novel metric that characterizes LVLMrobustnessat a granular level. Empirical results across 18 state-of-the-artLVLMsuncover pronounced vulnerabilities tomisleading visual inputs, and our in-depth analyses onMVI-Benchprovide actionable insights that can guide the development of more reliable and robustLVLMs. The benchmark and codebase can be accessed at https://github.com/chenyil6/MVI-Bench.",
  "metadata": {
    "paper_name": "MVI-Bench: A Comprehensive Benchmark for Evaluating Robustness to Misleading Visual Inputs in LVLMs",
    "github_url": "https://github.com/chenyil6/MVI-Bench",
    "huggingface_url": "https://huggingface.co/papers/2511.14159",
    "upvote": "24",
    "tags": [
      "visual",
      "inputs",
      "bench"
    ]
  }
}