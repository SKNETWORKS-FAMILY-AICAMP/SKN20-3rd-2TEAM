{
  "context": "We present AraLingBench: a fully human annotated benchmark for evaluating the Arabic linguistic competence oflarge language models(LLMs). The benchmark spans five core categories:grammar,morphology,spelling,reading comprehension, andsyntax, through 150 expert-designedmultiple choice questionsthat directly assessstructural language understanding. Evaluating 35 Arabic and bilingualLLMsreveals that current models demonstrate strong surface level proficiency but struggle with deeper grammatical and syntactic reasoning. AraLingBench highlights a persistent gap between high scores on knowledge-based benchmarks and true linguistic mastery, showing that many models succeed through memorization or pattern recognition rather than authentic comprehension. By isolating and measuring fundamental linguistic skills, AraLingBench provides a diagnostic framework for developing ArabicLLMs. The full evaluation code is publicly available on GitHub.",
  "metadata": {
    "paper_name": "AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models",
    "github_url": "https://github.com/hammoudhasan/AraLingBench",
    "huggingface_url": "https://huggingface.co/papers/2511.14295",
    "upvote": "70",
    "tags": [
      "aralingbench",
      "linguistic",
      "models"
    ]
  }
}