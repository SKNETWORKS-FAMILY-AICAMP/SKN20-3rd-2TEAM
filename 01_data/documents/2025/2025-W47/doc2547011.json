{
  "context": "Video Modelshave achieved remarkable success in high-fidelityvideo generationwithcoherent motion dynamics. Analogous to the development from text generation to text-based reasoning in language modeling, the development ofvideo modelsmotivates us to ask: Canvideo modelsreason viavideo generation? Compared with the discrete text corpus, video grounds reasoning in explicit spatial layouts andtemporal continuity, which serves as an ideal substrate forspatial reasoning. In this work, we explore the reasoning via video paradigm and introduceVR-Bench-- a comprehensive benchmark designed to systematically evaluatevideo models' reasoning capabilities. Grounded inmaze-solving tasksthat inherently requirespatial planningandmulti-step reasoning,VR-Benchcontains 7,920 procedurally generated videos across five maze types and diverse visual styles. Our empirical analysis demonstrates thatSFTcan efficiently elicit the reasoning ability of video model.Video modelsexhibit stronger spatial perception during reasoning, outperformingleading VLMsand generalizing well across diverse scenarios, tasks, and levels of complexity. We further discover atest-time scaling effect, where diverse sampling during inference improves reasoning reliability by 10--20%. These findings highlight the unique potential and scalability of reasoning via video forspatial reasoningtasks.",
  "metadata": {
    "paper_name": "Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks",
    "github_url": "https://github.com/ImYangC7/VR-Bench",
    "huggingface_url": "https://huggingface.co/papers/2511.15065",
    "upvote": "73",
    "tags": [
      "reasoning",
      "video",
      "diverse"
    ]
  }
}