{
  "context": "Recent advances in image-text pretraining have significantly enhanced visual understanding by aligning visual and textual representations.Contrastive Language-Image Pretraining (CLIP)has played a pivotal role in multimodal learning. However, its focus on single-label, single-granularity alignment limits its effectiveness in complex domains such as medical imaging, where images often correspond to multiple high-level labels (e.g., disease categories) across different annotation granularities (e.g., diagnostic description, clinical explanation). To address this, we propose Multi-Granular Language Learning (MGLL), a contrastive learning framework designed to improve both multi-label and cross-granularity alignment. MGLL leveragesstructured multi-label supervision, integrates textual descriptions across granularities, and introducessoft-label supervisionwith point-wise constraints to enhance alignment. MGLL employssmooth Kullback-Leibler (KL) divergenceto ensure cross-granularity consistency while maintaining computational efficiency as a plug-and-play module forvision-language models. Pretrained on our constructed large-scale multi-granular datasets and evaluated across multiple datasets, MGLL outperforms other state-of-the-art methods in downstream tasks. The code is available at https://github.com/HUANGLIZI/MGLL{https://github.com/HUANGLIZI/MGLL}.",
  "metadata": {
    "paper_name": "Boosting Medical Visual Understanding From Multi-Granular Language Learning",
    "github_url": "https://github.com/HUANGLIZI/MGLL",
    "huggingface_url": "https://huggingface.co/papers/2511.15943",
    "upvote": "1",
    "tags": [
      "mgll",
      "label",
      "multi"
    ]
  }
}