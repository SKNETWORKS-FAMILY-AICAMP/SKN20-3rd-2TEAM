{
  "context": "We introduceTimeViper, ahybrid vision-language modeldesigned to tackle challenges of long video understanding. Processing long videos demands both an efficient model architecture and an effective mechanism for handling extended temporal contexts. To this end,TimeViperadopts a hybridMamba-Transformerbackbone that combines the efficiency ofstate-space modelswith the expressivity ofattention mechanisms. Through this hybrid design, we reveal thevision-to-text information aggregationphenomenon, where information progressively flows fromvision tokenstotext tokensacross increasingLLM depth, resulting in severevision token redundancy. Motivated by this observation, we proposeTransV, atoken information transfer modulethat transfers and compressesvision tokensinto instruction tokens while maintainingmultimodal understandingcapabilities. This design enablesTimeViperto process hour-long videos exceeding 10,000 frames. Extensive experiments across multiple benchmarks demonstrate thatTimeVipercompetes with state-of-the-art models while extending frame numbers. We further analyze attention behaviors of both Mamba and Transformer layers, offering new insights intohybrid model interpretability. This work represents an initial step towards developing, interpreting, and compressing hybridMamba-Transformerarchitectures.",
  "metadata": {
    "paper_name": "TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding",
    "github_url": "https://github.com/xiaomi-research/timeviper",
    "huggingface_url": "https://huggingface.co/papers/2511.16595",
    "upvote": "9",
    "tags": [
      "information",
      "long",
      "design"
    ]
  }
}