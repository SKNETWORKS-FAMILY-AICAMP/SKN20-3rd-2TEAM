{
  "context": "Surgical video segmentation is crucial for computer-assisted surgery, enabling precise localization and tracking of instruments and tissues.Interactive Video Object Segmentation(iVOS) models such asSegment Anything Model 2(SAM2) provide prompt-based flexibility beyond methods with predefined categories, but face challenges in surgical scenarios due to the domain gap and limited long-term tracking. To address these limitations, we constructSA-SV, the largest surgicaliVOSbenchmark with instance-level spatio-temporal annotations (masklets) spanning eight procedure types (61k frames, 1.6k masklets), enabling comprehensive development and evaluation for long-term tracking andzero-shot generalization. Building onSA-SV, we proposeSAM2S, a foundation model enhancingSAM2for SurgicaliVOSthrough: (1)DiveMem, a trainable diverse memory mechanism for robust long-term tracking; (2)temporal semantic learningfor instrument understanding; and (3)ambiguity-resilient learningto mitigate annotation inconsistencies across multi-source datasets. Extensive experiments demonstrate that fine-tuning onSA-SVenables substantial performance gains, withSAM2improving by 12.99 average J\\&F over vanillaSAM2.SAM2S further advances performance to 80.42 average J\\&F, surpassing vanilla and fine-tunedSAM2by 17.10 and 4.11 points respectively, while maintaining 68 FPSreal-time inferenceand strongzero-shot generalization. Code and dataset will be released at https://jinlab-imvr.github.io/SAM2S.",
  "metadata": {
    "paper_name": "SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking",
    "github_url": "https://github.com/jinlab-imvr/SAM2S",
    "huggingface_url": "https://huggingface.co/papers/2511.16618",
    "upvote": "7",
    "tags": [
      "tracking",
      "long",
      "term"
    ]
  }
}