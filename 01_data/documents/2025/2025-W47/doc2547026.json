{
  "context": "Reinforcement learning(RL) provides a principled framework for improvingVision-Language Models(VLMs) on complex reasoning tasks. However, existing RL approaches often rely on human-annotated labels or task-specific heuristics to define verifiable rewards, both of which are costly and difficult to scale. We introduce VisPlay, aself-evolving RL frameworkthat enables VLMs to autonomously improve their reasoning abilities using large amounts of unlabeled image data. Starting from a single base VLM, VisPlay assigns the model into two interacting roles: anImage-Conditioned Questionerthat formulates challenging yet answerable visual questions, and aMultimodal Reasonerthat generates silver responses. These roles are jointly trained withGroup Relative Policy Optimization(GRPO), which incorporates diversity and difficulty rewards to balance the complexity of generated questions with the quality of the silver answers. VisPlay scales efficiently across two model families. When trained on Qwen2.5-VL and MiMo-VL, VisPlay achieves consistent improvements invisual reasoning,compositional generalization, andhallucination reductionacross eight benchmarks, including MM-Vet and MMMU, demonstrating a scalable path toward self-evolving multimodal intelligence. The project page is available at https://bruno686.github.io/VisPlay/",
  "metadata": {
    "paper_name": "VisPlay: Self-Evolving Vision-Language Models from Images",
    "github_url": "https://github.com/bruno686/VisPlay",
    "huggingface_url": "https://huggingface.co/papers/2511.15661",
    "upvote": "41",
    "tags": [
      "visplay",
      "reasoning",
      "rl"
    ]
  }
}