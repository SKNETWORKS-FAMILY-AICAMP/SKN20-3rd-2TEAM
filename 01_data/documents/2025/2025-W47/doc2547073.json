{
  "context": "Long-term training oflarge language models(LLMs) requires maintaining stable exploration to prevent the model from collapsing into sub-optimal behaviors.Entropyis crucial in this context, as it controls exploration and helps avoid premature convergence to sub-optimal solutions. However, existingreinforcement learningmethods struggle to maintain an appropriate level ofentropy, as the training process involves a mix ofpositive and negative samples, each affectingentropyin different ways across steps. To address this, we proposeEntropystablilization viaProportional-Integral Control(EntroPIC), a novel method that adaptively adjusts the influence ofpositive and negative samplesby dynamically tuning their loss coefficients. This approach stabilizesentropythroughout training, ensuring efficient exploration and steady progress. We provide a comprehensive theoretical analysis for both on-policy andoff-policy learningsettings, demonstrating that EntroPIC is effective at controllingentropyin large-scale LLM training. Experimental results show that our method successfully maintains desiredentropylevels, enabling stable and optimal RL training for LLMs.",
  "metadata": {
    "paper_name": "EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization with Proportional-Integral Control",
    "github_url": "https://github.com/yk7333/EntroPIC",
    "huggingface_url": "https://huggingface.co/papers/2511.15248",
    "upvote": "5",
    "tags": [
      "training",
      "exploration",
      "optimal"
    ]
  }
}