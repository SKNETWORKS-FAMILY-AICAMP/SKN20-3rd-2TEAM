{
  "context": "Large language models(LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or partially open, limiting transparency and reproducibility. In this work, we introduceInstella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered byAMD Instinct MI300X GPUs,Instellais developed throughlarge-scale pre-training,general-purpose instruction tuning, andalignment with human preferences. Despite using substantially fewer pre-training tokens than many contemporaries,Instellaachieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size. We further release two specialized variants:Instella-Long, capable of handling context lengths up to 128K tokens, andInstella-Math, a reasoning-focused model enhanced throughsupervised fine-tuningandreinforcement learningon mathematical tasks. Together, these contributions establishInstellaas a transparent, performant, and versatile alternative for the community, advancing the goal of open and reproducible language modeling research.",
  "metadata": {
    "paper_name": "Instella: Fully Open Language Models with Stellar Performance",
    "github_url": "https://github.com/AMD-AGI/Instella",
    "huggingface_url": "https://huggingface.co/papers/2511.10628",
    "upvote": "4",
    "tags": [
      "models",
      "open",
      "language"
    ]
  }
}