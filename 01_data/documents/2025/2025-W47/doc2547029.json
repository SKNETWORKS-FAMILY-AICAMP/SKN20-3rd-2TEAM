{
  "context": "We introduceVirtual Width Networks(VWN), a framework that delivers the benefits of wider representations without incurring the quadratic cost of increasing the hidden size.VWNdecouplesrepresentational widthfrombackbone width, expanding theembedding spacewhile keeping backbone compute nearly constant. In our large-scale experiment, an 8-times expansion accelerates optimization by over 2 times for next-token and 3 times fornext-2-token prediction. The advantage amplifies over training as both the loss gap grows and the convergence-speedup ratio increases, showing thatVWNis not onlytoken-efficientbut also increasingly effective with scale. Moreover, we identify an approximately log-linear scaling relation between virtual width andloss reduction, offering an initial empirical basis and motivation for exploring virtual-width scaling as a new dimension of large-model efficiency.",
  "metadata": {
    "paper_name": "Virtual Width Networks",
    "github_url": null,
    "huggingface_url": "https://huggingface.co/papers/2511.11238",
    "upvote": "35",
    "tags": [
      "width",
      "times",
      "large"
    ]
  }
}