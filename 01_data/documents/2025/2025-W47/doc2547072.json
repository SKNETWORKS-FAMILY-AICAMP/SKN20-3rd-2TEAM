{
  "context": "Open-vocabulary detectorsachieve impressive performance onCOCO, but often fail to generalize to real-world datasets with out-of-distribution classes not typically found in their pre-training. Rather than simply fine-tuning a heavy-weightvision-language model(VLM) for new domains, we introduce RF-DETR, a light-weight specialistdetection transformerthat discovers accuracy-latency Pareto curves for any target dataset withweight-sharing neural architecture search(NAS). Ourapproach fine-tunes a pre-trained base network on a target dataset and evaluates thousands of network configurations with different accuracy-latency tradeoffs without re-training. Further, we revisit the \"tunable knobs\" forNASto improve the transferability of DETRs to diverse target domains. Notably, RF-DETR significantly improves on prior state-of-the-artreal-time methodsonCOCOandRoboflow100-VL. RF-DETR (nano) achieves 48.0APonCOCO, beating D-FINE (nano) by 5.3APat similar latency, and RF-DETR (2x-large) outperforms GroundingDINO (tiny) by 1.2APonRoboflow100-VLwhile running 20x as fast. To the best of our knowledge, RF-DETR (2x-large) is the first real-time detector to surpass 60APonCOCO. Our code is at https://github.com/roboflow/rf-detr",
  "metadata": {
    "paper_name": "RF-DETR: Neural Architecture Search for Real-Time Detection Transformers",
    "github_url": null,
    "huggingface_url": "https://huggingface.co/papers/2511.09554",
    "upvote": "6",
    "tags": [
      "detr",
      "rf",
      "fine"
    ]
  }
}