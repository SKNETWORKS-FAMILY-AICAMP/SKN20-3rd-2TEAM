{
  "context": "To provide a unified and flexible solution for daily communication among hearing-impaired individuals, we introduce theOmni-Modelparadigm into assistive technology and presentHI-TransPA, aninstruction-drivenaudio-visual personal assistant. The model fusesindistinct speechwithhigh-frame-rate lip dynamics, enabling both translation and dialogue within a single multimodal framework. To tackle the challenges of noisy and heterogeneous raw data and the limited adaptability of existingOmni-Models to hearing-impaired speech, we construct a comprehensivepreprocessingandcuration pipelinethat detectsfacial landmarks, isolates and stabilizes the lip region, and quantitatively assessesmultimodal sample quality. These quality scores guide acurriculum learningstrategy that first trains on clean, high-confidence samples and progressively incorporates harder cases to strengthen model robustness. We further adopt aSigLIP encodercombined with aUnified 3D-Resamplerto efficiently encode high-frame-rate lip motion. Experiments on our purpose-builtHI-Dialogue datasetshow thatHI-TransPAachieves state-of-the-art performance in bothliteral accuracyandsemantic fidelity. This work establishes a foundation for applyingOmni-Models to assistive communication technology, providing an end-to-end modeling framework and essential processing tools for future research.",
  "metadata": {
    "paper_name": "HI-TransPA: Hearing Impairments Translation Personal Assistant",
    "github_url": null,
    "huggingface_url": "https://huggingface.co/papers/2511.09915",
    "upvote": "6",
    "tags": [
      "lip",
      "assistive",
      "communication"
    ]
  }
}