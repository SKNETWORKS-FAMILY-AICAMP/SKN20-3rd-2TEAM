{
  "context": "We present SAM 3D, agenerative modelfor visually grounded3D object reconstruction, predictinggeometry,texture, andlayoutfrom a single image. SAM 3D excels in natural images, where occlusion and scene clutter are common and visual recognition cues from context play a larger role. We achieve this with a human- and model-in-the-loop pipeline for annotatingobject shape,texture, and pose, providing visually grounded 3D reconstruction data at unprecedented scale. We learn from this data in a modern,multi-stage trainingframework that combinessynthetic pretrainingwithreal-world alignment, breaking the 3D \"data barrier\". We obtain significant gains over recent work, with at least a 5:1 win rate inhuman preference testson real-world objects and scenes. We will release our code and model weights, an online demo, and a new challengingbenchmarkfor in-the-wild3D object reconstruction.",
  "metadata": {
    "paper_name": "SAM 3D: 3Dfy Anything in Images",
    "github_url": "https://github.com/facebookresearch/sam-3d-objects",
    "huggingface_url": "https://huggingface.co/papers/2511.16624",
    "upvote": "102",
    "tags": [
      "3d",
      "data",
      "reconstruction"
    ]
  }
}