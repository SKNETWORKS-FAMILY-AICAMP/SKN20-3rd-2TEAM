{
  "context": "Recent advances inunified multimodal models(UMMs) have enabled impressive progress in visualcomprehensionandgeneration. However, existing datasets and benchmarks focus primarily on single-turn interactions, failing to capture the multi-turn, context-dependent nature of real-world image creation andediting. To address this gap, we presentWEAVE, the first suite for in-context interleaved cross-modalitycomprehensionandgeneration. Our suite consists of two complementary parts.WEAVE-100kis a large-scale dataset of 100Kinterleaved samplesspanning over 370Kdialogue turnsand 500K images, coveringcomprehension,editing, andgenerationtasks that require reasoning over historical context.WEAVEBenchis ahuman-annotated benchmarkwith 100 tasks based on 480 images, featuring a hybridVLM judgerevaluation framework based on both the reference image and the combination of the original image witheditinginstructions that assesses models' abilities inmulti-turn generation,visual memory, andworld-knowledge reasoningacross diverse domains. Experiments demonstrate that training onWEAVE-100kenablesvision comprehension,image editing, andcomprehension-generation collaborationcapabilities. Furthermore, it facilitates UMMs to developemergent visual-memory capabilities, while extensive evaluations onWEAVEBenchexpose the persistent limitations and challenges of current approaches in multi-turn, context-aware imagegenerationandediting. We believeWEAVEprovides a view and foundation for studying in-context interleavedcomprehensionandgenerationfor multi-modal community.",
  "metadata": {
    "paper_name": "WEAVE: Unleashing and Benchmarking the In-context Interleaved Comprehension and Generation",
    "github_url": "https://github.com/weichow23/weave",
    "huggingface_url": "https://huggingface.co/papers/2511.11434",
    "upvote": "44",
    "tags": [
      "context",
      "image",
      "turn"
    ]
  }
}