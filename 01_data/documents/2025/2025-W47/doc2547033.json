{
  "context": "User interface (UI) programming is a core yet highly complex part of modern software development. Recent advances invisual language models(VLMs) highlight the potential of automatic UI coding, but current approaches face two key limitations: multimodal coding capabilities remain underdeveloped, and single-turn paradigms make little use of iterative visual feedback. We address these challenges with an interactiveUI-to-codeparadigm that better reflects real-world workflows and raises the upper bound of achievable performance. Under this paradigm, we present UI2Code^N, a visual language model trained throughstaged pretraining,fine-tuning, andreinforcement learningto achieve foundational improvements in multimodal coding. The model unifies three key capabilities:UI-to-code generation,UI editing, andUI polishing. We further explore test-time scaling for interactive generation, enabling systematic use ofmulti-turn feedback. Experiments onUI-to-codeandUI polishing benchmarksshow that UI2Code^N establishes a new state of the art among open-source models and achieves performance comparable to leading closed-source models such as Claude-4-Sonnet and GPT-5. Our code and models are available at https://github.com/zai-org/UI2Code_N.",
  "metadata": {
    "paper_name": "UI2Code^N: A Visual Language Model for Test-Time Scalable Interactive UI-to-Code Generation",
    "github_url": "https://github.com/zai-org/UI2Code_N",
    "huggingface_url": "https://huggingface.co/papers/2511.08195",
    "upvote": "30",
    "tags": [
      "models",
      "ui",
      "coding"
    ]
  }
}