{
  "context": "The evaluation ofdiscourse-level translationin expert domains remains inadequate, despite its centrality to knowledge dissemination and cross-lingual scholarly communication. While these translations demand discourse-level coherence and strict terminological precision, current evaluation methods predominantly focus on segment-levelaccuracyandfluency. To address this limitation, we introduceDiscoX, a new benchmark for discourse-level and expert-level Chinese-English translation. It comprises 200 professionally-curated texts from 7 domains, with an average length exceeding 1700 tokens. To evaluate performance onDiscoX, we also developMetric-S, areference-free systemthat provides fine-grained automatic assessments acrossaccuracy,fluency, andappropriateness.Metric-Sdemonstrates strong consistency withhuman judgments, significantly outperforming existing metrics. Our experiments reveal a remarkable performance gap: even the most advancedLLMsstill trail human experts on these tasks. This finding validates the difficulty ofDiscoXand underscores the challenges that remain in achievingprofessional-grade machine translation. The proposed benchmark and evaluation system provide a robust framework for more rigorous evaluation, facilitating future advancements in LLM-based translation.",
  "metadata": {
    "paper_name": "DiscoX: Benchmarking Discourse-Level Translation task in Expert Domains",
    "github_url": null,
    "huggingface_url": "https://huggingface.co/papers/2511.10984",
    "upvote": "4",
    "tags": [
      "evaluation",
      "level",
      "translation"
    ]
  }
}