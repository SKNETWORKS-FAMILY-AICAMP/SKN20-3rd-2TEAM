{
  "context": "Omnimodal large language models (OmniLLMs) have attracted increasing research attention of late towards unified audio-video understanding, wherein processing audio-video token sequences creates a significant computational bottleneck, however. Existing token compression methods have yet to accommodate this emerging need of jointly compressing multimodal tokens. To bridge this gap, we present OmniZip, a training-free, audio-guidedaudio-visual token-compressionframework that optimizes multimodal token representation and accelerates inference. Specifically, OmniZip first identifiessalient audio tokens, then computes anaudio retention scorefor each time group to capture information density, thereby dynamically guidingvideo token pruningand preserving cues from audio anchors enhanced bycross-modal similarity. For each time window, OmniZip compresses the video tokens using aninterleaved spatio-temporal scheme. Extensive empirical results demonstrate the merits of OmniZip - it achieves 3.42X inference speedup and 1.4X memory reduction over other top-performing counterparts, while maintaining performance with no training.",
  "metadata": {
    "paper_name": "OmniZip: Audio-Guided Dynamic Token Compression for Fast Omnimodal Large Language Models",
    "github_url": "https://github.com/KD-TAO/OmniZip",
    "huggingface_url": "https://huggingface.co/papers/2511.14582",
    "upvote": "17",
    "tags": [
      "audio",
      "token",
      "omnizip"
    ]
  }
}