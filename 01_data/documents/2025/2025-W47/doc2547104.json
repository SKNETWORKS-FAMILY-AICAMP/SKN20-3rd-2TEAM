{
  "context": "Vehicle-to-Vehicle (V2V) cooperative perception has great potential to enhance autonomous driving performance by overcoming perception limitations in complex adverse traffic scenarios (CATS). Meanwhile, data serves as the fundamental infrastructure for modern autonomous driving AI. However, due to stringent data collection requirements, existing datasets focus primarily on ordinary traffic scenarios, constraining the benefits of cooperative perception. To address this challenge, we introduce CATS-V2V, the first-of-its-kind real-world dataset for V2V cooperative perception under complex adverse traffic scenarios. The dataset was collected by two hardware time-synchronized vehicles, covering 10 weather and lighting conditions across 10 diverse locations. The 100-clip dataset includes 60K frames of 10 HzLiDAR point cloudsand 1.26M multi-view 30 Hz camera images, along with 750K anonymized yet high-precisionRTK-fixed GNSSandIMU records. Correspondingly, we provide time-consistent3D bounding box annotationsfor objects, as well as static scenes to construct a4D BEV representation. On this basis, we propose atarget-based temporal alignmentmethod, ensuring that all objects are precisely aligned across all sensor modalities. We hope that CATS-V2V, the largest-scale, most supportive, and highest-quality dataset of its kind to date, will benefit the autonomous driving community in related tasks.",
  "metadata": {
    "paper_name": "CATS-V2V: A Real-World Vehicle-to-Vehicle Cooperative Perception Dataset with Complex Adverse Traffic Scenarios",
    "github_url": null,
    "huggingface_url": "https://huggingface.co/papers/2511.11168",
    "upvote": "-",
    "tags": [
      "dataset",
      "perception",
      "v2v"
    ]
  }
}