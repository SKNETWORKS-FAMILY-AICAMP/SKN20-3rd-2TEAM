{
  "context": "Emotion plays a pivotal role in video-based expression, but existing video generation systems predominantly focus on low-level visual metrics while neglecting affective dimensions. Althoughemotion analysishas made progress in the visual domain, the video community lacks dedicated resources to bridge emotion understanding with generative tasks, particularly for stylized and non-realistic contexts. To address this gap, we introduce EmoVid, the firstmultimodal,emotion-annotatedvideo dataset specifically designed for creative media, which includes cartoon animations, movie clips, and animated stickers. Each video is annotated with emotion labels, visual attributes (brightness, colorfulness, hue), and text captions. Through systematic analysis, we uncover spatial and temporal patterns linking visual features to emotional perceptions across diverse video forms. Building on these insights, we develop an emotion-conditioned video generation technique by fine-tuning theWan2.1 model. The results show a significant improvement in both quantitative metrics and the visual quality of generated videos fortext-to-videoandimage-to-videotasks. EmoVid establishes a new benchmark foraffective video computing. Our work not only offers valuable insights into visualemotion analysisin artistically styled videos, but also provides practical methods for enhancing emotional expression in video generation.",
  "metadata": {
    "paper_name": "EmoVid: A Multimodal Emotion Video Dataset for Emotion-Centric Video Understanding and Generation",
    "github_url": null,
    "huggingface_url": "https://huggingface.co/papers/2511.11002",
    "upvote": "3",
    "tags": [
      "video",
      "emotion",
      "visual"
    ]
  }
}