{
  "context": "Artemis, a perception-policy learning framework, enhances performance on visual tasks by using structured spatial reasoning with (label, bounding-box) pairs instead of linguistic intermediate reasoning. Recentreinforcement-learning frameworksforvisual perception policyhave begun to incorporateintermediate reasoning chainsexpressed innatural language. Empirical observations indicate that such purely linguistic intermediate reasoning often reduces performance on perception tasks. We argue that the core issue lies not in reasoning per se but in the form of reasoning: while these chains performsemantic reasoningin an unstructured linguistic space, visual perception requires reasoning in a spatial andobject-centric space. In response, we introduce Artemis, a perception-policy learning framework that performsstructured proposal-based reasoning, where each intermediate step is represented as a (label,bounding-box) pair capturing averifiable visual state. This design enables explicit tracking of intermediate states,direct supervisionfor proposal quality, and avoidsambiguityintroduced by language-based reasoning. Artemis is built on Qwen2.5-VL-3B, achieves strong performance ongroundinganddetectiontask and exhibits substantial generalization tocountingandgeometric-perceptiontasks. The consistent improvements across these diverse settings confirm that aligning reasoning with spatial representations enhances perception-policy learning. Owing to its strengthened visual reasoning, Artemis also achieves competitive performance on generalMLLM benchmarks, illustrating that spatially grounded reasoning provides a principled route toward scalable and general perception policies. Recent reinforcement-learning frameworks for visual perception policy have begun to incorporate intermediate reasoning chains expressed in natural language. Empirical observations indicate that such purely linguistic intermediate reasoning often reduces performance on perception tasks. We argue that the core issue lies not in reasoning per se but in the form of reasoning: while these chains perform semantic reasoning in an unstructured linguistic space,visual perception requires reasoning in a spatial and object-centric space.In response, we introduceArtemis, a perception-policy learning framework that performs structured proposal-based reasoning, where each intermediate step is represented as a (label, bounding-box) pair capturing a verifiable visual state. This design enables explicit tracking of intermediate states, direct supervision for proposal quality, and avoids ambiguity introduced by language-based reasoning.Artemis is built on Qwen2.5-VL-3B, achieves strong performance on grounding and detection task and exhibits substantial generalization to counting and geometric-perception tasks. The consistent improvements across these diverse settings confirm that aligning reasoning with spatial representations enhances perception-policy learning. Owing to its strengthened visual reasoning, Artemis also achieves competitive performance on general MLLM benchmarks, illustrating that spatially grounded reasoning provides a principled route toward scalable and general perception policies. Â·Sign uporlog into comment",
  "metadata": {
    "paper_name": "Artemis: Structured Visual Reasoning for Perception Policy Learning",
    "github_url": "https://github.com/WayneTomas/Artemis",
    "huggingface_url": "https://huggingface.co/papers/2512.01988",
    "upvote": 1,
    "tags": [
      "reasoning",
      "perception",
      "visual"
    ]
  }
}