{
  "context": "Video4Spatial demonstrates that video diffusion models can perform complex spatial tasks using only visual data, achieving strong spatial understanding and generalization. We investigate whether video generative models can exhibit visuospatial intelligence, a capability central to human cognition, using only visual data. To this end, we present Video4Spatial, a framework showing thatvideo diffusion modelsconditioned solely on video-based scene context can perform complex spatial tasks. We validate on two tasks:scene navigation- following camera-pose instructions while remaining consistent with3D geometryof the scene, andobject grounding- which requiressemantic localization,instruction following, andplanning. Both tasks usevideo-only inputs, without auxiliary modalities such as depth or poses. With simple yet effective design choices in the framework and data curation, Video4Spatial demonstrates strong spatial understanding from video context: it plans navigation and grounds target objects end-to-end, follows camera-pose instructions while maintainingspatial consistency, and generalizes to long contexts and out-of-domain environments. Taken together, these results advance video generative models toward general visuospatial reasoning.  Â·Sign uporlog into comment",
  "metadata": {
    "paper_name": "Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation",
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2512.03040",
    "upvote": 0,
    "tags": [
      "video",
      "tasks",
      "spatial"
    ]
  }
}