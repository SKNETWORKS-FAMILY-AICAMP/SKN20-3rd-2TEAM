{
  "context": "YingVideo-MV generates high-quality music performance videos with synchronized camera motion using cascaded frameworks, audio semantic analysis, and temporal-aware diffusion Transformers. Whilediffusion modelforaudio-driven avatar video generationhave achieved notable process in synthesizing long sequences with natural audio-visual synchronization and identity consistency, the generation of music-performance videos with camera motions remains largely unexplored. We present YingVideo-MV, the first cascaded framework for music-driven long-video generation. Our approach integratesaudio semantic analysis, an interpretableshot planning module(MV-Director),temporal-aware diffusion Transformer architectures, andlong-sequence consistency modelingto enable automatic synthesis of high-quality music performance videos from audio signals. We construct a large-scaleMusic-in-the-Wild Datasetby collecting web data to support the achievement of diverse, high-quality results. Observing that existing long-video generation methods lack explicit camera motion control, we introduce acamera adapter modulethat embeds camera poses intolatent noise. To enhance continulity between clips during long-sequence inference, we further propose atime-aware dynamic window range strategythat adaptively adjustdenoising rangesbased onaudio embedding. Comprehensive benchmark tests demonstrate that YingVideo-MV achieves outstanding performance in generating coherent and expressive music videos, and enables precisemusic-motion-camera synchronization. More videos are available in our project page: https://giantailab.github.io/YingVideo-MV/ .  Â·Sign uporlog into comment",
  "metadata": {
    "paper_name": "YingVideo-MV: Music-Driven Multi-Stage Video Generation",
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2512.02492",
    "upvote": 0,
    "tags": [
      "videos",
      "camera",
      "music"
    ]
  }
}