{
  "context": "AlignBench evaluates image-text models using detailed captions generated by diverse models, revealing insights into their alignment and compositional reasoning capabilities. Assessingimage-text alignmentmodels such asCLIPis crucial for bridging visual and linguistic representations. Yet existing benchmarks rely on rule-based perturbations or short captions, limiting their ability to measure fine-grained alignment. We introduceAlignBench, a benchmark that provides a new indicator ofimage-text alignmentby evaluating detailed image-caption pairs generated by diverseimage-to-textandtext-to-imagemodels. Each sentence is annotated for correctness, enabling direct assessment of VLMs as alignment evaluators. Benchmarking a wide range ofdecoder-based VLMsreveals three key findings: (i)CLIP-based models, even those tailored forcompositional reasoning, remain nearly blind; (ii) detectors systematically over-score early sentences; and (iii) they show strongself-preference, favoring their own outputs and harming detection performance. Our project page will be available at https://dahlian00.github.io/AlignBench/. Assessing image-text alignment models such as CLIP is crucial for bridging visual and linguistic representations. Yet existing benchmarks rely on rule-based perturbations or short captions, limiting their ability to measure fine-grained alignment. We introduce AlignBench, a benchmark that provides a new indicator of image-text alignment by evaluating detailed image-caption pairs generated by diverse image-to-text and text-to-image models. Each sentence is annotated for correctness, enabling direct assessment of VLMs as alignment evaluators. Benchmarking a wide range of decoder-based VLMs reveals three key findings: (i) CLIP-based models, even those tailored for compositional reasoning, remain nearly blind; (ii) detectors systematically over-score early sentences; and (iii) they show strong self-preference, favoring their own outputs and harming detection performance. Our project page will be available athttps://dahlian00.github.io/AlignBench/. Â·Sign uporlog into comment",
  "metadata": {
    "paper_name": "AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs",
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2511.20515",
    "upvote": 3,
    "tag1": "alignment",
    "tag2": "image",
    "tag3": "text"
  }
}