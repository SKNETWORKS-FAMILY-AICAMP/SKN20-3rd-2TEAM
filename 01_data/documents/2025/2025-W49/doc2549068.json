{
  "context": "INSPO, a novel Instruction-Policy co-evolution framework, dynamically optimizes instructions within the reinforcement learning loop, enhancing performance in multi-turn retrieval and reasoning tasks compared to static instruction-based methods. Reinforcement Learning with Verifiable Rewards (RLVR)has advanced the reasoning capability oflarge language models (LLMs), enabling autonomous agents that can conduct effective multi-turn andtool-integrated reasoning. While instructions serve as the primary protocol for defining agents, RLVR typically relies on static and manually designed instructions. However, those instructions may be suboptimal for the base model, and the optimal instruction may change as the agent's policy improves and explores the interaction with the environment. To bridge the gap, we introduce INSPO, a novel Instruction-Policy co-evolution framework that integratesinstruction optimizationas adynamic componentof thereinforcement learning (RL) loop. INSPO maintains a dynamic population ofinstruction candidatesthat are sampled with questions, wherereward signalsin RL loops are automatically attributed to each instruction, and low performers are periodically pruned. New instructions are generated and verified through anon-policy reflection mechanism, where anLLM-based optimizeranalyzes past experience from areplay bufferand evolves more effective strategies given the current policy. We conduct extensive experiments on multi-turn retrieval and reasoning tasks, demonstrating that INSPO substantially outperforms strong baselines relying on static instructions. INSPO discovers innovative instructions that guide the agent toward morestrategic reasoning paths, achieving substantial performance gains with only a marginal increase in computational overhead. INSPO is a novel Instruction-Policy Co-Evolution framework that integrates instruction optimization as a dynamic component of the reinforcement learning (RL) loop for training large language models. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "paper_name": "Agentic Policy Optimization via Instruction-Policy Co-Evolution",
    "github_url": "https://github.com/cambridgeltl/inspo",
    "huggingface_url": "https://huggingface.co/papers/2512.01945",
    "upvote": 3,
    "tags": [
      "instruction",
      "instructions",
      "inspo"
    ]
  }
}