{
  "context": "SwiftVLA enhances compact Vision-Language-Action models with 4D understanding using Fusion Tokens and a mask-and-reconstruct strategy, achieving high performance with reduced computational and memory demands. Vision-Language-Action (VLA) models built onpretrained Vision-Language Models(VLMs) show strong potential but are limited in practicality due to their large parameter counts. To mitigate this issue, using alightweight VLMhas been explored, but it compromisesspatiotemporal reasoning. Although some methods suggest that incorporating additional3D inputscan help, they usually rely on large VLMs to fuse 3D and 2D inputs and still lack temporal understanding. Therefore, we propose SwiftVLA, an architecture that enhances a compact model with 4D understanding while preserving design efficiency. Specifically, our approach features a pretrained4D visual geometry transformerwith atemporal cachethat extracts 4D features from 2D images. Then, to enhance the VLM's ability to exploit both 2D images and 4D features, we introduceFusion Tokens, a set of learnable tokens trained with afuture prediction objectiveto generate unified representations for action generation. Finally, we introduce amask-and-reconstruct strategythat masks 4D inputs to the VLM and trains the VLA to reconstruct them, enabling the VLM to learn effective4D representationsand allowing the 4D branch to be dropped at inference with minimal performance loss. Experiments in real and simulated environments show that SwiftVLA outperforms lightweight baselines and rivals VLAs up to 7 times larger, achieving comparable performance onedge deviceswhile being 18 times faster and reducing memory footprint by 12 times. https://github.com/GigaAI-research/SwiftVLA https://swiftvla.github.io/ This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "paper_name": "SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead",
    "github_url": "https://github.com/GigaAI-research/SwiftVLA",
    "huggingface_url": "https://huggingface.co/papers/2512.00903",
    "upvote": 5,
    "tags": [
      "swiftvla",
      "action",
      "comment"
    ]
  }
}