{
  "context": "The paper provides a theoretical foundation for optimizing sequence-level rewards in reinforcement learning using token-level objectives, highlighting the importance of techniques like importance sampling correction, clipping, and Routing Replay for stabilizing training, especially with large language models. This paper proposes a novel formulation forreinforcement learning(RL) withlarge language models, explaining why and under what conditions the truesequence-level rewardcan be optimized via a surrogatetoken-level objectiveinpolicy gradient methodssuch asREINFORCE. Specifically, through afirst-order approximation, we show that this surrogate becomes increasingly valid only when both thetraining-inference discrepancyandpolicy stalenessare minimized. This insight provides a principled explanation for the crucial role of several widely adopted techniques in stabilizing RL training, includingimportance sampling correction,clipping, and particularlyRouting ReplayforMixture-of-Experts(MoE) models. Through extensive experiments with a 30B MoE model totaling hundreds of thousands of GPU hours, we show that foron-policy training, the basic policy gradient algorithm withimportance sampling correctionachieves the highest training stability. Whenoff-policy updatesare introduced to accelerate convergence, combiningclippingandRouting Replaybecomes essential to mitigate the instability caused bypolicy staleness. Notably, once training is stabilized, prolonged optimization consistently yields comparable final performance regardless of cold-start initialization. We hope that the shared insights and the developed recipes for stable RL training will facilitate future research. From the simple and intuitive perspective of \"first-order approximation\", we formulate and explain the rationale behind optimizing sequence-level rewards using token-level objectives, and highlight that the validity of this approximation requires minimizing both the \"train-inference gap\" and \"policy staleness\". Our formulation provides a principled explanation: stabilization techniques such as importance sampling (IS) correction, clipping, and Routing Replay all fundamentally serve to maintain the validity of this first-order approximation. We conduct extensive experiments using a 30B MoE model (over ×00,000 GPU hours, with FP8 inference and BF16 training), which strongly validate the above predictions and help us identify effective recipes for stable RL training. In particular, we demonstrate that as long as training remains stable over the long term, different cold-start initializations consistently converge to similar performance levels. We firmly believe that stability is the key to scaling reinforcement learning! This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ·Sign uporlog into comment",
  "metadata": {
    "paper_name": "Stabilizing Reinforcement Learning with LLMs: Formulation and Practices",
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2512.01374",
    "upvote": 66,
    "tags": [
      "training",
      "level",
      "approximation"
    ]
  }
}