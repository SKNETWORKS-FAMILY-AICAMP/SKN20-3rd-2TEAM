{
  "context": "Generalist LLMs outperform specialized clinical AI assistants in a multi-task benchmark, highlighting the need for independent evaluation of clinical decision support tools. Specializedclinical AI assistantsare rapidly entering medical practice, often framed as safer or more reliable than general-purposelarge language models(LLMs). Yet, unlike frontier models, these clinical tools are rarely subjected to independent, quantitative evaluation, creating a critical evidence gap despite their growing influence on diagnosis, triage, and guideline interpretation. We assessed two widely deployed clinical AI systems (OpenEvidenceandUpToDate Expert AI) against three state-of-the-art generalistLLMs(GPT-5,Gemini 3 Pro, andClaude Sonnet 4.5) using a 1,000-item mini-benchmark combiningMedQA(medical knowledge) andHealthBench(clinician-alignment) tasks. Generalist models consistently outperformed clinical tools, withGPT-5achieving the highest scores, whileOpenEvidenceand UpToDate demonstrated deficits in completeness, communication quality, context awareness, andsystems-based safety reasoning. These findings reveal that tools marketed forclinical decision supportmay often lag behind frontierLLMs, underscoring the urgent need for transparent, independent evaluation before deployment in patient-facing workflows. Several high-profile generative AI tools have been introduced directly into clinical workflows, including Open Evidence and UpToDate Expert AI. Unlike frontier LLMs such as GPT-5, Gemini 3 Pro (Preview), and Claude Sonnet 4.5, which have undergone extensive benchmarking, these clinical-facing systems have seen almost no independent evaluation. OpenEvidence, reportedly used by 40% of U.S. physicians, claims “perfect” USMLE performance, yet these claims cannot be verified because the tool does not provide open weights or even an accessible API. Similarly, the newly released UpToDate Expert AI, already used in an estimated 70% of major enterprise health systems, has not been subjected to any form of public scrutiny or third-party testing. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend ·Sign uporlog into comment",
  "metadata": {
    "paper_name": "Generalist Large Language Models Outperform Clinical Tools on Medical Benchmarks",
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2512.01191",
    "upvote": 0,
    "tags": [
      "clinical",
      "ai",
      "tools"
    ]
  }
}