{
  "context": "CFG-Bench evaluates multimodal large language models on fine-grained action intelligence and higher-order reasoning in embodied agent tasks, revealing limitations and potential for improvement. Multimodal Large Language Models (MLLMs)show promising results as decision-making engines forembodied agentsoperating in complex, physical environments. However, existing benchmarks often prioritize high-level planning or spatial reasoning, leaving the fine-grained action intelligence required for embodiedphysical interactionunderexplored. To address this gap, we introduceCFG-Bench, a new benchmark designed to systematically evaluate this crucial capability.CFG-Benchconsists of 1,368 curated videos paired with 19,562three-modalitiesquestion-answer pairs targeting four cognitive abilities: 1)Physical Interaction, 2)Temporal-Causal Relation, 3)Intentional Understanding, and 4)Evaluative Judgment. Together, these dimensions provide a systematic framework for assessing a model's ability to translate visual observations into actionable knowledge, moving beyond mere surface-level recognition. Our comprehensive evaluation onCFG-Benchreveals that leading MLLMs struggle to produce detailed instructions forphysical interactions and exhibit profound limitations in the higher-order reasoning of intention and evaluation. Moreover,supervised fine-tuning (SFT)on our data demonstrates that teaching an MLLMs to articulate fine-grained actions directly translates to significant performance gains on establishedembodied benchmarks. Our analysis highlights these limitations and offers insights for developing more capable and groundedembodied agents. Multimodal Large Language Models (MLLMs) show promising results as decision-making engines for embodied agents operating in complex, physical environments. However, existing benchmarks often prioritize high-level planning or spatial reasoning, leaving the fine-grained action intelligence required for embodied physical interaction underexplored. To address this gap, we introduce CFG-Bench, a new benchmark designed to systematically evaluate this crucial capability. CFG-Bench consists of 1,368 curated videos paired with 19,562 three-modalities question-answer pairs targeting four cognitive abilities: 1) Physical Interaction, 2) Temporal-Causal Relation, 3) Intentional Understanding, and 4) Evaluative Judgment. Together, these dimensions provide a systematic framework for assessing a model's ability to translate visual observations into actionable knowledge, moving beyond mere surface-level recognition. Our comprehensive evaluation on CFG-Bench reveals that leading MLLMs struggle to produce detailed instructions for physical interactions and exhibit profound limitations in the higher-order reasoning of intention and evaluation. Moreover, supervised fine-tuning (SFT) on our data demonstrates that teaching an MLLMs to articulate fine-grained actions directly translates to significant performance gains on established embodied benchmarks. Our analysis highlights these limitations and offers insights for developing more capable and grounded embodied agents. Project page: \\href{https://cfg-bench.github.io/}{https://cfg-bench.github.io/}. This is an automated message from theLibrarian Bot. I found the following papers similar to this paper. The following papers were recommended by the Semantic Scholar API Please give a thumbs up to this comment if you found it helpful! If you want recommendations for any Paper on Hugging Face checkoutthisSpace You can directly ask Librarian Bot for paper recommendations by tagging it in a comment:@librarian-botrecommend Â·Sign uporlog into comment",
  "metadata": {
    "paper_name": "Beyond Description: Cognitively Benchmarking Fine-Grained Action for Embodied Agents",
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2511.18685",
    "upvote": 3,
    "tag1": "bench",
    "tag2": "cfg",
    "tag3": "fine"
  }
}