{
  "context": "TwinFlow is a 1-step generative model framework that enhances inference efficiency without requiring fixed pretrained teacher models or standard adversarial networks, achieving high performance on text-to-image tasks and scaling efficiently. Recent advances in large multi-modal generative models have demonstrated impressive capabilities in multi-modal generation, including image and video generation. These models are typically built upon multi-step frameworks likediffusionandflow matching, which inherently limits theirinference efficiency(requiring 40-100Number of Function Evaluations(NFEs)). While variousfew-step methodsaim to accelerate the inference, existing solutions have clear limitations. Prominent distillation-based methods, such as progressive andconsistency distillation, either require an iterative distillation procedure or show significant degradation at very few steps (< 4-NFE). Meanwhile, integratingadversarial traininginto distillation (e.g.,DMD/DMD2andSANA-Sprint) to enhance performance introduces training instability, added complexity, and high GPU memory overhead due to the auxiliary trained models. To this end, we proposeTwinFlow, a simple yet effective framework for training 1-step generative models that bypasses the need of fixed pretrained teacher models and avoids standard adversarial networks during training, making it ideal for building large-scale, efficient models. On text-to-image tasks, our method achieves aGenEvalscore of 0.83 in 1-NFE, outperforming strong baselines likeSANA-Sprint(a GAN loss-based framework) andRCGM(a consistency-based framework). Notably, we demonstrate the scalability ofTwinFlowby full-parameter training onQwen-Image-20Band transform it into an efficient few-step generator. With just 1-NFE, our approach matches the performance of the original 100-NFE model on both theGenEvalandDPG-Benchbenchmarks, reducing computational cost by 100times with minor quality degradation. Project page is available at https://zhenglin-cheng.com/twinflow. Taming 20B full-parameter few-step training with self-adversarial flows! ðŸ‘ðŸ» Checkout our2-NFE imagesgenerated by ourTwinFlow-Qwen-Image!ðŸ‘‡  We are also working onZ-Image-Turbo, stay tuned! very nice paper! ðŸŽ‰ðŸ‘ Hope it there will be one for **OnomaAIResearch/Illustrious-xl-early-release-v0 ** gonna save us from 24/29 sampling steps for every GEN ðŸ‘€ Â·Sign uporlog into comment",
  "metadata": {
    "title": "TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows",
    "authors": [],
    "publication_year": 2025,
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2512.05150",
    "upvote": 40,
    "tags": []
  }
}