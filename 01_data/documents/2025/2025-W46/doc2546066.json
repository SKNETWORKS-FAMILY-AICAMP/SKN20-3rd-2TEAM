{
  "context": "VLA models, combining vision, language, and action, are advancing through milestones like multimodality, reasoning, and safety, with trends focusing on spatial understanding and human coordination.\nAI-generated summary\nDue to their ability of follow natural language instructions, vision-language-action (VLA) models are increasingly prevalent in the embodied AI arena, following the widespread success of their precursors -- LLMs and VLMs. In this paper, we discuss 10 principal milestones in the ongoing development of VLA models -- multimodality, reasoning, data, evaluation, cross-robot action generalization, efficiency, whole-body coordination, safety, agents, and coordination with humans. Furthermore, we discuss the emerging trends of using spatial understanding, modeling world dynamics, post training, and data synthesis -- all aiming to reach these milestones. Through these discussions, we hope to bring attention to the research avenues that may accelerate the development of VLA models into wider acceptability.",
  "metadata": {
    "paper_name": "10 Open Challenges Steering the Future of Vision-Language-Action Models",
    "github_url": "",
    "huggingface_url": "https://huggingface.co/papers/2511.05936",
    "upvote": 5,
    "tags": [
      "vla",
      "models",
      "language"
    ]
  }
}