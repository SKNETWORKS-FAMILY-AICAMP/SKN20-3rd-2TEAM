{
    "content": "This survey reviews Efficient Vision-Language-Action models, addressing computational and data challenges through model design, training, and data collection techniques.\nVision-Language-Action models (VLAs) represent a significant frontier in embodied intelligence, aiming to bridge digital knowledge with physical-world interaction. While these models have demonstrated remarkable generalist capabilities, their deployment is severely hampered by the substantial computational and data requirements inherent to their underlying large-scale foundation models. Motivated by the urgent need to address these challenges, this survey presents the first comprehensive review of Efficient Vision-Language-Action models (Efficient VLAs) across the entire data-model-training process. Specifically, we introduce a unified taxonomy to systematically organize the disparate efforts in this domain, categorizing current techniques into three core pillars: (1) Efficient Model Design, focusing on efficient architectures and model compression; (2) Efficient Training, which reduces computational burdens during model learning; and (3) Efficient Data Collection, which addresses the bottlenecks in acquiring and utilizing robotic data. Through a critical review of state-of-the-art methods within this framework, this survey not only establishes a foundational reference for the community but also summarizes representative applications, delineates key challenges, and charts a roadmap for future research. We maintain a continuously updated project page to track our latest developments: https://evla-survey.github.io/",
    "metadata": {
        "paper_name": "A Survey on Efficient Vision-Language-Action Models",
        "github_url": "https://github.com/YuZhaoshu/Efficient-VLAs-Survey",
        "huggingface_url": "https://huggingface.co/papers/2510.24795",
        "upvote": 5,
        "tags": [
            "efficient",
            "data",
            "models"
        ]
    }
}