import os
import warnings
import sys
from dotenv import load_dotenv

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from pathlib import Path
from duckduckgo_search import DDGS  # pip install duckduckgo-search

from langsmith import Client, wrappers, evaluate
from openai import OpenAI

# ê²½ê³ ë©”ì„¸ì§€ ì‚­ì œ
warnings.filterwarnings('ignore')
load_dotenv()

# openapi key í™•ì¸
API_KEY = os.getenv('OPENAI_API_KEY')
if not API_KEY:
    raise ValueError('.enví™•ì¸, keyì—†ìŒ')

# vectordb ëª¨ë“ˆ import
SRC_DIR = Path(__file__).parent.parent.parent
sys.path.insert(0, str(SRC_DIR / "02_utils"))
from vectordb import load_vectordb

client = Client()

dataset = client.create_dataset(
    dataset_name="ds-pertinent-fiesta-37", description="A sample dataset in LangSmith."
)
examples = [
    {
        "inputs": {"question": "ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration"},
        "outputs": {"answer": """title : ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration
huggingface_url : https://huggingface.co/papers/2511.21689
git_url : https://github.com/NVlabs/ToolOrchestra/
upvote:99
authors : Hongjin Su, Shizhe Diao, Ximing Lu"""},
    },
    {
        "inputs": {"question": "RFTë¥¼ LVLMs (large video language models) ìœ¼ë¡œ í™•ì¥í•˜ëŠ” ë°©ë²•ì€ ë¬´ì—‡ì´ ìˆë‚˜ìš”?"},
        "outputs": {"answer": """title: VIDEOP2R: Video Understanding from Perception to Reasoning
huggingface_url:https://huggingface.co/papers/2511.11113
git_url: ì—†ìŒ
authors : Yifan Jiang, Yueying Wang, Rui Zhao, Toufiq Parag
upvote:111"""},
    },
    {
        "inputs": {"question": "LLMì—ì„œ ê¸´ ë¬¸ë§¥ì˜ ì¶”ë¡ ì„ í–¥ìƒì‹œí‚¤ëŠ” GSW (Generative Semantic Workspace)ì— ëŒ€í•œ ë…¼ë¬¸ì´ ìˆë‹¤ë©´ ì†Œê°œì‹œì¼œì£¼ì„¸ìš”"},
        "outputs": {"answer": """title: Beyond Fact Retrieval: Episodic Memory for RAG with Generative Semantic Workspaces
hugginfFace_url: https://huggingface.co/papers/2511.07587
git_url: ì—†ìŒ
Authors: Shreyas Rajesh, Pavan Holur, Chenda Duan, David Chong
upvote:8"""}
    },
    {
        "inputs": {"question": "GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents"},
        "outputs": {"answer": """title: GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents
huggingface_url: https://huggingface.co/papers/2511.04307
git_url: ì—†ìŒ
authors: Jian Mu, Chaoyun Zhang, Chiming Ni, Lu Wang
upvote:14"""}
    },
    {
        "inputs": {"question": "ì˜¤ë””ì˜¤ ê¸°ë°˜ ì• ë‹ˆë©”ì´ì…˜ì˜ ì •ì²´ì„±ì„ ìœ ì§€í•˜ëŠ” ë°©ë²•ì´ ìˆë‚˜ìš”?"},
        "outputs": {"answer": """title: https://huggingface.co/papers/2510.23581
huggingface_url : https://huggingface.co/papers/2510.23581
git_url: ì—†ìŒ
authors : Junyoung Seo, Rodrigo Mira, Alexandros Haliassos
upvote:41"""}
    },
    {
        "inputs": {"question": "core attention disaggregation ì€ ë¬´ì—‡ì¸ê°€ìš”?"},
        "outputs": {"answer": """title : Efficient Long-context Language Model Training by Core Attention Disaggregation
huggingface_url: https://huggingface.co/papers/2510.18121
git_url: ì—†ìŒ
authors:Yonghao Zhuang, Junda Chen, Bo Pang, Yi Gu
upvote:121"""}
    },
    {
        "inputs": {"question": "LLMì—ì„œ í™˜ê°íƒì§€ë¥¼ í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ì…‹ì„ ì•Œë ¤ì£¼ì„¸ìš”"},
        "outputs": {"answer": """title: When Models Lie, We Learn: Multilingual Span-Level Hallucination Detection with PsiloQA
huggingface_url : https://huggingface.co/papers/2510.04849
git_url : https://github.com/s-nlp/PsiloQA
authors : Elisei Rykov, Kseniia Petrushina, Maksim Savkin"""}
    },
    {
        "inputs": {"question": "LLMì—ì„œ ìºì‹œì™€ ê´€ë ¨ëœ ë…¼ë¬¸ì´ ìˆë‚˜ìš”?"},
        "outputs": {"answer": """title: Cache-to-Cache: Direct Semantic Communication Between Large Language Models
huggingface_url: https://huggingface.co/papers/2510.03215
git_url: https://github.com/thu-nics/C2C
authors: Tianyu Fu, Zihan Min, Hanling Zhang
upvote: 97"""}
    },
    {
        "inputs": {"question": "Adrian Kosowski ì €ìì˜ ìµœê·¼ ë…¼ë¬¸ì´ ìˆë‚˜ìš”?"},
        "outputs": {"answer": """title: The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain
huggingface_url:https://huggingface.co/papers/2509.26507
git_url: https://github.com/pathwaycom/bdh
authors: Adrian Kosowski, PrzemysÅ‚aw UznaÅ„ski, Jan Chorowski
upvote:535"""}
    },
    {
        "inputs": {"question": "í•´ë¦¬í¬í„° ì¤„ê±°ë¦¬ ì•Œë ¤ì£¼ì„¸ìš”"},
        "outputs": {"answer": """í•´ë‹¹ì—†ë‹¤"""}
    },
    {
        "inputs": {"question": "ìµœê·¼ ê³µê°œëœ ë…¼ë¬¸ì—ì„œ ì¢‹ì•„ìš”ìˆ˜ë¥¼ 300ê°œ ì´ìƒ ë°›ì€ ë…¼ë¬¸ì€ ëª‡ê°œì¸ê°€ìš”? (5ê°œ ì´í•˜ì¸ê²½ìš°, 5ê°œ ë…¼ë¬¸ì— ëŒ€í•´ì„œ ì†Œê°œí•´ì£¼ì„¸ìš”)"},
        "outputs": {"answer": ""}
    }

]
client.create_examples(dataset_id=dataset.id, examples=examples)

# Wrap the OpenAI client for LangSmith tracing
openai_client = wrappers.wrap_openai(OpenAI())


# Define the application logic to evaluate.
# Dataset inputs are automatically sent to this target function.
def target(inputs: dict) -> dict:
    response = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "Answer the following question accurately"},
            {"role": "user", "content": inputs["question"]},
        ],
    )
    return {"answer": response.choices[0].message.content}

# Define an LLM-as-a-judge evaluator to evaluate correctness of the output
def correctness_evaluator(inputs: dict, outputs: dict, reference_outputs: dict):
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì˜ ì •í™•ì„±ì„ í‰ê°€í•˜ëŠ” í•¨ìˆ˜
    """
    from langchain_openai import ChatOpenAI

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    eval_prompt = f"""You are an expert evaluator. Compare the predicted answer with the reference answer.

Question: {inputs.get('question', '')}
Predicted Answer: {outputs.get('answer', '')}
Reference Answer: {reference_outputs.get('answer', '')}

Evaluate if the predicted answer is correct and relevant. Provide a score from 0 to 1 where:
- 1.0 means perfect match or fully correct
- 0.5 means partially correct
- 0.0 means completely incorrect

Respond with ONLY a JSON object: {{"score": <float>, "reasoning": "<explanation>"}}
"""

    try:
        response = llm.invoke(eval_prompt)
        import json
        result = json.loads(response.content)
        return {
            "key": "correctness",
            "score": result.get("score", 0.0),
            "comment": result.get("reasoning", "")
        }
    except Exception as e:
        print(f"í‰ê°€ ì˜¤ë¥˜: {e}")
        return {
            "key": "correctness",
            "score": 0.0,
            "comment": f"í‰ê°€ ì‹¤íŒ¨: {str(e)}"
        }

class SimpleRAGSystem:
    '''ê°„ë‹¨í•œ RAG ì‹œìŠ¤í…œ ë˜í¼ í´ë˜ìŠ¤'''
    def __init__(self, vectorstore, llm, retriever_k=3):
        self.vectorstore = vectorstore
        self.llm = llm
        self.retriever_k = retriever_k
        self.retriever = vectorstore.as_retriever(
            search_type='similarity',
            search_kwargs={'k': retriever_k}
        )
        self.chain = self._build_chain()
        self.chat_history = []
    
    def _build_chain(self):
        '''RAG ì²´ì¸ êµ¬ì„±''' 
        prompt = ChatPromptTemplate.from_messages([
            ("system", """
You are **"AI Tech Trend Navigator"**, an expert assistant for AI/ML research papers.

[Role]
- You help users understand and leverage recent AI/ML papers collected from HuggingFace Weekly Papers.
- Your goals:
  - Summarize papers clearly.
  - Explain core ideas simply.
  - Highlight practical use-cases for real products/services.

[Inputs Provided]
The system supplies:
- user_question
- chat_history
- context:
    - If there are relevant papers:
        â†’ a concatenation of [Paper i] blocks.
    - If there are NO relevant papers:
        â†’ a string that begins with the line EXACTLY:
        NO_RELEVANT_PAPERS
        followed by one or more [WebResult i] blocks from DuckDuckGo.

**IMPORTANT MODE SWITCH**
If the FIRST LINE of context is EXACTLY "NO_RELEVANT_PAPERS":
    â†’ You MUST answer using ONLY general AI/ML knowledge + DuckDuckGo results.
    â†’ You MUST output using strictly the <web search using> format.
    â†’ You MUST NOT output:
        - "Sources summary"
        - Any paper list
        - Any paper title
        - ANY reference to HuggingFace papers or metadata

If context contains papers:
    â†’ You MUST ignore DuckDuckGo rules COMPLETELY.
    â†’ You MUST answer ONLY based on:
         (1) given context papers
         (2) general high-level knowledge (no invented details)
    â†’ You MUST output using the "paper mode" format:
         1) One-line summary
         2) Key insights
         3) Related papers
         4) Detailed explanation
         5) Sources summary (based strictly on metadata)

[PAPER MODE â€” detailed behavior]
- Use only relevant papers (1-3 typically).
- DO NOT hallucinate titles, authors, datasets, years, URLs, metrics, or numbers.
- If metadata items are missing, write "No information".
- If papers do not directly answer the user's question, explicitly say so.

Paper Output Format:
1) One-line summary
2) Key insights (3~6 bullets)
3) Matched papers (only title from md.get("title"))
4) Detailed explanation
5) Sources summary
   For each used paper:
     Â· title:
     Â· authors:
     Â· huggingface_url:
     Â· github_url:
     Â· upvote:

[WEB SEARCH MODE â€” detailed behavior]
Triggered ONLY when the FIRST LINE of context is EXACTLY "NO_RELEVANT_PAPERS".

Below that line, you will see one or more blocks like:
    [WebResult 1]
    title: ...
    url: ...
    snippet: ...
Use them as your ONLY external information.

Output MUST follow this EXACT structure:
1) One-line summary
2) Detailed explanation
3) source : DuckDuckgo
   - First URL
   - Second URL

URL RULES (VERY IMPORTANT):
- ONLY two URL use
- The URLs written under "source : DuckDuckgo" MUST be copied **exactly** from the `url:` fields.
- DO NOT invent or fabricate URLs.
- If only one valid URL exists, output only one URL line.
- If NO valid URLs exist, output:
    3) source : DuckDuckgo
       - (no URL available)

You MUST NOT:
- Output "Sources summary" in this mode
- Mention HuggingFace papers
- Output any paper titles

[Style]
- ALWAYS respond in Korean.
- Keep explanations clear and non-academic.
- Briefly explain technical terms when helpful.
"""),
            ("human", """
[QUESTION]
{question}

[CHAT HISTORY]
{chat_history}

[Context]
======= START =======
{context}
======= END =======

Follow the output rules based on whether papers exist.
""")
        ])
        return (
            prompt
            | self.llm
            | StrOutputParser()
        )
    
    def _web_search(self, query: str, num_results: int = 5):
        """DuckDuckGoë¡œ ê²€ìƒ‰ (duckduckgo-search ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©)"""
        try:
            ddgs = DDGS()
            results = []
            
            # text() ë©”ì„œë“œë¡œ ê²€ìƒ‰ ìˆ˜í–‰
            search_results = ddgs.text(query, max_results=num_results)
            
            for item in search_results:
                results.append({
                    "title": item.get("title", ""),
                    "url": item.get("href", ""),
                    "snippet": item.get("body", "")
                })
            
            return results
        except Exception as e:
            print(f"ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def _format_web_results(self, results):
        """ê²€ìƒ‰ ê²°ê³¼ â†’ LLM í”„ë¡¬í”„íŠ¸ìš© í…ìŠ¤íŠ¸"""
        if not results:
            return "NO_WEB_RESULTS"

        blocks = []
        for i, r in enumerate(results, 1):
            blocks.append(f"""
[WebResult {i}]
title: {r['title']}
url: {r['url']}

snippet:
{r['snippet']}
""")
        return "\n\n".join(blocks)
    
    @staticmethod
    def _format_docs(docs):
        """retrieverê°€ ë°˜í™˜í•œ Documentë“¤ì„ í”„ë¡¬í”„íŠ¸ìš© í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        if not docs:
            return "NO_RELEVANT_PAPERS"

        lines = []
        for i, doc in enumerate(docs, start=1):
            md = doc.metadata or {}

            title = md.get("title") or md.get("paper_name") or "No information"

            raw_authors = md.get("authors")
            if isinstance(raw_authors, list):
                authors = ", ".join(raw_authors)
            else:
                authors = raw_authors or "No information"

            title = md.get("title") or "No information"
            authors = md.get("authors") or "No information"
            huggingface_url = md.get("huggingface_url") or "No information"
            github_url = md.get("github_url") or "No information"
            upvote = md.get("upvote") or "No information"
            publication_year = md.get("publication_year") or "No information"
            doc_id = md.get("doc_id") or "No information"
            chunk_index = md.get("chunk_index") or "No information"

            block = f"""
[Paper {i}]
title: {title}
authors: {authors}
huggingface_url: {huggingface_url}
github_url: {github_url}
upvote: {upvote}
publication_year: {publication_year}
doc_id: {doc_id}
chunk_index: {chunk_index}

content:
{doc.page_content}
"""
            lines.append(block)

        return "\n\n".join(lines)

    def _format_chat_history(self):
        """ì €ì¥ëœ ëŒ€í™” ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ êµ¬ì„±"""
        if not self.chat_history:
            return "(no previous conversation)"

        history_lines = []
        for turn in self.chat_history:
            history_lines.append(f"User: {turn['user']}")
            history_lines.append(f"Assistant: {turn['assistant']}")
        return "\n".join(history_lines)

    def chat(self, user_message: str, score_threshold: float = 1.2) -> str:
        """
        ëŒ€í™” ëª¨ë“œ: íˆìŠ¤í† ë¦¬ ì €ì¥ + RAG ë‹µë³€
        
        Args:
            user_message: ì‚¬ìš©ì ì§ˆë¬¸
            score_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’ (ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬í•¨, ê¸°ë³¸ê°’ 1.0)
        """
        # 1. similarity_search_with_score ìˆ˜í–‰
        docs_and_scores = self.vectorstore.similarity_search_with_score(
            user_message, 
            k=self.retriever_k
        )
        
        # ë””ë²„ê¹…ìš© ì¶œë ¥
        print(f"\n[DEBUG] ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(docs_and_scores)}")
        for i, (doc, score) in enumerate(docs_and_scores):
            title = doc.metadata.get("title", "ì œëª©ì—†ìŒ")
            print(f"  ë¬¸ì„œ {i+1}: score={score:.4f}, title={title}")

        # 2. score ê¸°ì¤€ í•„í„°ë§ (ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬ â†’ <= ì‚¬ìš©)
        relevant_docs = [doc for doc, score in docs_and_scores if score <= score_threshold]
        
        print(f"[DEBUG] ì„ê³„ê°’ {score_threshold} ì´í•˜ ë¬¸ì„œ: {len(relevant_docs)}ê°œ\n")

        # 3. context ê²°ì •
        if not relevant_docs:
            # ì›¹ ê²€ìƒ‰ ëª¨ë“œ
            print("[INFO] ê´€ë ¨ ë…¼ë¬¸ ì—†ìŒ â†’ ì›¹ ê²€ìƒ‰ ëª¨ë“œ ì‹¤í–‰")
            web_results = self._web_search(user_message)
            web_block = self._format_web_results(web_results)
            context = "NO_RELEVANT_PAPERS\n\n" + web_block
        else:
            # ë…¼ë¬¸ ëª¨ë“œ
            print(f"[INFO] ë…¼ë¬¸ ëª¨ë“œ ì‹¤í–‰ (ê´€ë ¨ ë…¼ë¬¸ {len(relevant_docs)}ê°œ)")
            context = self._format_docs(relevant_docs)

        # 4. chain ì‹¤í–‰
        response = self.chain.invoke({
            "question": user_message,
            "context": context,
            "chat_history": self._format_chat_history()
        })

        # 5. íˆìŠ¤í† ë¦¬ ì €ì¥
        self.chat_history.append({
            "user": user_message,
            "assistant": response
        })

        return response

    def ask(self, question: str, score_threshold: float = 1.2) -> str:
        """
        ì§ˆë¬¸ì— ë‹µë³€ (ë‹¨ë°œì„±)
        
        Args:
            question: ì§ˆë¬¸ ë‚´ìš©
            score_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
        """
        # 1. ë²¡í„°DB ê²€ìƒ‰
        docs_and_scores = self.vectorstore.similarity_search_with_score(
            question,
            k=self.retriever_k
        )
        
        # 2. score í•„í„°ë§
        relevant_docs = [doc for doc, score in docs_and_scores if score <= score_threshold]

        # 3. context ê²°ì •
        if not relevant_docs:
            web_results = self._web_search(question)
            web_block = self._format_web_results(web_results)
            context = "NO_RELEVANT_PAPERS\n\n" + web_block
        else:
            context = self._format_docs(relevant_docs)

        # 4. chain ì‹¤í–‰
        return self.chain.invoke({
            "question": question,
            "context": context,
            "chat_history": self._format_chat_history()
        })

    def ask_with_sources(self, question: str, stream: bool = False, score_threshold: float = 1.2):
        """ì§ˆë¬¸ì— ë‹µë³€ + ì¶œì²˜ ë°˜í™˜"""
        # 1. similarity_search_with_score ì‚¬ìš©
        docs_and_scores = self.vectorstore.similarity_search_with_score(
            question,
            k=self.retriever_k
        )
        
        # 2. score ê¸°ì¤€ í•„í„°ë§ (ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬)
        relevant_docs = [doc for doc, score in docs_and_scores if score <= score_threshold]

        # 3. context ê²°ì •
        if not relevant_docs:
            # ì›¹ ê²€ìƒ‰ ëª¨ë“œ
            web_results = self._web_search(question)
            web_block = self._format_web_results(web_results)
            context = "NO_RELEVANT_PAPERS\n\n" + web_block

            sources = [{
                "paper_name": r["title"],
                "huggingface_url": None,
                "github_url": None,
                "upvote": None,
                "url": r["url"]
            } for r in web_results]
        else:
            # ë…¼ë¬¸ ëª¨ë“œ
            context = self._format_docs(relevant_docs)
            sources = []
            for doc in relevant_docs:
                md = doc.metadata or {}
                title = md.get("title") or md.get("paper_name") or "(no title)"

                raw_authors = md.get("authors")
                if isinstance(raw_authors, list):
                    authors = ", ".join(raw_authors)
                else:
                    authors = raw_authors or "No information"

                sources.append({
                    "paper_name": title,
                    "authors": authors,
                    "huggingface_url": md.get("huggingface_url"),
                    "github_url": md.get("github_url"),
                    "upvote": md.get("upvote"),
                })

        # 4. chain ì‹¤í–‰
        chain_input = {
            "question": question,
            "context": context,
            "chat_history": self._format_chat_history()
        }

        if stream:
            return {
                "answer_stream": self.chain.stream(chain_input),
                "sources": sources,
            }
        else:
            answer = self.chain.invoke(chain_input)
            return {
                "answer": answer,
                "sources": sources,
            }

    def clear_history(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.chat_history = []


# if __name__ == '__main__':
#     # chunk íŒŒì¼ë¡œ ì„ì‹œ í™•ì¸
#     def get_project_root():
#         curr = Path().resolve()
#         for parent in [curr] + list(curr.parents):
#             if (parent / ".git").exists():
#                 return parent
#         raise FileNotFoundError("í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸° ì‹¤íŒ¨")

#     MODEL_NAME = os.getenv("MODEL_NAME")
#     CHUNK_SIZE = int(os.getenv("CHUNK_SIZE"))
#     CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP"))
    
#     vectorstore = load_vectordb(
#             model_name=MODEL_NAME,
#             chunk_size=CHUNK_SIZE,
#             chunk_overlap=CHUNK_OVERLAP
#     )
  
#     llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)

#     rag_system = SimpleRAGSystem(vectorstore, llm)
#     user_question = "í•´ë¦¬í¬í„° ì¤„ê±°ë¦¬ ì•Œë ¤ì¤˜"
#     result = rag_system.ask_with_sources(user_question)

#     print(f"ì§ˆë¬¸: {user_question}")
#     print("\n[ë‹µë³€]\n")
#     print(result["answer"])

#     # --------------------------------------------------------
#     # ğŸ”¥ ì—¬ê¸° ì•„ë˜ ì±—ë´‡ ëª¨ë“œ ì…ë ¥ ë£¨í”„ ë„£ìœ¼ë©´ ë¨!
#     # --------------------------------------------------------

#     print("\n=== AI Tech Trend Navigator Chatbot ===")
#     print("ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ë˜ëŠ” 'quit' ì…ë ¥\n")

#     while True:
#         user_msg = input("You: ")

#         if user_msg.lower() in ["exit", "quit"]:
#             print("ì±—ë´‡ ì¢…ë£Œ!")
#             break

#         answer = rag_system.chat(user_msg)
#         print(f"\nAssistant:\n{answer}\n")

if __name__ == '__main__':
    MODEL_NAME = os.getenv("MODEL_NAME")
    CHUNK_SIZE = int(os.getenv("CHUNK_SIZE"))
    CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP"))

    print("VectorDB ë¡œë”© ì¤‘...")
    vectorstore = load_vectordb(
        model_name=MODEL_NAME,
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP
    )

    llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)
    rag_system = SimpleRAGSystem(vectorstore, llm, retriever_k=5)

    print("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ\n")

    # target í•¨ìˆ˜ë¥¼ RAG ì‹œìŠ¤í…œì„ ì‚¬ìš©í•˜ë„ë¡ ì¬ì •ì˜
    def rag_target(inputs: dict) -> dict:
        """RAG ì‹œìŠ¤í…œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” target í•¨ìˆ˜"""
        question = inputs["question"]
        result = rag_system.ask(question, score_threshold=1.2)
        return {"answer": result}

    print("LangSmith í‰ê°€ ì‹¤í–‰ ì¤‘...")
    try:
        experiment_results = evaluate(
            rag_target,  # RAG ì‹œìŠ¤í…œì„ ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜
            data="ds-pertinent-fiesta-37",
            evaluators=[correctness_evaluator],
            experiment_prefix="experiment-rag-system",
            max_concurrency=2,
        )
        print("í‰ê°€ ì™„ë£Œ!")
        print(f"ì‹¤í—˜ ê²°ê³¼: {experiment_results}")
    except Exception as e:
        print(f"í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("í‰ê°€ë¥¼ ê±´ë„ˆë›°ê³  ì±—ë´‡ ëª¨ë“œë¡œ ì§„í–‰í•©ë‹ˆë‹¤.\n")

    # í…ŒìŠ¤íŠ¸
    print("\n=== í…ŒìŠ¤íŠ¸ 1: ë…¼ë¬¸ ê²€ìƒ‰ ===")
    result = rag_system.ask_with_sources("transformer architecture", score_threshold=1.2)
    print(f"\n[ë‹µë³€]\n{result['answer']}\n")
    print(f"[ì¶œì²˜ ìˆ˜]: {len(result['sources'])}")

    print("\n=== AI Tech Trend Navigator Chatbot ===")
    print("ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ë˜ëŠ” 'quit' ì…ë ¥\n")

    while True:
        user_msg = input("You: ")
        if user_msg.lower() in ["exit", "quit"]:
            print("ì±—ë´‡ ì¢…ë£Œ!")
            break

        # score_threshold ì¡°ì • ê°€ëŠ¥ (ê¸°ë³¸ê°’ 1.2)
        answer = rag_system.chat(user_msg, score_threshold=1.2)
        print(f"\nAssistant:\n{answer}\n")