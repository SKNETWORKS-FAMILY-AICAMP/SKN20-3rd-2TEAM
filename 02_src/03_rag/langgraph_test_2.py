"""
LangGraph ê¸°ë°˜ RAG ì‹œìŠ¤í…œ

ì´ ëª¨ë“ˆì€ LangGraphë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì€ ë¼ìš°íŒ… ë¡œì§ì„ êµ¬í˜„í•©ë‹ˆë‹¤:
- retrieve â†’ evaluate_document_relevance â†’ {HIGH, MEDIUM, LOW}
- HIGH â†’ generate_final_answer
- MEDIUM â†’ cluster_similarity_check â†’ {HIGH, LOW}
  - HIGH â†’ generate_final_answer
  - LOW â†’ web_search â†’ generate_final_answer
- LOW â†’ reject

VectorStoreëŠ” ì‹œì‘ ì‹œ ë¡œë“œë˜ì–´ ì¬ì‚¬ìš©ë©ë‹ˆë‹¤.
"""

# ===== SECTION 1: IMPORTS =====
import os
import sys
import json
import re
import warnings
import hashlib
from pathlib import Path
from typing import List, Dict, Any, Optional, Literal
from typing_extensions import TypedDict
from dotenv import load_dotenv

# LangChain
from langchain_core.documents import Document
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# LangChain Community (BM25)
from langchain_community.retrievers import BM25Retriever

# LangGraph
from langgraph.graph import StateGraph, START, END

# ===== SECTION 2: ENVIRONMENT & PATHS =====
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT / "02_src" / "02_utils"))

warnings.filterwarnings("ignore")
load_dotenv()

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
MODEL_NAME = os.getenv("MODEL_NAME", "MiniLM-L6")
CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", 300))
CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", 45))

# ===== SECTION 3: GRAPHSTATE =====
class GraphState(TypedDict):
    """
    LangGraph ìƒíƒœ ê´€ë¦¬ë¥¼ ìœ„í•œ TypedDict

    Attributes:
        question: ê²€ìƒ‰ì— ì‚¬ìš©ë  ì§ˆë¬¸ (ì˜ì–´ ë²ˆì—­ëœ ì§ˆë¬¸ ë˜ëŠ” ì›ë³¸)
        original_question: ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì›ë³¸ ì§ˆë¬¸
        translated_question: ì˜ì–´ë¡œ ë²ˆì—­ëœ ì§ˆë¬¸ (í•œê¸€ì¸ ê²½ìš°)
        is_korean: ì›ë³¸ ì§ˆë¬¸ì´ í•œê¸€ì¸ì§€ ì—¬ë¶€
        documents: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        doc_scores: ê° ë¬¸ì„œì˜ ìœ ì‚¬ë„ ì ìˆ˜ (L2 distance)
        cluster_id: ê²€ìƒ‰ëœ ë¬¸ì„œì˜ í´ëŸ¬ìŠ¤í„° ID
        cluster_similarity_score: í´ëŸ¬ìŠ¤í„° ë‚´ í‰ê·  ìœ ì‚¬ë„ ì ìˆ˜
        search_type: ê²€ìƒ‰ ìœ í˜• ("internal", "cluster", "web", "rejected")
        relevance_level: ë¬¸ì„œ ê´€ë ¨ì„± ìˆ˜ì¤€ ("high", "medium", "low")
        answer: LLMì´ ìƒì„±í•œ ìµœì¢… ë‹µë³€
        sources: ì°¸ì¡° ë¬¸ì„œ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        _vectorstore: VectorStore ê°ì²´ (ë‚´ë¶€ ì „ìš©)
        _llm: LLM ê°ì²´ (ë‚´ë¶€ ì „ìš©)
        _cluster_metadata_path: í´ëŸ¬ìŠ¤í„° ë©”íƒ€ë°ì´í„° ê²½ë¡œ (ë‚´ë¶€ ì „ìš©)
    """
    question: str   # ê²€ìƒ‰ì— ì‚¬ìš©ë  ì§ˆë¬¸ (ì˜ì–´ ë²ˆì—­ëœ ì§ˆë¬¸ ë˜ëŠ” ì›ë³¸)
    original_question: str  # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì›ë³¸ ì§ˆë¬¸
    translated_question: Optional[str]
    is_korean: bool
    documents: List[Document]
    doc_scores: List[float]
    cluster_id: Optional[int]
    cluster_similarity_score: Optional[float]
    search_type: str
    relevance_level: str
    answer: str
    sources: List[Dict[str, Any]]
    # Internal (injected at runtime)
    _vectorstore: Any
    _llm: Any
    _bm25_retriever: Any
    _cluster_metadata_path: str


# ===== SECTION 4: HELPER FUNCTIONS =====

def get_doc_hash_key(doc: Document) -> str:
    """
    ë¬¸ì„œì˜ ê³ ìœ  í•´ì‹œ í‚¤ ìƒì„± (RRF ì¤‘ë³µ ì œê±°ìš©)

    ë¬¸ì„œ ë‚´ìš©(ìµœëŒ€ 1000ì)ê³¼ ì¶œì²˜ë¥¼ ê²°í•©í•˜ì—¬ SHA256 í•´ì‹œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    ì´ë¥¼ í†µí•´ BM25ì™€ ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë™ì¼ ë¬¸ì„œë¥¼ ì‹ë³„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    Args:
        doc: Document ê°ì²´

    Returns:
        str: SHA256 í•´ì‹œ ë¬¸ìì—´
    """
    content = doc.page_content[:1000]
    source = doc.metadata.get('source', '')
    data_to_hash = f"{content}|{source}"
    return hashlib.sha256(data_to_hash.encode('utf-8')).hexdigest()


def is_korean_text(text: str) -> bool:
    """
    í…ìŠ¤íŠ¸ì— í•œê¸€ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸

    Args:
        text: í™•ì¸í•  í…ìŠ¤íŠ¸

    Returns:
        bool: í•œê¸€ í¬í•¨ ì—¬ë¶€
    """
    # í•œê¸€ ìœ ë‹ˆì½”ë“œ ë²”ìœ„: AC00-D7A3 (ì™„ì„±í˜•), 1100-11FF (ìëª¨)
    korean_pattern = re.compile(r'[ê°€-í£ã„±-ã…ã…-ã…£]')
    return bool(korean_pattern.search(text))

def is_ai_ml_related_by_llm(question: str, llm) -> bool:
    """
    LLMì„ ì‚¬ìš©í•´ì„œ ì§ˆë¬¸ì´ AI/ML ì—°êµ¬ ë…¼ë¬¸ ê´€ë ¨ì¸ì§€ íŒë³„í•œë‹¤.
    - True  : AI/ML ì—°êµ¬, ëª¨ë¸, í•™ìŠµ, ìµœì í™”, ì‘ìš© ì„œë¹„ìŠ¤ ë“±ê³¼ ê´€ë ¨
    - False : í•´ë¦¬í¬í„° ì¤„ê±°ë¦¬, ë‚ ì”¨, ì¼ìƒ, ì˜í™”/ì†Œì„¤ ê°ìƒ ë“± ì¼ë°˜ ì§ˆë¬¸

    LLMì€ ë°˜ë“œì‹œ 'YES' ë˜ëŠ” 'NO'ë§Œ ì¶œë ¥í•´ì•¼ í•œë‹¤.
    """
    if not question or llm is None:
        return False

    prompt = ChatPromptTemplate.from_messages([
(
    "system",
    """
You are a classifier that determines whether a user's question *could reasonably relate* to AI/ML/DL research topics.

Interpret broadly:  
If the question can *plausibly* be answered by AI/ML papers â€” even indirectly â€” return "YES".

Examples of YES:
- Topics about generation, animation, audio/visual models, identity preservation
- Conceptual questions that could relate to ML models
- Practical application questions (e.g., "How do I keep identity consistent when animating with audio?")
- General topics that ML papers often study (stability, alignment, optimization, synthesis)

Return "NO" **only** if the question has *no plausible connection* to ML research:
- Daily life, weather, food, personal advice
- Fictional stories, movies
- Purely philosophical or subjective opinions
- Generic coding help unrelated to ML models

Output must be exactly one word: YES or NO.
"""
),
        ("human", "{question}")
    ])

    chain = prompt | llm | StrOutputParser()
    try:
        result = chain.invoke({"question": question}).strip().upper()
        return result.startswith("Y")
    except Exception as e:
        print(f"[WARN] LLM topic classification failed: {e}")
        # íŒë³„ ì‹¤íŒ¨í•˜ë©´ ë³´ìˆ˜ì ìœ¼ë¡œ False ì·¨ê¸‰ (ë¹„ ê´€ë ¨ìœ¼ë¡œ ê°„ì£¼)
        return False


# ===== SECTION 5: NODE FUNCTIONS =====

def translate_node(state: GraphState) -> dict:
    """
    ë…¸ë“œ 0: í•œê¸€ ì§ˆë¬¸ ë²ˆì—­

    ì§ˆë¬¸ì´ í•œê¸€ì¸ ê²½ìš° ì˜ì–´ë¡œ ë²ˆì—­í•˜ì—¬ ê²€ìƒ‰ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
    ì˜ì–´ ë…¼ë¬¸ ê²€ìƒ‰ì— ìµœì í™”ë˜ë„ë¡ í•™ìˆ ì  í‘œí˜„ìœ¼ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤.

    Args:
        state: í˜„ì¬ GraphState

    Returns:
        dict: question, original_question, translated_question, is_koreanì„ í¬í•¨í•œ ìƒíƒœ ì—…ë°ì´íŠ¸
    """
    print("\n" + "="*60)
    print("[NODE: translate] ì§ˆë¬¸ ì–¸ì–´ í™•ì¸ ë° ë²ˆì—­")
    print("="*60)

    original_question = state["original_question"]
    llm = state.get("_llm")

    # í•œê¸€ í¬í•¨ ì—¬ë¶€ í™•ì¸
    has_korean = is_korean_text(original_question)

    if not has_korean:
        print(f"[translate] ì˜ì–´ ì§ˆë¬¸ ê°ì§€ - ë²ˆì—­ ìŠ¤í‚µ")
        print(f"[translate] ì›ë³¸: {original_question}")
        return {
            "question": original_question,
            "original_question": original_question,
            "translated_question": None,
            "is_korean": False
        }

    # í•œê¸€ ì§ˆë¬¸ ë²ˆì—­
    print(f"[translate] í•œê¸€ ì§ˆë¬¸ ê°ì§€ - ì˜ì–´ë¡œ ë²ˆì—­ ì¤‘...")
    print(f"[translate] ì›ë³¸: {original_question}")

    try:
        # ë²ˆì—­ í”„ë¡¬í”„íŠ¸
        translate_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a professional translator specializing in AI/ML/DL research topics.
Translate the Korean question into English with academic terminology.
Keep technical terms accurate and use standard AI/ML terminology.
Output ONLY the translated English text, nothing else."""),
            ("human", "{korean_text}")
        ])

        chain = translate_prompt | llm | StrOutputParser()
        translated = chain.invoke({"korean_text": original_question}).strip()

        print(f"[translate] ë²ˆì—­ ì™„ë£Œ: {translated}")

        return {
            "question": translated,  # ê²€ìƒ‰ì— ì‚¬ìš©ë  ì§ˆë¬¸
            "original_question": original_question,
            "translated_question": translated,
            "is_korean": True
        }

    except Exception as e:
        print(f"[ERROR] ë²ˆì—­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"[translate] ë²ˆì—­ ì‹¤íŒ¨ - ì›ë³¸ ì§ˆë¬¸ ì‚¬ìš©")
        return {
            "question": original_question,
            "original_question": original_question,
            "translated_question": None,
            "is_korean": True
        }


def retrieve_node(state: GraphState) -> dict:
    """
    ë…¸ë“œ 1: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (BM25 + Vector Search with RRF)

    BM25ê°€ ì—†ìœ¼ë©´ ë²¡í„° ê²€ìƒ‰ë§Œ ì‚¬ìš©í•˜ê³ ,
    ë‘˜ ë‹¤ ìˆìœ¼ë©´ RRFë¡œ í†µí•©í•œë‹¤.
    """
    print("\n" + "="*60)
    print("[NODE: retrieve] í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œì‘ (BM25 + Vector + RRF)")
    print("="*60)

    question = state["question"]
    vectorstore = state.get("_vectorstore")
    bm25_retriever = state.get("_bm25_retriever")

    # âœ… ìµœì†Œ ì¡°ê±´: VectorStoreëŠ” ë°˜ë“œì‹œ ìˆì–´ì•¼ í•¨
    if vectorstore is None:
        print("[ERROR] VectorStoreê°€ ë¡œë“œë˜ì§€ ì•ŠìŒ")
        return {
            "documents": [],
            "doc_scores": [],
            "cluster_id": None,
            "search_type": "vector"  # ì˜ë¯¸ìƒë§Œ í‘œì‹œ
        }

    use_bm25 = bm25_retriever is not None

    try:
        # 1. ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰
        vector_docs_with_scores = vectorstore.similarity_search_with_score(question, k=5)
        vector_docs = [doc for doc, score in vector_docs_with_scores]
        print(f"[retrieve] ë²¡í„° ê²€ìƒ‰: {len(vector_docs)}ê°œ ë¬¸ì„œ")

        # BM25ê°€ ì—†ìœ¼ë©´ ê·¸ëƒ¥ ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ë§Œ ì¨ë²„ë¦¬ê¸°
        if not use_bm25:
            print("[retrieve] BM25Retriever ì—†ìŒ â†’ ë²¡í„° ê²€ìƒ‰ë§Œ ì‚¬ìš©")
            documents = vector_docs
            scores = [score for doc, score in vector_docs_with_scores]

            cluster_id = None
            if documents:
                cluster_id = documents[0].metadata.get("cluster_id", -1)

            return {
                "documents": documents,
                "doc_scores": scores,
                "cluster_id": cluster_id,
                "search_type": "vector"
            }

        # 2. BM25 í‚¤ì›Œë“œ ê²€ìƒ‰ (BM25ê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš°ë§Œ)
        bm25_docs = bm25_retriever.invoke(question)
        print(f"[retrieve] BM25 ê²€ìƒ‰: {len(bm25_docs)}ê°œ ë¬¸ì„œ")

        # 3. RRF (Reciprocal Rank Fusion) ì ìˆ˜ ê³„ì‚°
        RRF_K = 60  # í‘œì¤€ RANK_BIAS ê°’
        fusion_scores = {}
        doc_map = {}

        # 3-1. ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬
        for rank, (doc, _score) in enumerate(vector_docs_with_scores):
            doc_key = get_doc_hash_key(doc)
            if doc_key not in doc_map:
                doc_map[doc_key] = doc
            score = 1 / (RRF_K + rank + 1)  # rankëŠ” 0ë¶€í„° ì‹œì‘í•˜ë¯€ë¡œ +1
            fusion_scores[doc_key] = fusion_scores.get(doc_key, 0.0) + score

        # 3-2. BM25 ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬
        for rank, doc in enumerate(bm25_docs):
            doc_key = get_doc_hash_key(doc)
            if doc_key not in doc_map:
                doc_map[doc_key] = doc
            score = 1 / (RRF_K + rank + 1)
            fusion_scores[doc_key] = fusion_scores.get(doc_key, 0.0) + score

        # 4. ì ìˆ˜ë¡œ ì •ë ¬ ë° ìƒìœ„ 5ê°œ ì¶”ì¶œ
        sorted_items = sorted(
            fusion_scores.items(), key=lambda x: x[1], reverse=True
        )

        documents = []
        scores = []
        for doc_key, score in sorted_items[:5]:
            documents.append(doc_map[doc_key])
            scores.append(score)

        if not documents:
            print("[retrieve] RRF í†µí•© ê²°ê³¼ ë¬¸ì„œ ì—†ìŒ")
            return {
                "documents": [],
                "doc_scores": [],
                "cluster_id": None,
                "search_type": "hybrid"
            }

        # ì²« ë²ˆì§¸ ë¬¸ì„œì˜ cluster_id ì¶”ì¶œ
        cluster_id = documents[0].metadata.get("cluster_id", -1)

        print(f"[retrieve] RRF í†µí•© ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ")
        print(f"[retrieve] ìµœìƒìœ„ RRF ì ìˆ˜: {scores[0]:.4f}")
        print(f"[retrieve] Cluster ID: {cluster_id}")

        return {
            "documents": documents,
            "doc_scores": scores,
            "cluster_id": cluster_id,
            "search_type": "hybrid"
        }

    except Exception as e:
        print(f"[ERROR] ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return {
            "documents": [],
            "doc_scores": [],
            "cluster_id": None,
            "search_type": "hybrid"
        }



def evaluate_document_relevance_node(state: GraphState) -> dict:
    """
    ë…¸ë“œ 2: ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€

    ê²€ìƒ‰ëœ ë¬¸ì„œì˜ RRF ìŠ¤ì½”ì–´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ì„±ì„ 3ë‹¨ê³„ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.
    RRF ìŠ¤ì½”ì–´ëŠ” ë†’ì„ìˆ˜ë¡ ê´€ë ¨ì„±ì´ ë†’ìŠµë‹ˆë‹¤.
    
    - HIGH (score >= 0.020): ë§¤ìš° ê´€ë ¨ì„±ì´ ë†’ìŒ
    - MEDIUM (0.015 <= score < 0.020): ë³´í†µ ê´€ë ¨ì„±
    - LOW (score < 0.015): ê´€ë ¨ì„± ë‚®ìŒ

    Args:
        state: í˜„ì¬ GraphState

    Returns:
        dict: relevance_levelì„ í¬í•¨í•œ ìƒíƒœ ì—…ë°ì´íŠ¸
    """
    print("\n" + "="*60)
    print("[NODE: evaluate] ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€")
    print("="*60)

    # 0) LLM ê¸°ë°˜ í† í”½ ê°€ë“œ: AI/ML ê´€ë ¨ì´ ì•„ë‹ˆë©´ ë°”ë¡œ LOWë¡œ ì²˜ë¦¬
    original_q = state.get("original_question", "")
    llm = state.get("_llm")

    try:
        if not is_ai_ml_related_by_llm(original_q, llm):
            print(f"[evaluate] LLM íŒë³„: ë¹„ AI/ML ì§ˆë¬¸ â†’ LOWë¡œ ì²˜ë¦¬: {original_q}")
            return {"relevance_level": "low"}
    except Exception as e:
        print(f"[WARN] í† í”½ íŒë³„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ, ìŠ¤ì½”ì–´ ê¸°ë°˜ í‰ê°€ë¡œ ì§„í–‰: {e}")

    # --- ì—¬ê¸°ì„œë¶€í„°ëŠ” ê¸°ì¡´ RRF ìŠ¤ì½”ì–´ ê¸°ë°˜ ë¡œì§ ê·¸ëŒ€ë¡œ ---
    documents = state["documents"]
    scores = state["doc_scores"]

    if not documents or not scores:
        print("[evaluate] ë¬¸ì„œ ì—†ìŒ â†’ LOW")
        return {"relevance_level": "low"}

    # âœ… ìˆ˜ì •: RRFëŠ” ë†’ì„ìˆ˜ë¡ ì¢‹ìœ¼ë¯€ë¡œ max ì‚¬ìš©
    best_score = max(scores)

    # âœ… ìˆ˜ì •: RRF ìŠ¤ì½”ì–´ íŠ¹ì„±ì— ë§ëŠ” ì„ê³„ê°’
    # RRF with K=60: 1ë“± = 1/61 â‰ˆ 0.0164, 2ë“± = 1/62 â‰ˆ 0.0161
    # ë‘ ê²€ìƒ‰ ë°©ë²•ì—ì„œ ëª¨ë‘ 1ë“±ì´ë©´: 0.0164 * 2 = 0.0328
    if best_score >= 0.020:  # ìƒìœ„ê¶Œì—ì„œ ì¤‘ë³µ ë°œê²¬
        level = "high"
    elif best_score >= 0.015:  # í•œìª½ì—ì„œë§Œ ìƒìœ„ê¶Œ
        level = "medium"
    else:  # ë‚®ì€ ìˆœìœ„
        level = "low"

    print(f"[evaluate] ìµœìƒìœ„ RRF ìŠ¤ì½”ì–´: {best_score:.6f} â†’ {level.upper()}")
    
    # ë””ë²„ê¹…ì„ ìœ„í•œ ìƒì„¸ ì •ë³´
    if len(scores) >= 3:
        print(f"[evaluate] Top-3 ìŠ¤ì½”ì–´: {scores[0]:.6f}, {scores[1]:.6f}, {scores[2]:.6f}")

    return {"relevance_level": level}


def cluster_similarity_check_node(state: GraphState) -> dict:
    """
    ë…¸ë“œ 3: í´ëŸ¬ìŠ¤í„° ìœ ì‚¬ë„ ì²´í¬

    ê²€ìƒ‰ëœ ë¬¸ì„œì™€ ê°™ì€ í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ì¶”ê°€ ë¬¸ì„œë“¤ì„ ê²€ìƒ‰í•˜ì—¬
    í´ëŸ¬ìŠ¤í„° ë‚´ í‰ê·  ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. í´ëŸ¬ìŠ¤í„° ë°€ë„(density)ë„
    í•¨ê»˜ ê³ ë ¤í•˜ì—¬ HIGH/LOWë¥¼ ê²°ì •í•©ë‹ˆë‹¤.

    ì¡°ê±´:
    - HIGH: avg_score <= 0.9 AND density >= 1.0
    - LOW: ê·¸ ì™¸

    Args:
        state: í˜„ì¬ GraphState

    Returns:
        dict: cluster_similarity_score, documentsë¥¼ í¬í•¨í•œ ìƒíƒœ ì—…ë°ì´íŠ¸
    """
    print("\n" + "="*60)
    print("[NODE: cluster_check] í´ëŸ¬ìŠ¤í„° ìœ ì‚¬ë„ ì²´í¬")
    print("="*60)

    cluster_id = state["cluster_id"]
    question = state["question"]
    vectorstore = state.get("_vectorstore")
    cluster_metadata_path = state.get("_cluster_metadata_path")

    if cluster_id is None or cluster_id == -1:
        print("[cluster_check] cluster_id ì—†ìŒ â†’ LOW")
        return {
            "cluster_similarity_score": 0.0,
            "search_type": "cluster"
        }

    try:
        # í´ëŸ¬ìŠ¤í„° ë©”íƒ€ë°ì´í„° ë¡œë“œ
        with open(cluster_metadata_path, 'r', encoding='utf-8') as f:
            cluster_meta = json.load(f)

        cluster_info = cluster_meta["clusters"].get(str(cluster_id))
        if not cluster_info:
            print(f"[cluster_check] cluster_id={cluster_id} ì •ë³´ ì—†ìŒ â†’ LOW")
            return {
                "cluster_similarity_score": 0.0,
                "search_type": "cluster"
            }

        cluster_density = cluster_info.get("density", 0.0)
        print(f"[cluster_check] Cluster {cluster_id} ë°€ë„: {cluster_density:.3f}")

        # ê°™ì€ í´ëŸ¬ìŠ¤í„° ë‚´ ì¶”ê°€ ë¬¸ì„œ ê²€ìƒ‰ (post-filtering ë°©ì‹)
        # ë” ë§ì€ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•œ í›„ cluster_idë¡œ í•„í„°ë§
        all_docs = vectorstore.similarity_search_with_score(question, k=20)

        # cluster_idë¡œ í•„í„°ë§
        filtered_docs = [
            (doc, score) for doc, score in all_docs
            if doc.metadata.get("cluster_id") == cluster_id
        ][:5]

        if not filtered_docs:
            print("[cluster_check] í´ëŸ¬ìŠ¤í„° ë‚´ ì¶”ê°€ ë¬¸ì„œ ì—†ìŒ â†’ LOW")
            return {
                "cluster_similarity_score": 0.0,
                "search_type": "cluster"
            }

        # í‰ê·  ì ìˆ˜ ê³„ì‚°
        avg_score = sum(score for doc, score in filtered_docs) / len(filtered_docs)

        print(f"[cluster_check] í´ëŸ¬ìŠ¤í„° ë‚´ ë¬¸ì„œ {len(filtered_docs)}ê°œ ê²€ìƒ‰")
        print(f"[cluster_check] í‰ê·  ì ìˆ˜: {avg_score:.4f}")

        # ê¸°ì¡´ ë¬¸ì„œì— ì¶”ê°€ ë¬¸ì„œ ë³‘í•© (ì¤‘ë³µ ì œê±°)
        existing_docs = state["documents"]
        additional_docs = [doc for doc, score in filtered_docs[:3]]

        # ì¤‘ë³µ ì œê±° (page_content ê¸°ì¤€)
        existing_contents = {doc.page_content for doc in existing_docs}
        unique_additional = [
            doc for doc in additional_docs
            if doc.page_content not in existing_contents
        ]

        merged_docs = existing_docs + unique_additional

        print(f"[cluster_check] ì¶”ê°€ ë¬¸ì„œ {len(unique_additional)}ê°œ ë³‘í•©")

        return {
            "cluster_similarity_score": avg_score,
            "documents": merged_docs,
            "search_type": "cluster"
        }

    except Exception as e:
        print(f"[ERROR] í´ëŸ¬ìŠ¤í„° ì²´í¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return {
            "cluster_similarity_score": 0.0,
            "search_type": "cluster"
        }


def web_search_node(state: GraphState) -> dict:
    """
    ë…¸ë“œ 4: ì›¹ ê²€ìƒ‰

    í´ëŸ¬ìŠ¤í„° ìœ ì‚¬ë„ê°€ ë‚®ì„ ë•Œ Tavily APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    Tavily APIê°€ ì‹¤íŒ¨í•˜ë©´ ê´€ë ¨ ì±—ë´‡ì´ ì•„ë‹ˆë¼ëŠ” ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    ê²€ìƒ‰ ê²°ê³¼ì—ì„œ URLì„ ì¶”ì¶œí•˜ì—¬ ë©”íƒ€ë°ì´í„°ì— í¬í•¨ì‹œí‚µë‹ˆë‹¤.

    Args:
        state: í˜„ì¬ GraphState

    Returns:
        dict: documents, search_typeì„ í¬í•¨í•œ ìƒíƒœ ì—…ë°ì´íŠ¸
    """
    print("\n" + "="*60)
    print("[NODE: web_search] ì›¹ ê²€ìƒ‰ ì‹œì‘")
    print("="*60)

    question = state["question"]

    # Tavily API ì‹œë„
    try:
        from langchain_community.retrievers import TavilySearchAPIRetriever
        print("[web_search] Tavily API ì‚¬ìš© ì¤‘...")

        retriever = TavilySearchAPIRetriever(k=5)
        web_docs_raw = retriever.invoke(question)

        print(f"[web_search] Tavilyë¡œ {len(web_docs_raw)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ")

        # â˜… ê°œì„ : ì›¹ ê²€ìƒ‰ ê²°ê³¼ì˜ ë©”íƒ€ë°ì´í„° ì •ë¦¬
        processed_web_docs = []
        for i, doc in enumerate(web_docs_raw):
            # Tavilyê°€ ë°˜í™˜í•˜ëŠ” ë©”íƒ€ë°ì´í„°: title, source, score ë“±
            original_meta = doc.metadata
            
            # ì œëª©ê³¼ ì¶œì²˜ ì¶”ì¶œ
            title = original_meta.get('title', 'ì›¹ ê²€ìƒ‰ ê²°ê³¼')
            source_url = original_meta.get('source', '')
            score = original_meta.get('score', 0.5)
            
            # ìƒˆë¡œìš´ Document ìƒì„± (ë©”íƒ€ë°ì´í„° í˜•ì‹ í†µì¼)
            web_doc = Document(
                page_content=doc.page_content,
                metadata={
                    'title': title,  # ì›¹ í˜ì´ì§€ ì œëª©
                    'source': source_url,  # URL
                    'source_type': 'web',  # ì›¹ ê²€ìƒ‰ì„ì„ ëª…ì‹œ
                    'score': score,
                    'index': i
                }
            )
            processed_web_docs.append(web_doc)
            
            print(f"  [{i+1}] {title[:60]}...")

        return {
            "documents": processed_web_docs,
            "search_type": "web",
            "doc_scores": [doc.metadata['score'] for doc in processed_web_docs]
        }

    except Exception as e:
        print(f"[web_search] Tavily ì‹¤íŒ¨: {e}")
        print("[web_search] AI/ML ë…¼ë¬¸ ê´€ë ¨ ì±—ë´‡ì´ ì•„ë‹™ë‹ˆë‹¤")

        # Tavily ì‹¤íŒ¨ ì‹œ ê´€ë ¨ ì±—ë´‡ì´ ì•„ë‹ˆë¼ëŠ” ë©”ì‹œì§€ë¥¼ ë‹´ì€ ë¬¸ì„œ ë°˜í™˜
        return {
            "documents": [Document(
                page_content="ì´ ì§ˆë¬¸ì€ AI/ML ì—°êµ¬ ë…¼ë¬¸ ì±—ë´‡ì˜ ë²”ìœ„ë¥¼ ë²—ì–´ë‚©ë‹ˆë‹¤.",
                metadata={"source": "system", "source_type": "error"}
            )],
            "search_type": "web_failed",
            "doc_scores": [2.0]  # ë†’ì€ ì ìˆ˜ = ê´€ë ¨ì„± ë‚®ìŒ
        }


def generate_final_answer_node(state: GraphState) -> dict:
    """
    ë…¸ë“œ 5: ìµœì¢… ë‹µë³€ ìƒì„±

    í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼(ë…¼ë¬¸ + ì›¹)ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©í•˜ì—¬
    AI Tech Trend Navigator ìŠ¤íƒ€ì¼ë¡œ ë‹µë³€ì„ ìƒì„±í•œë‹¤.
    """

    print("\n" + "=" * 60)
    print("[NODE: generate] ìµœì¢… ë‹µë³€ ìƒì„± (AI Tech Trend Navigator í”„ë¡¬í”„íŠ¸)")
    print("=" * 60)

    original_question = state["original_question"]      # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸
    documents = state["documents"]
    search_type = state.get("search_type", "internal")
    is_korean = state.get("is_korean", False)
    llm = state.get("_llm")

    # ë²ˆì—­ ì •ë³´ ì¶œë ¥ (ë””ë²„ê·¸ìš©)
    if is_korean:
        print(f"[generate] ì›ë³¸ ì§ˆë¬¸(í•œê¸€): {original_question}")
        print(f"[generate] ê²€ìƒ‰ ì§ˆë¬¸(ì˜ì–´): {state.get('question')}")

    # 1) CONTEXT ë¸”ë¡ ë§Œë“¤ê¸°
    if not documents:
        # ë„ˆê°€ ì§œë†“ì€ í”„ë¡¬í”„íŠ¸ ê·œì•½ì— ë§ì¶¤
        context_str = "NO_RELEVANT_PAPERS"
    else:
        context_blocks = []
        for i, doc in enumerate(documents[:5], 1):  # ìµœëŒ€ 5ê°œ ë¬¸ì„œë§Œ ì‚¬ìš©
            meta = doc.metadata or {}

            title = meta.get("title", meta.get("paper_name", "No information"))
            authors = meta.get("authors", "No information")
            hf_url = meta.get("huggingface_url", meta.get("source", "No information"))
            gh_url = meta.get("github_url", "No information")
            upvote = meta.get("upvote", "No information")
            year = meta.get("publication_year", "No information")
            total_chunks = meta.get("total_chunks", "No information")
            doc_id = meta.get("doc_id", "No information")
            chunk_index = meta.get("chunk_index", "No information")

            block = f"""
[DOCUMENT {i}]
page_content:
{doc.page_content[:1000]}

metadata:
  title: {title}
  huggingface_url: {hf_url}
  github_url: {gh_url}
  upvote: {upvote}
  authors: {authors}
  publication_year: {year}
  total_chunks: {total_chunks}
  doc_id: {doc_id}
  chunk_index: {chunk_index}
"""
            context_blocks.append(block)

        context_str = "\n".join(context_blocks)

    # 2) í”„ë¡¬í”„íŠ¸ ì •ì˜ (AI Tech Trend Navigator ë²„ì „)
    prompt = ChatPromptTemplate.from_messages([
        (
            "system",
            """
You are "AI Tech Trend Navigator", an expert assistant for AI/ML research papers.

[Role]
- You help users understand and leverage recent AI/ML papers collected from HuggingFace DailyPapers.
- Your main goals are:
  - Summarize and compare relevant papers clearly.
  - Explain core ideas in simple terms.
  - Highlight practical use-cases and implications for real-world services or products.

[Inputs]
The system provides:
- user_question: the userâ€™s question.
- context: a set of retrieved documents, formatted as a single text block.
- Sometimes the context may be exactly the string "NO_RELEVANT_PAPERS".
- Each paper in the context may contain:
  - page_content (text)
  - metadata:
    - title
    - github_url (optional)
    - huggingface_url
    - upvote (integer)
    - authors
    - publication_year
    - total_chunks
    - doc_id
    - chunk_index

You must rely only on:
- the given context, and
- general, high-level AI/ML knowledge.
Do NOT invent specific paper titles, authors, datasets, metrics, or numerical results that are not supported by the context.

[Context Handling]
- If the context is "NO_RELEVANT_PAPERS", it means:
  - The retrieval system could not find any clearly relevant papers.
  - In this case, you may answer purely from your own general AI/ML knowledge.
  - Do NOT fabricate specific paper titles, authors, datasets, or numerical results.
  - You may skip the "Related papers" section or keep it very generic.
- If the context contains one or more papers:
  - Prefer to base your answer on those papers.
  - Use only the papers that are reasonably related to the userâ€™s question.

[Main Tasks]
1. Understand the userâ€™s intent
   - Roughly classify the question as one of:
     (a) concept/background explanation
     (b) single-paper summary
     (c) comparison or trend analysis
     (d) practical application and use-case ideas
   - If the intent is ambiguous, make a reasonable assumption and continue.
2. Use only the relevant papers
   - Focus on the most relevant 1-3 papers.
   - Ignore clearly unrelated ones.
3. Summarize each selected paper
   - What problem it tries to solve.
   - What approach/model/idea it uses.
   - What is new or strong.
   - Any obvious limitations or trade-offs.
4. Produce a synthesized answer
   - Donâ€™t just list papers. Directly answer the userâ€™s question.
   - When relevant, relate ideas to RAG, long-context, multimodal, etc.
   - Suggest how to apply the ideas in real projects/services.
5. Be honest about uncertainty
   - If context is weak, say so.
   - Suggest what extra info or papers would help.

[Style]
- Prefer clear, concise sentences over heavy academic wording.
- Briefly explain technical terms when needed.
- Never fabricate paper titles, authors, datasets, or numerical results.
- Regardless of the input language, ALWAYS respond in Korean.
"""
        ),
        (
            "human",
            """
[QUESTION]
{question}

[CHAT HISTORY]
{chat_history}

[Context]
The following CONTEXT block may contain 0 or more papers.
If it is "NO_RELEVANT_PAPERS", please answer from your general AI/ML knowledge.

[CONTEXT]
======== START ========
{context}
======== END =========

Please structure your answer as follows (flexible, but try to follow this):

1) One-line summary  
2) Key insights (3-6 bullets)  
3) Detailed explanation  
4) Sources summary  
    Organize the papers used above based on metadata:
        [title: ...]
            - authors: ...
            - huggingface_url: ...
            - github_url: ...
            - ğŸ‘ 23

        [title: ...]
            - authors: ...
            - huggingface_url: ...
            - github_url: ...
            - ğŸ‘ 10

âš  For information not present in the metadata, write â€œNo information available.â€
âš  Do not hallucinate papers or details not shown in context.
âš  Regardless of the input language, ALWAYS respond in Korean.
âš  If you want to use bold, italics, and other text formatting elements, use â€˜HTMLâ€™. Markdown formatting is not supported.
    """)
    ])

    # 3) ì²´ì¸ ì‹¤í–‰
    try:
        chain = prompt | llm | StrOutputParser()
        answer = chain.invoke({
            "question": original_question,
            "chat_history": "",      # ì•„ì§ íˆìŠ¤í† ë¦¬ ì—†ìœ¼ë‹ˆ ë¹ˆ ë¬¸ìì—´
            "context": context_str
        })
        print("[generate] ë‹µë³€ ìƒì„± ì™„ë£Œ (AI Tech Trend Navigator ìŠ¤íƒ€ì¼)")
    except Exception as e:
        print(f"[ERROR] ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        answer = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    # 4) sources ë° ì°¸ê³  URL êµ¬ì„±
    sources: List[Dict[str, Any]] = []

    return {
        "answer": answer,
        "sources": sources
    }


def reject_node(state: GraphState) -> dict:
    """
    ë…¸ë“œ 6: ê±°ë¶€ ì‘ë‹µ

    ê´€ë ¨ì„±ì´ ë‚®ì€ ì§ˆë¬¸ì— ëŒ€í•´ ì •ì¤‘í•˜ê²Œ ê±°ë¶€í•˜ê³ 
    ë” ë‚˜ì€ ì§ˆë¬¸ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.

    Args:
        state: í˜„ì¬ GraphState

    Returns:
        dict: answer, sources, search_typeì„ í¬í•¨í•œ ìƒíƒœ ì—…ë°ì´íŠ¸
    """
    print("\n" + "="*60)
    print("[NODE: reject] ì§ˆë¬¸ ê±°ë¶€")
    print("="*60)

    question = state["original_question"]

    answer = f"""ì£„ì†¡í•©ë‹ˆë‹¤. '{question}'ì™€ ê´€ë ¨ëœ ì ì ˆí•œ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.

ë‹¤ìŒê³¼ ê°™ì´ ì‹œë„í•´ë³´ì„¸ìš”:
1. ë” êµ¬ì²´ì ì¸ í‚¤ì›Œë“œ ì‚¬ìš© (ì˜ˆ: "transformer", "attention mechanism")
2. ì˜ì–´ í•™ìˆ  ìš©ì–´ ì‚¬ìš©
3. ì§ˆë¬¸ì„ ë‹¤ì‹œ í‘œí˜„í•´ë³´ê¸°
4. AI/ML/DL ê´€ë ¨ ì£¼ì œë¡œ ì§ˆë¬¸í•˜ê¸°

ì´ ì‹œìŠ¤í…œì€ HuggingFaceì— ê²Œì‹œëœ AI/ML ì—°êµ¬ ë…¼ë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤."""

    print(f"[reject] ê±°ë¶€ ë©”ì‹œì§€ ë°˜í™˜")

    return {
        "answer": answer,
        "sources": [],
        "search_type": "rejected"
    }


# ===== SECTION 5: CONDITIONAL EDGE FUNCTIONS =====

def route_after_evaluate(state: GraphState) -> Literal["generate", "cluster_check", "reject"]:
    """
    ì¡°ê±´ë¶€ ë¼ìš°íŒ…: evaluate ë…¸ë“œ ì´í›„

    ê´€ë ¨ì„± ìˆ˜ì¤€ì— ë”°ë¼ ë‹¤ìŒ ë…¸ë“œë¥¼ ê²°ì •í•©ë‹ˆë‹¤:
    - high â†’ generate (ë°”ë¡œ ë‹µë³€ ìƒì„±)
    - medium â†’ cluster_check (í´ëŸ¬ìŠ¤í„° ìœ ì‚¬ë„ í™•ì¸)
    - low â†’ reject (ì§ˆë¬¸ ê±°ë¶€)

    Args:
        state: í˜„ì¬ GraphState

    Returns:
        str: ë‹¤ìŒ ë…¸ë“œ ì´ë¦„
    """
    level = state.get("relevance_level", "low")

    print(f"\n[ROUTING] evaluate â†’ {level.upper()}", end=" â†’ ")

    if level == "high":
        print("generate")
        return "generate"
    elif level == "medium":
        print("cluster_check")
        return "cluster_check"
    else:  # low
        print("reject")
        return "reject"


def route_after_cluster_check(state: GraphState) -> Literal["generate", "web_search"]:
    """
    ì¡°ê±´ë¶€ ë¼ìš°íŒ…: cluster_check ë…¸ë“œ ì´í›„

    í´ëŸ¬ìŠ¤í„° ìœ ì‚¬ë„ì™€ ë°€ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ ë…¸ë“œë¥¼ ê²°ì •í•©ë‹ˆë‹¤:
    - HIGH (avg_score <= 0.9 AND density >= 1.0) â†’ generate
    - LOW â†’ web_search

    Args:
        state: í˜„ì¬ GraphState

    Returns:
        str: ë‹¤ìŒ ë…¸ë“œ ì´ë¦„
    """
    cluster_score = state.get("cluster_similarity_score", 0.0)
    cluster_id = state.get("cluster_id", -1)
    cluster_metadata_path = state.get("_cluster_metadata_path")

    print(f"\n[ROUTING] cluster_check â†’ ", end="")

    # í´ëŸ¬ìŠ¤í„° ë©”íƒ€ë°ì´í„°ì—ì„œ ë°€ë„ í™•ì¸
    try:
        with open(cluster_metadata_path, 'r', encoding='utf-8') as f:
            cluster_meta = json.load(f)

        cluster_info = cluster_meta["clusters"].get(str(cluster_id), {})
        density = cluster_info.get("density", 0.0)

        # HIGH ì¡°ê±´: í‰ê·  ì ìˆ˜ <= 0.9 AND ë°€ë„ >= 1.0
        if cluster_score <= 0.9 and density >= 1.0:
            print(f"HIGH (score={cluster_score:.3f}, density={density:.3f}) â†’ generate")
            return "generate"
        else:
            print(f"LOW (score={cluster_score:.3f}, density={density:.3f}) â†’ web_search")
            return "web_search"

    except Exception as e:
        print(f"ERROR ({e}) â†’ web_search")
        return "web_search"


# ===== SECTION 6: GRAPH BUILDER =====

def build_langgraph_rag():
    """
    LangGraph StateGraph êµ¬ì¶• ë° ì»´íŒŒì¼

    ëª¨ë“  ë…¸ë“œì™€ ì¡°ê±´ë¶€ ì—£ì§€ë¥¼ í¬í•¨í•œ ì™„ì „í•œ ê·¸ë˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Returns:
        CompiledGraph: ì»´íŒŒì¼ëœ LangGraph ê°ì²´
    """
    print("\n" + "="*60)
    print("[GRAPH BUILD] LangGraph êµ¬ì¶• ì‹œì‘")
    print("="*60)

    # StateGraph ìƒì„±
    graph = StateGraph(GraphState)

    # ë…¸ë“œ ì¶”ê°€
    graph.add_node("translate", translate_node)
    graph.add_node("retrieve", retrieve_node)
    graph.add_node("evaluate", evaluate_document_relevance_node)
    graph.add_node("cluster_check", cluster_similarity_check_node)
    graph.add_node("web_search", web_search_node)
    graph.add_node("generate", generate_final_answer_node)
    graph.add_node("reject", reject_node)

    print("[GRAPH] 7ê°œ ë…¸ë“œ ì¶”ê°€ ì™„ë£Œ (translate í¬í•¨)")

    # ê¸°ë³¸ ì—£ì§€
    graph.add_edge(START, "translate")  # START â†’ translate (í•œê¸€ ë²ˆì—­)
    graph.add_edge("translate", "retrieve")  # translate â†’ retrieve
    graph.add_edge("retrieve", "evaluate")

    # ì¡°ê±´ë¶€ ì—£ì§€
    graph.add_conditional_edges(
        "evaluate",
        route_after_evaluate,
        {
            "generate": "generate",
            "cluster_check": "cluster_check",
            "reject": "reject"
        }
    )

    graph.add_conditional_edges(
        "cluster_check",
        route_after_cluster_check,
        {
            "generate": "generate",
            "web_search": "web_search"
        }
    )

    # ì¢…ë£Œ ì—£ì§€
    graph.add_edge("web_search", "generate")
    graph.add_edge("generate", END)
    graph.add_edge("reject", END)

    print("[GRAPH] ëª¨ë“  ì—£ì§€ ì¶”ê°€ ì™„ë£Œ")

    # ì»´íŒŒì¼
    compiled_graph = graph.compile()
    print("[GRAPH] ì»´íŒŒì¼ ì™„ë£Œ")

    return compiled_graph


# ===== SECTION 7: INTERACTIVE MODE =====

def run_interactive_mode(vectorstore, llm, bm25_retriever, cluster_metadata_path, langgraph_app):
    """
    ëŒ€í™”í˜• ëª¨ë“œ ì‹¤í–‰

    ì‚¬ìš©ìê°€ CMDì—ì„œ ì§ì ‘ ì§ˆë¬¸ì„ ì…ë ¥í•˜ê³  ë‹µë³€ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    'quit', 'exit', 'q'ë¥¼ ì…ë ¥í•˜ë©´ ì¢…ë£Œë©ë‹ˆë‹¤.

    Args:
        vectorstore: VectorStore ê°ì²´
        llm: LLM ê°ì²´
        bm25_retriever: BM25Retriever ê°ì²´
        cluster_metadata_path: í´ëŸ¬ìŠ¤í„° ë©”íƒ€ë°ì´í„° ê²½ë¡œ
        langgraph_app: ì»´íŒŒì¼ëœ LangGraph ì•±
    """
    print("\n" + "="*60)
    print("ëŒ€í™”í˜• ëª¨ë“œ ì‹œì‘")
    print("="*60)
    print("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: quit, exit, q)")
    print("="*60 + "\n")

    while True:
        try:
            # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
            question = input("\n[ì§ˆë¬¸] >> ").strip()

            # ì¢…ë£Œ ëª…ë ¹ ì²´í¬
            if question.lower() in ['quit', 'exit', 'q', 'ì¢…ë£Œ']:
                print("\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break

            # ë¹ˆ ì…ë ¥ ì²´í¬
            if not question:
                print("[ê²½ê³ ] ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue

            print(f"\n{'#'*60}")
            print(f"# ì§ˆë¬¸: {question}")
            print(f"{'#'*60}")

            # ì´ˆê¸° ìƒíƒœ êµ¬ì„±
            initial_state = {
                "question": "",  # ë²ˆì—­ í›„ ì—…ë°ì´íŠ¸ë¨
                "original_question": question,  # ì‚¬ìš©ì ì…ë ¥ ì›ë³¸
                "translated_question": None,
                "is_korean": False,
                "documents": [],
                "doc_scores": [],
                "cluster_id": None,
                "cluster_similarity_score": None,
                "search_type": "",
                "relevance_level": "",
                "answer": "",
                "sources": [],
                "_vectorstore": vectorstore,
                "_llm": llm,
                "_bm25_retriever": bm25_retriever,
                "_cluster_metadata_path": cluster_metadata_path
            }

            # LangGraph ì‹¤í–‰
            result = langgraph_app.invoke(initial_state)

            # ê²°ê³¼ ì¶œë ¥
            print("\n" + "="*60)
            print("[ìµœì¢… ê²°ê³¼]")
            print("="*60)
            print(f"ì›ë³¸ ì§ˆë¬¸: {result.get('original_question')}")
            if result.get('is_korean') and result.get('translated_question'):
                print(f"ë²ˆì—­ëœ ì§ˆë¬¸: {result.get('translated_question')}")
            print(f"ê²€ìƒ‰ ìœ í˜•: {result.get('search_type')}")
            print(f"ê´€ë ¨ì„± ìˆ˜ì¤€: {result.get('relevance_level')}")
            print(f"í´ëŸ¬ìŠ¤í„° ID: {result.get('cluster_id')}")
            print(f"\n[ë‹µë³€]\n{result['answer']}")
            print(f"\n[ì¶œì²˜ ê°œìˆ˜] {len(result.get('sources', []))}")

            # ì¶œì²˜ ìƒì„¸ ì •ë³´ (ì²˜ìŒ 3ê°œë§Œ)
            sources = result.get('sources', [])
            if sources:
                print("\n[ì£¼ìš” ì¶œì²˜]")
                for i, source in enumerate(sources[:3], 1):
                    if source.get('type') == 'paper':
                        print(f"  {i}. {source.get('title')} (upvote: {source.get('upvote', 0)})")
                        # HuggingFace URL í‘œì‹œ
                        if source.get('source'):
                            print(f"     HuggingFace: {source.get('source')}")
                        # GitHub URL í‘œì‹œ
                        if source.get('github_url'):
                            print(f"     GitHub: {source.get('github_url')}")
                    else:
                        print(f"  {i}. {source.get('title')} (ì›¹)")
                        # ì›¹ URL í‘œì‹œ
                        if source.get('source'):
                            print(f"     URL: {source.get('source')}")

            print("\n" + "-"*60)

        except KeyboardInterrupt:
            print("\n\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"\n[ERROR] ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()


# ===== SECTION 8: EXTERNAL API FUNCTIONS =====

# ì „ì—­ ë³€ìˆ˜ë¡œ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ (ì™¸ë¶€ì—ì„œ ì‚¬ìš©)
_vectorstore = None
_llm = None
_bm25_retriever = None
_cluster_metadata_path = None
_langgraph_app = None


def initialize_langgraph_system(
    model_name: Optional[str] = None,
    chunk_size: Optional[int] = None,
    chunk_overlap: Optional[int] = None,
    llm_model: str = "gpt-4o-mini",
    llm_temperature: float = 0
) -> dict:
    """
    LangGraph ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ì™¸ë¶€ ì„œë²„ì—ì„œ í˜¸ì¶œìš©)

    ì´ í•¨ìˆ˜ëŠ” FastAPI/Flask ì„œë²„ ì‹œì‘ ì‹œ í•œ ë²ˆ í˜¸ì¶œí•˜ì—¬
    VectorStore, LLM, LangGraphë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

    Args:
        model_name: ì„ë² ë”© ëª¨ë¸ ì´ë¦„ (ê¸°ë³¸ê°’: í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
        chunk_size: ì²­í¬ í¬ê¸° (ê¸°ë³¸ê°’: í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
        chunk_overlap: ì²­í¬ ì˜¤ë²„ë© (ê¸°ë³¸ê°’: í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
        llm_model: LLM ëª¨ë¸ ì´ë¦„
        llm_temperature: LLM temperature ì„¤ì •

    Returns:
        dict: ì´ˆê¸°í™” ìƒíƒœ ì •ë³´

    Example:
        >>> from langgraph_test import initialize_langgraph_system
        >>> result = initialize_langgraph_system()
        >>> print(result['status'])
        'success'
    """
    global _vectorstore, _llm, _bm25_retriever, _cluster_metadata_path, _langgraph_app

    try:
        print("\n[INIT] LangGraph ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")

        # í™˜ê²½ë³€ìˆ˜ì—ì„œ ê¸°ë³¸ê°’ ë¡œë“œ
        if model_name is None:
            model_name = MODEL_NAME
        if chunk_size is None:
            chunk_size = CHUNK_SIZE
        if chunk_overlap is None:
            chunk_overlap = CHUNK_OVERLAP

        # VectorStore ë¡œë“œ
        from vectordb import load_vectordb
        print(f"[LOADING] VectorStore ë¡œë”© ì¤‘... (model: {model_name})")
        _vectorstore = load_vectordb(model_name, chunk_size, chunk_overlap)

        # BM25 Retriever ì´ˆê¸°í™”
        print("[LOADING] BM25 Retriever ì´ˆê¸°í™” ì¤‘...")
        collection_data = _vectorstore._collection.get(include=['documents', 'metadatas'])
        all_documents = [
            Document(page_content=content, metadata=metadata)
            for content, metadata in zip(collection_data['documents'], collection_data['metadatas'])
        ]
        if not all_documents:
            raise ValueError('Chroma DBì— ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. BM25 ì¸ë±ìŠ¤ ìƒì„±ì´ ë¶ˆê°€í•©ë‹ˆë‹¤.')

        _bm25_retriever = BM25Retriever.from_documents(all_documents)
        _bm25_retriever.k = 3  # BM25 ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜ ì„¤ì •
        print(f"[SUCCESS] BM25 ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {len(all_documents)}ê°œ ë¬¸ì„œ")

        # LLM ì´ˆê¸°í™”
        print(f"[LOADING] LLM ì´ˆê¸°í™” ì¤‘... (model: {llm_model})")
        _llm = ChatOpenAI(model=llm_model, temperature=llm_temperature)

        # Cluster metadata path
        _cluster_metadata_path = str(PROJECT_ROOT / "01_data" / "clusters" / "cluster_metadata.json")

        # LangGraph ì»´íŒŒì¼
        print("[LOADING] LangGraph ì»´íŒŒì¼ ì¤‘...")
        _langgraph_app = build_langgraph_rag()

        print("[SUCCESS] LangGraph ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!\n")

        return {
            'status': 'success',
            'message': 'LangGraph system initialized successfully',
            'config': {
                'model_name': model_name,
                'chunk_size': chunk_size,
                'chunk_overlap': chunk_overlap,
                'llm_model': llm_model,
                'llm_temperature': llm_temperature
            }
        }

    except Exception as e:
        print(f"[ERROR] ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return {
            'status': 'error',
            'message': str(e)
        }


def ask_question(question: str, verbose: bool = False) -> dict:
    """
    LangGraphë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€ (ì™¸ë¶€ ì„œë²„ì—ì„œ í˜¸ì¶œìš©)

    ì´ í•¨ìˆ˜ëŠ” FastAPI/Flask ì—”ë“œí¬ì¸íŠ¸ì—ì„œ í˜¸ì¶œí•˜ì—¬
    ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
        verbose: ìƒì„¸ ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€

    Returns:
        dict: ë‹µë³€ ë° ì¶œì²˜ ì •ë³´
        {
            'success': bool,
            'question': str,
            'answer': str,  # URL í¬í•¨ëœ ë‹µë³€
            'sources': List[dict],
            'metadata': {
                'search_type': str,
                'relevance_level': str,
                'cluster_id': int,
                'is_korean': bool,
                'translated_question': str
            }
        }

    Example:
        >>> from langgraph_test import initialize_langgraph_system, ask_question
        >>> initialize_langgraph_system()
        >>> result = ask_question("What is transformer architecture?")
        >>> print(result['answer'])
    """
    global _vectorstore, _llm, _bm25_retriever, _cluster_metadata_path, _langgraph_app

    # ì´ˆê¸°í™” í™•ì¸
    if _langgraph_app is None:
        return {
            'success': False,
            'error': 'LangGraph system not initialized. Call initialize_langgraph_system() first.'
        }

    try:
        if verbose:
            print(f"\n[QUESTION] {question}")

        # ì´ˆê¸° ìƒíƒœ êµ¬ì„±
        initial_state = {
            "question": "",  # ë²ˆì—­ í›„ ì—…ë°ì´íŠ¸ë¨
            "original_question": question,
            "translated_question": None,
            "is_korean": False,
            "documents": [],
            "doc_scores": [],
            "cluster_id": None,
            "cluster_similarity_score": None,
            "search_type": "",
            "relevance_level": "",
            "answer": "",
            "sources": [],
            "_vectorstore": _vectorstore,
            "_llm": _llm,
            "_bm25_retriever": _bm25_retriever,
            "_cluster_metadata_path": _cluster_metadata_path
        }

        # LangGraph ì‹¤í–‰
        result = _langgraph_app.invoke(initial_state)

        if verbose:
            print(f"[ANSWER] ë‹µë³€ ìƒì„± ì™„ë£Œ")
            print(f"[SOURCES] {len(result.get('sources', []))}ê°œ ì¶œì²˜")

        # ì‘ë‹µ êµ¬ì„±
        return {
            'success': True,
            'question': question,
            'answer': result.get('answer', ''),
            'sources': result.get('sources', []),
            'metadata': {
                'search_type': result.get('search_type', ''),
                'relevance_level': result.get('relevance_level', ''),
                'cluster_id': result.get('cluster_id'),
                'is_korean': result.get('is_korean', False),
                'translated_question': result.get('translated_question')
            }
        }

    except Exception as e:
        print(f"[ERROR] ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return {
            'success': False,
            'error': str(e)
        }


def get_system_status() -> dict:
    """
    ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ (ì™¸ë¶€ ì„œë²„ì—ì„œ í˜¸ì¶œìš©)

    Returns:
        dict: ì‹œìŠ¤í…œ ì´ˆê¸°í™” ìƒíƒœ
    """
    return {
        'initialized': _langgraph_app is not None,
        'vectorstore_loaded': _vectorstore is not None,
        'llm_loaded': _llm is not None,
        'bm25_retriever_loaded': _bm25_retriever is not None,
        'cluster_metadata_loaded': _cluster_metadata_path is not None
    }


# ===== SECTION 9: MAIN EXECUTION =====

if __name__ == "__main__":
    print("\n" + "="*60)
    print("LangGraph RAG System - ëŒ€í™”í˜• ëª¨ë“œ")
    print("="*60)

    # ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™”
    print("\n[INIT] ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™” ì‹œì‘...")

    try:
        from vectordb import load_vectordb

        # VectorStore ë¡œë“œ
        print("[LOADING] VectorStore ë¡œë”© ì¤‘...")
        vectorstore = load_vectordb(MODEL_NAME, CHUNK_SIZE, CHUNK_OVERLAP)

        # BM25 Retriever ì´ˆê¸°í™”
        print("[LOADING] BM25 Retriever ì´ˆê¸°í™” ì¤‘...")
        collection_data = vectorstore._collection.get(include=['documents', 'metadatas'])
        all_documents = [
            Document(page_content=content, metadata=metadata)
            for content, metadata in zip(collection_data['documents'], collection_data['metadatas'])
        ]
        if not all_documents:
            raise ValueError('Chroma DBì— ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. BM25 ì¸ë±ìŠ¤ ìƒì„±ì´ ë¶ˆê°€í•©ë‹ˆë‹¤.')

        bm25_retriever = BM25Retriever.from_documents(all_documents)
        bm25_retriever.k = 3  # BM25 ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜ ì„¤ì •
        print(f"[SUCCESS] BM25 ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {len(all_documents)}ê°œ ë¬¸ì„œ")

        # LLM ì´ˆê¸°í™”
        print("[LOADING] LLM ì´ˆê¸°í™” ì¤‘...")
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

        # Cluster metadata path
        cluster_metadata_path = str(PROJECT_ROOT / "01_data" / "clusters" / "cluster_metadata.json")

        # LangGraph ì»´íŒŒì¼
        print("[LOADING] LangGraph ì»´íŒŒì¼ ì¤‘...")
        langgraph_app = build_langgraph_rag()

        print("\n[SUCCESS] ëª¨ë“  ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ!\n")

        # ëŒ€í™”í˜• ëª¨ë“œ ì‹¤í–‰
        run_interactive_mode(vectorstore, llm, bm25_retriever, cluster_metadata_path, langgraph_app)

    except Exception as e:
        print(f"\n[ERROR] ì´ˆê¸°í™” ë˜ëŠ” ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
