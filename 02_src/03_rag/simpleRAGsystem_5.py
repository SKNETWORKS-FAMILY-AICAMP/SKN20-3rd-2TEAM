import os
import warnings
import sys
from dotenv import load_dotenv

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from pathlib import Path
from duckduckgo_search import DDGS  # pip install duckduckgo-search

# ê²½ê³ ë©”ì„¸ì§€ ì‚­ì œ
warnings.filterwarnings('ignore')
load_dotenv()

# openapi key í™•ì¸
API_KEY = os.getenv('OPENAI_API_KEY')
if not API_KEY:
    raise ValueError('.enví™•ì¸, keyì—†ìŒ')

# vectordb ëª¨ë“ˆ import
SRC_DIR = Path(__file__).parent.parent
sys.path.insert(0, str(SRC_DIR / "02_utils"))
from vectordb import load_vectordb


class SimpleRAGSystem:
    '''ê°„ë‹¨í•œ RAG ì‹œìŠ¤í…œ ë˜í¼ í´ë˜ìŠ¤'''
    def __init__(self, vectorstore, llm, retriever_k=3):
        self.vectorstore = vectorstore
        self.llm = llm
        self.retriever_k = retriever_k
        self.retriever = vectorstore.as_retriever(
            search_type='similarity',
            search_kwargs={'k': retriever_k}
        )
        self.chain = self._build_chain()
        self.chat_history = []
    
    def _build_chain(self):
        '''RAG ì²´ì¸ êµ¬ì„±''' 
        prompt = ChatPromptTemplate.from_messages([
            ("system", """
You are **"AI Tech Trend Navigator"**, an expert assistant for AI/ML research papers.

[Role]
- You help users understand and leverage recent AI/ML papers collected from HuggingFace Weekly Papers.
- Your goals:
  - Summarize papers clearly.
  - Explain core ideas simply.
  - Highlight practical use-cases for real products/services.

[Inputs Provided]
The system supplies:
- user_question
- chat_history
- context:
    - If there are relevant papers:
        â†’ a concatenation of [Paper i] blocks.
    - If there are NO relevant papers:
        â†’ a string that begins with the line EXACTLY:
        NO_RELEVANT_PAPERS
        followed by one or more [WebResult i] blocks from DuckDuckGo.

**IMPORTANT MODE SWITCH**
If the FIRST LINE of context is EXACTLY "NO_RELEVANT_PAPERS":
    â†’ You MUST answer using ONLY general AI/ML knowledge + DuckDuckGo results.
    â†’ You MUST output using strictly the <web search using> format.
    â†’ You MUST NOT output:
        - "Sources summary"
        - Any paper list
        - Any paper title
        - ANY reference to HuggingFace papers or metadata

If context contains papers:
    â†’ You MUST ignore DuckDuckGo rules COMPLETELY.
    â†’ You MUST answer ONLY based on:
         (1) given context papers
         (2) general high-level knowledge (no invented details)
    â†’ You MUST output using the "paper mode" format:
         1) One-line summary
         2) Key insights
         3) Related papers
         4) Detailed explanation
         5) Sources summary (based strictly on metadata)

[PAPER MODE â€” detailed behavior]
- Use only relevant papers (1-3 typically).
- DO NOT hallucinate titles, authors, datasets, years, URLs, metrics, or numbers.
- If metadata items are missing, write "No information".
- If papers do not directly answer the user's question, explicitly say so.

Paper Output Format:
1) One-line summary
2) Key insights (3~6 bullets)
3) Related papers (only titles)
4) Detailed explanation
5) Sources summary
   For each used paper:
     Â· title:
     Â· authors:
     Â· huggingface_url:
     Â· github_url:
     Â· upvote:

[WEB SEARCH MODE â€” detailed behavior]
Triggered ONLY when the FIRST LINE of context is EXACTLY "NO_RELEVANT_PAPERS".

Below that line, you will see one or more blocks like:
    [WebResult 1]
    title: ...
    url: ...
    snippet: ...
Use them as your ONLY external information.

Output MUST follow this EXACT structure:
1) One-line summary
2) Detailed explanation
3) source : DuckDuckgo
   - First URL
   - Second URL

URL RULES (VERY IMPORTANT):
- ONLY two URL use
- The URLs written under "source : DuckDuckgo" MUST be copied **exactly** from the `url:` fields.
- DO NOT invent or fabricate URLs.
- If only one valid URL exists, output only one URL line.
- If NO valid URLs exist, output:
    3) source : DuckDuckgo
       - (no URL available)

You MUST NOT:
- Output "Sources summary" in this mode
- Mention HuggingFace papers
- Output any paper titles

[Style]
- ALWAYS respond in Korean.
- Keep explanations clear and non-academic.
- Briefly explain technical terms when helpful.
"""),
            ("human", """
[QUESTION]
{question}

[CHAT HISTORY]
{chat_history}

[Context]
======= START =======
{context}
======= END =======

Follow the output rules based on whether papers exist.
""")
        ])
        return (
            prompt
            | self.llm
            | StrOutputParser()
        )
    
    def _web_search(self, query: str, num_results: int = 5):
        """DuckDuckGoë¡œ ê²€ìƒ‰ (duckduckgo-search ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©)"""
        try:
            ddgs = DDGS()
            results = []
            
            # text() ë©”ì„œë“œë¡œ ê²€ìƒ‰ ìˆ˜í–‰
            search_results = ddgs.text(query, max_results=num_results)
            
            for item in search_results:
                results.append({
                    "title": item.get("title", ""),
                    "url": item.get("href", ""),
                    "snippet": item.get("body", "")
                })
            
            return results
        except Exception as e:
            print(f"ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def _format_web_results(self, results):
        """ê²€ìƒ‰ ê²°ê³¼ â†’ LLM í”„ë¡¬í”„íŠ¸ìš© í…ìŠ¤íŠ¸"""
        if not results:
            return "NO_WEB_RESULTS"

        blocks = []
        for i, r in enumerate(results, 1):
            blocks.append(f"""
[WebResult {i}]
title: {r['title']}
url: {r['url']}

snippet:
{r['snippet']}
""")
        return "\n\n".join(blocks)
    
    @staticmethod
    def _format_docs(docs):
        """retrieverê°€ ë°˜í™˜í•œ Documentë“¤ì„ í”„ë¡¬í”„íŠ¸ìš© í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        if not docs:
            return "NO_RELEVANT_PAPERS"

        lines = []
        for i, doc in enumerate(docs, start=1):
            md = doc.metadata or {}

            title = md.get("title") or md.get("paper_name") or "No information"

            raw_authors = md.get("authors")
            if isinstance(raw_authors, list):
                authors = ", ".join(raw_authors)
            else:
                authors = raw_authors or "No information"

            huggingface_url = md.get("huggingface_url") or "No information"
            github_url = md.get("github_url") or "No information"
            upvote = md.get("upvote") or "No information"
            publication_year = md.get("publication_year") or "No information"
            doc_id = md.get("doc_id") or "No information"
            chunk_index = md.get("chunk_index") or "No information"

            block = f"""
[Paper {i}]
title: {title}
authors: {authors}
huggingface_url: {huggingface_url}
github_url: {github_url}
upvote: {upvote}
publication_year: {publication_year}
doc_id: {doc_id}
chunk_index: {chunk_index}

content:
{doc.page_content}
"""
            lines.append(block)

        return "\n\n".join(lines)

    def _format_chat_history(self):
        """ì €ì¥ëœ ëŒ€í™” ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ êµ¬ì„±"""
        if not self.chat_history:
            return "(no previous conversation)"

        history_lines = []
        for turn in self.chat_history:
            history_lines.append(f"User: {turn['user']}")
            history_lines.append(f"Assistant: {turn['assistant']}")
        return "\n".join(history_lines)

    def chat(self, user_message: str, score_threshold: float = 1.2) -> str:
        """
        ëŒ€í™” ëª¨ë“œ: íˆìŠ¤í† ë¦¬ ì €ì¥ + RAG ë‹µë³€
        
        Args:
            user_message: ì‚¬ìš©ì ì§ˆë¬¸
            score_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’ (ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬í•¨, ê¸°ë³¸ê°’ 1.0)
        """
        # 1. similarity_search_with_score ìˆ˜í–‰
        docs_and_scores = self.vectorstore.similarity_search_with_score(
            user_message, 
            k=self.retriever_k
        )
        
        # ë””ë²„ê¹…ìš© ì¶œë ¥
        print(f"\n[DEBUG] ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(docs_and_scores)}")
        for i, (doc, score) in enumerate(docs_and_scores):
            title = doc.metadata.get("title", "ì œëª©ì—†ìŒ")
            print(f"  ë¬¸ì„œ {i+1}: score={score:.4f}, title={title}")

        # 2. score ê¸°ì¤€ í•„í„°ë§ (ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬ â†’ <= ì‚¬ìš©)
        relevant_docs = [doc for doc, score in docs_and_scores if score <= score_threshold]
        
        print(f"[DEBUG] ì„ê³„ê°’ {score_threshold} ì´í•˜ ë¬¸ì„œ: {len(relevant_docs)}ê°œ\n")

        # 3. context ê²°ì •
        if not relevant_docs:
            # ì›¹ ê²€ìƒ‰ ëª¨ë“œ
            print("[INFO] ê´€ë ¨ ë…¼ë¬¸ ì—†ìŒ â†’ ì›¹ ê²€ìƒ‰ ëª¨ë“œ ì‹¤í–‰")
            web_results = self._web_search(user_message)
            web_block = self._format_web_results(web_results)
            context = "NO_RELEVANT_PAPERS\n\n" + web_block
        else:
            # ë…¼ë¬¸ ëª¨ë“œ
            print(f"[INFO] ë…¼ë¬¸ ëª¨ë“œ ì‹¤í–‰ (ê´€ë ¨ ë…¼ë¬¸ {len(relevant_docs)}ê°œ)")
            context = self._format_docs(relevant_docs)

        # 4. chain ì‹¤í–‰
        response = self.chain.invoke({
            "question": user_message,
            "context": context,
            "chat_history": self._format_chat_history()
        })

        # 5. íˆìŠ¤í† ë¦¬ ì €ì¥
        self.chat_history.append({
            "user": user_message,
            "assistant": response
        })

        return response

    def ask(self, question: str, score_threshold: float = 1.2) -> str:
        """
        ì§ˆë¬¸ì— ë‹µë³€ (ë‹¨ë°œì„±)
        
        Args:
            question: ì§ˆë¬¸ ë‚´ìš©
            score_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
        """
        # 1. ë²¡í„°DB ê²€ìƒ‰
        docs_and_scores = self.vectorstore.similarity_search_with_score(
            question,
            k=self.retriever_k
        )
        
        # 2. score í•„í„°ë§
        relevant_docs = [doc for doc, score in docs_and_scores if score <= score_threshold]

        # 3. context ê²°ì •
        if not relevant_docs:
            web_results = self._web_search(question)
            web_block = self._format_web_results(web_results)
            context = "NO_RELEVANT_PAPERS\n\n" + web_block
        else:
            context = self._format_docs(relevant_docs)

        # 4. chain ì‹¤í–‰
        return self.chain.invoke({
            "question": question,
            "context": context,
            "chat_history": self._format_chat_history()
        })

    def ask_with_sources(self, question: str, stream: bool = False, score_threshold: float = 1.2):
        """ì§ˆë¬¸ì— ë‹µë³€ + ì¶œì²˜ ë°˜í™˜"""
        # 1. similarity_search_with_score ì‚¬ìš©
        docs_and_scores = self.vectorstore.similarity_search_with_score(
            question,
            k=self.retriever_k
        )
        
        # 2. score ê¸°ì¤€ í•„í„°ë§ (ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬)
        relevant_docs = [doc for doc, score in docs_and_scores if score <= score_threshold]

        # 3. context ê²°ì •
        if not relevant_docs:
            # ì›¹ ê²€ìƒ‰ ëª¨ë“œ
            web_results = self._web_search(question)
            web_block = self._format_web_results(web_results)
            context = "NO_RELEVANT_PAPERS\n\n" + web_block

            sources = [{
                "paper_name": r["title"],
                "huggingface_url": None,
                "github_url": None,
                "upvote": None,
                "url": r["url"]
            } for r in web_results]
        else:
            # ë…¼ë¬¸ ëª¨ë“œ
            context = self._format_docs(relevant_docs)
            sources = []
            for doc in relevant_docs:
                md = doc.metadata or {}
                title = md.get("title") or md.get("paper_name") or "(no title)"

                raw_authors = md.get("authors")
                if isinstance(raw_authors, list):
                    authors = ", ".join(raw_authors)
                else:
                    authors = raw_authors or "No information"

                sources.append({
                    "paper_name": title,
                    "authors": authors,
                    "huggingface_url": md.get("huggingface_url"),
                    "github_url": md.get("github_url"),
                    "upvote": md.get("upvote"),
                })

        # 4. chain ì‹¤í–‰
        chain_input = {
            "question": question,
            "context": context,
            "chat_history": self._format_chat_history()
        }

        if stream:
            return {
                "answer_stream": self.chain.stream(chain_input),
                "sources": sources,
            }
        else:
            answer = self.chain.invoke(chain_input)
            return {
                "answer": answer,
                "sources": sources,
            }

    def clear_history(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.chat_history = []


# if __name__ == '__main__':
#     # chunk íŒŒì¼ë¡œ ì„ì‹œ í™•ì¸
#     def get_project_root():
#         curr = Path().resolve()
#         for parent in [curr] + list(curr.parents):
#             if (parent / ".git").exists():
#                 return parent
#         raise FileNotFoundError("í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸° ì‹¤íŒ¨")

#     MODEL_NAME = os.getenv("MODEL_NAME")
#     CHUNK_SIZE = int(os.getenv("CHUNK_SIZE"))
#     CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP"))
    
#     vectorstore = load_vectordb(
#             model_name=MODEL_NAME,
#             chunk_size=CHUNK_SIZE,
#             chunk_overlap=CHUNK_OVERLAP
#     )
  
#     llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)

#     rag_system = SimpleRAGSystem(vectorstore, llm)
#     user_question = "í•´ë¦¬í¬í„° ì¤„ê±°ë¦¬ ì•Œë ¤ì¤˜"
#     result = rag_system.ask_with_sources(user_question)

#     print(f"ì§ˆë¬¸: {user_question}")
#     print("\n[ë‹µë³€]\n")
#     print(result["answer"])

#     # --------------------------------------------------------
#     # ğŸ”¥ ì—¬ê¸° ì•„ë˜ ì±—ë´‡ ëª¨ë“œ ì…ë ¥ ë£¨í”„ ë„£ìœ¼ë©´ ë¨!
#     # --------------------------------------------------------

#     print("\n=== AI Tech Trend Navigator Chatbot ===")
#     print("ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ë˜ëŠ” 'quit' ì…ë ¥\n")

#     while True:
#         user_msg = input("You: ")

#         if user_msg.lower() in ["exit", "quit"]:
#             print("ì±—ë´‡ ì¢…ë£Œ!")
#             break

#         answer = rag_system.chat(user_msg)
#         print(f"\nAssistant:\n{answer}\n")

if __name__ == '__main__':
    MODEL_NAME = os.getenv("MODEL_NAME")
    CHUNK_SIZE = int(os.getenv("CHUNK_SIZE"))
    CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP"))
    
    vectorstore = load_vectordb(
        model_name=MODEL_NAME,
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP
    )
  
    llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)
    rag_system = SimpleRAGSystem(vectorstore, llm, retriever_k=5)

    # í…ŒìŠ¤íŠ¸
    print("=== í…ŒìŠ¤íŠ¸ 1: ë…¼ë¬¸ ê²€ìƒ‰ ===")
    result = rag_system.ask_with_sources("transformer architecture", score_threshold=1.2)
    print(f"\n[ë‹µë³€]\n{result['answer']}\n")
    print(f"[ì¶œì²˜ ìˆ˜]: {len(result['sources'])}")

    print("\n=== AI Tech Trend Navigator Chatbot ===")
    print("ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ë˜ëŠ” 'quit' ì…ë ¥\n")

    while True:
        user_msg = input("You: ")
        if user_msg.lower() in ["exit", "quit"]:
            print("ì±—ë´‡ ì¢…ë£Œ!")
            break

        # score_threshold ì¡°ì • ê°€ëŠ¥ (ê¸°ë³¸ê°’ 1.0)
        answer = rag_system.chat(user_msg, score_threshold=1.2)
        print(f"\nAssistant:\n{answer}\n")