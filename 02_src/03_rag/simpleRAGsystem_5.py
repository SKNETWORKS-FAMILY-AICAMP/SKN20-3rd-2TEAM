import os
import warnings
import sys
from dotenv import load_dotenv

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from pathlib import Path
import requests


# ê²½ê³ ë©”ì„¸ì§€ ì‚­ì œ
warnings.filterwarnings('ignore')
load_dotenv()

# openapi key í™•ì¸
API_KEY = os.getenv('OPENAI_API_KEY')
if not API_KEY:
    raise ValueError('.enví™•ì¸,  keyì—†ìŒ')

# vectordb ëª¨ë“ˆ import
SRC_DIR=Path(__file__).parent.parent
sys.path.insert(0, str(SRC_DIR / "02_utils"))
from vectordb import load_vectordb


class SimpleRAGSystem:
    '''ê°„ë‹¨í•œ RAG ì‹œìŠ¤í…œ ë˜í¼ í´ë˜ìŠ¤'''
    def __init__(self, vectorstore, llm, retriever_k=3):
        self.vectorstore = vectorstore
        self.llm = llm
        self.retriever = vectorstore.as_retriever(search_type = 'similarity', search_kwargs={'k':retriever_k})
        self.chain = self._build_chain()
        self.chat_history = []
    

    def _build_chain(self): ### ---------> ìµœì¢… ì‚¬ìš©ìì—ê²Œ ì „ë‹¬ë˜ëŠ” í”„ë¡¬í”„íŠ¸ ìˆ˜ì •
        '''RAG ì²´ì¸ êµ¬ì„±''' 
        
        prompt = ChatPromptTemplate.from_messages([
                ("system", """
You are **"AI Tech Trend Navigator"**, an expert assistant for AI/ML research papers.

[Role]
- You help users understand and leverage recent AI/ML papers collected from HuggingFace Weekly Papers.
- Your goals:
  - Summarize papers clearly.
  - Explain core ideas simply.
  - Highlight practical use-cases for real products/services.

[Inputs Provided]
The system supplies:
- user_question
- chat_history
- context (either a set of papers OR EXACT STRING: "NO_RELEVANT_PAPERS")

**IMPORTANT MODE SWITCH**
If context == "NO_RELEVANT_PAPERS":
    â†’ You MUST answer using ONLY general AI/ML knowledge + DuckDuckGo results.
    â†’ You MUST output using strictly the <web search useing> format.
    â†’ You MUST NOT output:
        - "Sources summary"
        - Any paper list
        - Any paper title
        - ANY reference to HuggingFace papers or metadata

If context contains papers:
    â†’ You MUST ignore DuckDuckGo rules COMPLETELY.
    â†’ You MUST answer ONLY based on:
         (1) given context papers
         (2) general high-level knowledge (no invented details)
    â†’ You MUST output using the "paper mode" format:
         1) One-line summary
         2) Key insights
         3) Related papers
         4) Detailed explanation
         5) Sources summary (based strictly on metadata)

[PAPER MODE â€” detailed behavior]
- Use only relevant papers (1â€“3 typically).
- DO NOT hallucinate titles, authors, datasets, years, URLs, metrics, or numbers.
- If metadata items are missing, write â€œNo informationâ€.
- If papers do not directly answer the userâ€™s question, explicitly say so.

Paper Output Format:
1) One-line summary
2) Key insights (3~6 bullets)
3) Related papers (only titles)
4) Detailed explanation
5) Sources summary
   For each used paper:
     Â· title:
     Â· authors:
     Â· huggingface_url:
     Â· github_url:
     Â· upvote:

[WEB SEARCH MODE â€” detailed behavior]
Triggered ONLY when context == "NO_RELEVANT_PAPERS".

Output MUST follow this EXACT structure:
1) One-line summary
2) Detailed explanation
3) source : DuckDuckgo
   - First URL
   - Second URL

RULES FOR WEB SEARCH MODE:
- NEVER output â€œSources summaryâ€
- NEVER output a paper title
- NEVER mention HuggingFace papers
- Treat results as general information, not research papers.

[Style]
- ALWAYS respond in Korean.
- Keep explanations clear and non-academic.
- Briefly explain technical terms when helpful.
"""),


("human", """
[QUESTION]
{question}

[CHAT HISTORY]
{chat_history}

[Context]
======= START =======
{context}
======= END =======

Follow the output rules based on whether papers exist.
""")
            ])
        return (
            prompt
            | self.llm
            | StrOutputParser()
        )
    
    
    def _web_search(self, query: str, num_results: int = 5):
        """DuckDuckGo APIë¡œ ê²€ìƒ‰"""
        url = "https://api.duckduckgo.com/"
        params = {
            "q": query,
            "format": "json",
            "no_redirect": 1,
            "no_html": 1,
        }
        res = requests.get(url, params=params)

        if res.status_code != 200:
            return []

        data = res.json()
        results = []

        # DuckDuckGoëŠ” ì£¼ìš” ê²€ìƒ‰ ê²°ê³¼ê°€ 'RelatedTopics'ì— ë“¤ì–´ê°
        for item in data.get("RelatedTopics", []):
            if "Text" in item:
                results.append({
                    "title": item.get("Text", ""),
                    "url": item.get("FirstURL", ""),
                    "snippet": item.get("Text", "")
                })

            # ì¼ë¶€ëŠ” ë‚´ë¶€ topics í˜•íƒœë¡œ ë“¤ì–´ìˆì„ ìˆ˜ ìˆìŒ
            if "Topics" in item:
                for t in item["Topics"]:
                    results.append({
                        "title": t.get("Text", ""),
                        "url": t.get("FirstURL", ""),
                        "snippet": t.get("Text", "")
                    })

        return results[:num_results]

    def _format_web_results(self, results):
        """ê²€ìƒ‰ ê²°ê³¼ â†’ LLM í”„ë¡¬í”„íŠ¸ìš© í…ìŠ¤íŠ¸"""
        if not results:
            return "NO_WEB_RESULTS"

        blocks = []
        for i, r in enumerate(results, 1):
            blocks.append(f"""
[WebResult {i}]
title: {r['title']}
url: {r['url']}

snippet:
{r['snippet']}
""")
        return "\n\n".join(blocks)
    



    @staticmethod
    def _format_docs(docs):
        """retrieverê°€ ë°˜í™˜í•œ Documentë“¤ì„ í”„ë¡¬í”„íŠ¸ìš© í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        if not docs:
            return "NO_RELEVANT_PAPERS"

        lines = []

        for i, doc in enumerate(docs, start=1):
            md = doc.metadata or {}

            block = f"""
    [Paper {i}]
    title: {md.get("title", "No information")}
    authors: {md.get("authors", "No information")}
    huggingface_url: {md.get("huggingface_url", "No information")}
    github_url: {md.get("github_url", "No information")}
    upvote: {md.get("upvote", "No information")}
    publication_year: {md.get("publication_year", "No information")}
    doc_id: {md.get("doc_id", "No information")}
    chunk_index: {md.get("chunk_index", "No information")}
    

    content:
    {doc.page_content}
    """

            lines.append(block)

        return "\n\n".join(lines)



    def _format_chat_history(self):
        """ì €ì¥ëœ ëŒ€í™” ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ êµ¬ì„±"""
        if not self.chat_history:
            return "(no previous conversation)"

        history_lines = []
        for turn in self.chat_history:
            history_lines.append(f"User: {turn['user']}")
            history_lines.append(f"Assistant: {turn['assistant']}")
        return "\n".join(history_lines)


    def chat(self, user_message: str) -> str:
        """ëŒ€í™” ëª¨ë“œ: íˆìŠ¤í† ë¦¬ ì €ì¥ + RAG ë‹µë³€ (similarity score í›„ì²˜ë¦¬)"""
        # 1. similarity_search_with_score ìˆ˜í–‰
        docs_and_scores = self.vectorstore.similarity_search_with_score(user_message, k=5)

        # 2. score ê¸°ì¤€ í•„í„°ë§
        score_threshold = 0.7
        relevant_docs = [doc for doc, score in docs_and_scores if score >= score_threshold]

        # 3. context ê²°ì •
        if not relevant_docs:
            context = "NO_RELEVANT_PAPERS"
            web_results = self._web_search(user_message)
            context = self._format_web_results(web_results)
        else:
            context = self._format_docs(relevant_docs)

        # 4. chain ì‹¤í–‰
        response = self.chain.invoke({
            "question": user_message,
            "context": context,
            "chat_history": self._format_chat_history()
        })

        # 5. íˆìŠ¤í† ë¦¬ ì €ì¥
        self.chat_history.append({
            "user": user_message,
            "assistant": response
        })

        return response




    def ask(self, question: str) -> str:
        '''ì§ˆë¬¸ì— ë‹µë³€ (ìµœì í™”: retrieval 1íšŒë§Œ ìˆ˜í–‰)'''
        # 1) ë²¡í„°DB ê²€ìƒ‰. retrieval ìˆ˜í–‰
        source_docs = self.retriever.invoke(question)
        context = self._format_docs(source_docs)
        
        # 2) context ê²°ì •
        if not source_docs:
            # --- ë‚´ë¶€ ë¬¸ì„œ ì—†ìŒ â†’ ì›¹ ê²€ìƒ‰ ---
            web_results = self._web_search(question)
            context = self._format_web_results(web_results)
        else:
            context = self._format_docs(source_docs)

        # 3) chain ì‹¤í–‰
        return self.chain.invoke({
            "question": question,
            "context": context,
            "chat_history": self._format_chat_history()
        })

    

    def ask_with_sources(self, question: str, stream: bool = False, score_threshold: float = 0.7):
        """ì§ˆë¬¸ì— ë‹µë³€ + ì¶œì²˜ ë°˜í™˜ (score í›„ì²˜ë¦¬ ë°©ì‹)"""
        # 1. similarity_search_with_score ì‚¬ìš©
        docs_and_scores = self.vectorstore.similarity_search_with_score(question, k=5)
        
        # 2. score ê¸°ì¤€ í•„í„°ë§
        relevant_docs = [doc for doc, score in docs_and_scores if score >= score_threshold]

        # 3. context ê²°ì •
        if not relevant_docs:
            context = "NO_RELEVANT_PAPERS"
            web_results = self._web_search(question)
            context = self._format_web_results(web_results)
            
            sources = [{
                "paper_name": r["title"],
                "huggingface_url": None,
                "github_url": None,
                "upvote": None,
                "url": r["url"]
            } for r in web_results]

        else:
            context = self._format_docs(relevant_docs)
            sources = []
            for doc in relevant_docs:
                md = doc.metadata or {}
                tags = [md.get("tag1"), md.get("tag2"), md.get("tag3")]
                tags = [t for t in tags if t]
                sources.append({
                    "paper_name": md.get("title", "(no title)"),
                    "huggingface_url": md.get("huggingface_url"),
                    "github_url": md.get("github_url"),
                    "upvote": md.get("upvote"),
                })


        # 4. chain ì‹¤í–‰ (ìŠ¤íŠ¸ë¦¬ë° or ì¼ë°˜)
        chain_input = {
            "question": question,
            "context": context,
            "chat_history": self._format_chat_history()
        }

        if stream:
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±ê¸° ë°˜í™˜
            return {
                "answer_stream": self.chain.stream(chain_input),
                "sources": sources,
            }
        else:
            # ì „ì²´ ë‹µë³€ ë°˜í™˜
            answer = self.chain.invoke(chain_input)
            return {
                "answer": answer,
                "sources": sources,
            }

    def clear_history(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.chat_history = []
   


if __name__ == '__main__':
    # chunk íŒŒì¼ë¡œ ì„ì‹œ í™•ì¸
    def get_project_root():
        curr = Path().resolve()
        for parent in [curr] + list(curr.parents):
            if (parent / ".git").exists():
                return parent
        raise FileNotFoundError("í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸° ì‹¤íŒ¨")

    MODEL_NAME = os.getenv("MODEL_NAME")
    CHUNK_SIZE = int(os.getenv("CHUNK_SIZE"))
    CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP"))
    
    vectorstore = load_vectordb(
            model_name=MODEL_NAME,
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP
    )
  
    llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)

    rag_system = SimpleRAGSystem(vectorstore, llm)
    user_question = "ë²¡í„°DBê°€ ë­ì•¼?"
    result = rag_system.ask_with_sources(user_question)

    print(f"ì§ˆë¬¸: {user_question}")
    print("\n[ë‹µë³€]\n")
    print(result["answer"])

    # print("\n[ì¶œì²˜]\n")
    # for i, src in enumerate(result["sources"], start=1):
    #     print(f"- [{i}] {src['paper_name']}")
    #     if src["huggingface_url"]:
    #         print(f"  HF: {src['huggingface_url']}")
    #     if src["github_url"]:
    #         print(f"  GitHub: {src['github_url']}")
    #     if src["upvote"] is not None:
    #         print(f"  upvote: {src['upvote']}")


    # --------------------------------------------------------
    # ğŸ”¥ ì—¬ê¸° ì•„ë˜ ì±—ë´‡ ëª¨ë“œ ì…ë ¥ ë£¨í”„ ë„£ìœ¼ë©´ ë¨!
    # --------------------------------------------------------

    print("\n=== AI Tech Trend Navigator Chatbot ===")
    print("ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ë˜ëŠ” 'quit' ì…ë ¥\n")

    while True:
        user_msg = input("You: ")

        if user_msg.lower() in ["exit", "quit"]:
            print("ì±—ë´‡ ì¢…ë£Œ!")
            break

        answer = rag_system.chat(user_msg)
        print(f"\nAssistant:\n{answer}\n")