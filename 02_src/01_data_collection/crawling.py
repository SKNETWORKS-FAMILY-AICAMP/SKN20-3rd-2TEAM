import os
import json
import re
import time
import requests
from typing import List, Dict

from bs4 import BeautifulSoup
from tqdm import tqdm

import nltk
# from nltk.corpus import stopwords
# from nltk.stem import WordNetLemmatizer
from keybert import KeyBERT

# NLTK ë¶ˆìš©ì–´ ë‹¤ìš´ë¡œë“œ
try:
    nltk.data.find("corpora/stopwords")
    nltk.data.find("corpora/wordnet")
except LookupError:
    print("[PREPARE] NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...")
    nltk.download("stopwords", quiet=True)
    nltk.download("wordnet", quiet=True)

# HTTP í—¤ë” ì„¤ì •
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0 Safari/537.36"
}

# ë¶ˆìš©ì–´ ë¡œë“œ
# ë„ë©”ì¸ íŠ¹í™” ë¶ˆìš©ì–´ ì¶”ê°€ (ì„ íƒì )
custom_stopwords = {
    "the", "a", "an", "and", "or", "but", "in", "on", "at", "to", "for",
    "of", "with", "by", "from", "as", "is", "was", "are", "been", "be",
    "have", "has", "had", "do", "does", "did", "will", "would", "could",
    "should", "may", "might", "can", "this", "that", "these", "those",
    "we", "our", "their", "they", "it", "its", "which", "who", "when",
    "where", "why", "how", "what", "if", "than", "such", "into", "through",
    # ë…¼ë¬¸ì—ì„œ ë„ˆë¬´ ë§ì´ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë“¤
    "paper", "propose", "present", "show", "demonstrate", "using", "used",
    "approach", "method", "model", "based", "results", "work",
    "task", "tasks", "result", "results", "data"
}


# KeyBERTë¥¼ ì‚¬ìš©í•˜ì—¬ keyword ì¶”ì¶œ
def extract_keywords(text: str, top_n: int = 3) -> List[str]:
    """
    KeyBERTë¥¼ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ

    Args:
        text: ë…¼ë¬¸ Abstract
        top_n: ì¶”ì¶œí•  í‚¤ì›Œë“œ ê°œìˆ˜

    Returns:
        í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    """
    if not text or len(text.split()) < 10:
        return [f"keyword{i+1}" for i in range(top_n)]

    try:
        # KeyBERT ëª¨ë¸ ì´ˆê¸°í™”
        kw_model = KeyBERT()

        # í‚¤ì›Œë“œ ì¶”ì¶œ (ë¶ˆìš©ì–´ ì œê±°)
        keywords = kw_model.extract_keywords(
            text,
            keyphrase_ngram_range=(1, 2),
            stop_words=list(custom_stopwords),
            top_n=top_n,
            use_maxsum=True,
        )

        # (keyword, score) íŠœí”Œì—ì„œ keywordë§Œ ì¶”ì¶œ
        result = [kw[0] for kw in keywords]

        # ë¶€ì¡±í•œ í‚¤ì›Œë“œëŠ” ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
        while len(result) < top_n:
            result.append(f"keyword{len(result)+1}")

        return result[:top_n]

    except Exception as e:
        print(f"âš ï¸  í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return [f"keyword{i+1}" for i in range(top_n)]


def get_with_retry(url: str, max_retries: int = 3):
    """
    ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ HTTP ìš”ì²­

    Args:
        url: ìš”ì²­ URL
        max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜

    Returns:
        requests.Response ë˜ëŠ” None
    """
    for attempt in range(max_retries):
        try:
            response = requests.get(url, headers=HEADERS, timeout=10)

            if response.status_code == 200:
                return response
            elif response.status_code == 429:
                print("[ERROR]  429 ì—ëŸ¬ (Too Many Requests), ëŒ€ê¸° ì¤‘...")
                time.sleep(5)

        except Exception as e:
            if attempt == max_retries - 1:
                print(f"[FATAL] ìš”ì²­ ì‹¤íŒ¨: {e}")

        time.sleep(2)

    return None


def fetch_weekly_papers(year: int, week: int) -> List[Dict[str, str]]:
    """
    HuggingFace DailyPapers Weekly í˜ì´ì§€ì—ì„œ ë…¼ë¬¸ ëª©ë¡ ì¶”ì¶œ

    Args:
        year (int): ì—°ë„
        week (int): ì£¼ì°¨ (1~52)

    Returns:
        List[Dict[str, str]]: ë…¼ë¬¸ URLê³¼ ì œëª© ë¦¬ìŠ¤íŠ¸
    """
    week_str = f"{year}-W{week:02d}"
    weekly_url = f"https://huggingface.co/papers/week/{week_str}"

    print(f"\n[FETCH] {week_str} ë…¼ë¬¸ ëª©ë¡ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")

    response = get_with_retry(weekly_url)
    if response is None:
        print(f"[FATAL] í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {weekly_url}")
        return []

    soup = BeautifulSoup(response.content, "html.parser")

    # ë…¼ë¬¸ ë§í¬ ì¶”ì¶œ (CSS Selector: a.line-clamp-3)
    paper_links = []
    for link in soup.select("a.line-clamp-3"):
        href = link.get("href")
        title = link.get_text(strip=True)

        if href:
            full_url = f"https://huggingface.co{href}"
            paper_links.append({"title": title, "url": full_url})

    print(f"[CHECK] ë…¼ë¬¸ {len(paper_links)}ê°œ ë°œê²¬")
    return paper_links


def fetch_paper_details(paper_url: str) -> Dict[str, any]:
    """
    ê°œë³„ ë…¼ë¬¸ ìƒì„¸ í˜ì´ì§€ì—ì„œ Abstract, Authors, GitHub URL, Upvote ì¶”ì¶œ

    Args:
        paper_url (str): _description_

    Returns:
        Dict: ë…¼ë¬¸ ìƒì„¸ ì •ë³´
            - context(abstract) (str): ë…¼ë¬¸ ì´ˆë¡ (ì—¬ëŸ¬ <p> íƒœê·¸ ê²°í•©)
            - github_url (str): GitHub ë ˆí¬ì§€í† ë¦¬ URL (ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´)
            - authors (List[str]): ì €ìë“¤
            - upvote (int): ì¶”ì²œ ìˆ˜ (ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ 0)
    """
    response = get_with_retry(paper_url)
    if response is None:
        return {"context": "", "authors": [], "github_url": "", "upvote": 0}

    soup = BeautifulSoup(response.content, "html.parser")

    # Abstract ì¶”ì¶œ (ì—¬ëŸ¬ <p> íƒœê·¸ë¥¼ ê²°í•©)
    abstract_section = soup.select_one("section div")
    abstract = ""
    if abstract_section:
        paragraphs = abstract_section.find_all("p")
        abstract = " ".join([p.get_text(strip=True) for p in paragraphs])

    # Authors ì¶”ì¶œ
    authors = []
    author_links = soup.select(
        "div.relative.flex.flex-wrap.items-center.gap-2.text-base.leading-tight a"
    )
    for link in author_links:
        author_name = link.get_text(strip=True)
        if author_name and "huggingface.co" not in author_name:
            authors.append(author_name)

    # GitHub URL ì¶”ì¶œ (ì„ íƒì )
    github_link = soup.select_one('a[href*="github.com"]')
    github_url = github_link["href"] if github_link else ""

    # Upvote ì¶”ì¶œ (ìˆ«ì í™•ì¸ í•„ìš”)
    upvote = 0
    upvote_elem = soup.select_one("div.font-semibold.text-orange-500")
    if upvote_elem:
        upvote_text = upvote_elem.get_text(strip=True)
        # ìˆ«ìë§Œ ì¶”ì¶œ (ì˜ˆ: "123" ë˜ëŠ” "123 upvotes")
        upvote_match = re.search(r"\d+", upvote_text)
        if upvote_match:
            upvote = int(upvote_match.group())

    return {
        "context": abstract,
        "authors": authors,
        "github_url": github_url,
        "upvote": upvote,
    }


def save_paper_json(paper_data: Dict, year: int, week: int, index: int) -> str:
    """
    ë…¼ë¬¸ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥

    Args:
        paper_data: ë…¼ë¬¸ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
            - title (str): ë…¼ë¬¸ ì œëª©
            - context (str): ë…¼ë¬¸ ì´ˆë¡
            - authors (List[str]): ì €ì List
            - github_url (str): GitHub URL
            - huggingface_url (str): HuggingFace ë…¼ë¬¸ URL
            - upvote (int): ì¶”ì²œ ìˆ˜
            - tags (List[str]): tag List
        year: ì—°ë„ (ì˜ˆ: 2025)
        week: ì£¼ì°¨ (1~52)
        index: ë…¼ë¬¸ ë²ˆí˜¸ (0ë¶€í„° ì‹œì‘)

    Returns:
        str: ì €ì¥ëœ íŒŒì¼ ID (ì˜ˆ: doc2545001)

    File Structure:
        01_data/documents/{year}/{year}-W{week}/doc{YY}{ww}{NNN}.json

    JSON Format:
        {
          "context": "Abstract í…ìŠ¤íŠ¸...",
          "metadata": {
            "title": "ë…¼ë¬¸ ì œëª©",
            "authors": ["ì €ì1", "ì €ì2", ...],
            "publiction_year": year (int)
            "github_url": "GitHub URL",
            "huggingface_url": "HuggingFace URL",
            "upvote": 123,
            "tags": ["keyword1", "keyword2", "keyword3"]
          }
        }
    """
    week_str = f"{year}-W{week:02d}"

    # íŒŒì¼ëª… ìƒì„±: doc{YY}{ww}{NNN}.json
    doc_id = f"doc{year % 100:02d}{week:02d}{index+1:03d}"
    filename = f"{doc_id}.json"

    # ë””ë ‰í† ë¦¬ ìƒì„±
    save_dir = f"01_data/documents/{year}/{week_str}"
    os.makedirs(save_dir, exist_ok=True)

    # JSON ë°ì´í„° êµ¬ì¡°
    document = {
        "context": paper_data["context"],
        "metadata": {
            "title": paper_data["title"],
            "authors": paper_data["authors"],
            "publication_year": year,
            "github_url": paper_data["github_url"],
            "huggingface_url": paper_data["huggingface_url"],
            "upvote": paper_data["upvote"],
            "tags": paper_data["tags"],
        },
    }

    # JSON íŒŒì¼ ì €ì¥
    file_path = os.path.join(save_dir, filename)
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(document, f, ensure_ascii=False, indent=2)

    return doc_id


def crawl_weekly_papers(year: int, week: int):
    """
    íŠ¹ì • ì£¼ì°¨ì˜ HuggingFace DailyPapers í¬ë¡¤ë§

    Args:
        year: ì—°ë„
        week: ì£¼ì°¨
    """
    week_str = f"{year}-W{week:02d}"
    print(f"\n{'='*60}")
    print(f"[START] {week_str} í¬ë¡¤ë§ ì‹œì‘")
    print(f"{'='*60}")

    # 1. Weekly í˜ì´ì§€ì—ì„œ ë…¼ë¬¸ ëª©ë¡ ì¶”ì¶œ
    papers = fetch_weekly_papers(year, week)

    if not papers:
        print(f"[WARNING]  {week_str}ì— ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 2. ê° ë…¼ë¬¸ ì²˜ë¦¬
    success_count = 0
    fail_count = 0

    for index, paper_info in enumerate(tqdm(papers, desc="[CRAWLING] ë…¼ë¬¸ ì²˜ë¦¬")):
        paper_url = paper_info["url"]
        paper_title = paper_info["title"]

        # ë…¼ë¬¸ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
        details = fetch_paper_details(paper_url)
        time.sleep(2)  # ì„œë²„ ë¶€í•˜ ë°©ì§€

        if not details["context"]:
            fail_count += 1
            continue

        # KeyBERTë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = extract_keywords(details["context"], top_n=3)

        # ë°ì´í„° ì €ì¥
        paper_data = {
            "title": paper_title,
            "context": details["context"],
            "authors": details["authors"],
            "github_url": details["github_url"],
            "huggingface_url": paper_url,
            "upvote": details["upvote"],
            "tags": keywords,
        }

        try:
            doc_id = save_paper_json(paper_data, year, week, index)
            success_count += 1
        except Exception as e:
            print(f"\n[FATAL] {doc_id} ì €ì¥ ì‹¤íŒ¨: {e}")
            fail_count += 1

        # Error 429 ë°©ì§€ - 40ê°œë§ˆë‹¤ íœ´ì‹
        if (index + 1) % 40 == 0 and index + 1 < len(papers):
            print(f"\nğŸ’¤ {index+1}ê°œ ì²˜ë¦¬ ì™„ë£Œ, 160ì´ˆ íœ´ì‹...")
            time.sleep(160)

    # 3. í†µê³„ ì¶œë ¥
    print(f"\n{'='*60}")
    print(f"[END] {week_str} í¬ë¡¤ë§ ì™„ë£Œ")
    print(f"   ì´ ë…¼ë¬¸: {len(papers)}ê°œ")
    print(f"   ì„±ê³µ: {success_count}ê°œ")
    print(f"   ì‹¤íŒ¨: {fail_count}ê°œ")
    print(f"{'='*60}")


if __name__ == "__main__":
    print("=== HuggingFace DailyPapers í¬ë¡¤ëŸ¬ ===\n")

    # í¬ë¡¤ë§ ì‹¤í–‰ ì˜ˆì‹œ (2025ë…„ 45~49ì£¼ì°¨)
    for week in range(45, 46):
        try:
            crawl_weekly_papers(year=2025, week=week)
        except Exception as e:
            print(f"\n[FATAL] W{week:02d} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

    print("\n[COMPLETE] ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ!")
