"""
HuggingFace DailyPapers í¬ë¡¤ë§ ë° í‚¤ì›Œë“œ ì¶”ì¶œ ëª¨ë“ˆ

ì£¼ìš” ê¸°ëŠ¥:
1. HuggingFace Weekly Papers í¬ë¡¤ë§
2. ë…¼ë¬¸ Abstractì—ì„œ ìë™ í‚¤ì›Œë“œ ì¶”ì¶œ (Lemmatization â†’ TF-IDF)
3. JSON í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ì €ì¥

í‚¤ì›Œë“œ ì¶”ì¶œ ì•Œê³ ë¦¬ì¦˜:
- Step 1: í† í°í™” (3ê¸€ì ì´ìƒ ì˜ë¬¸)
- Step 2: WordNet Lemmatization (ë‹¨ì–´ ê¸°ë³¸í˜• ë³€í™˜)
- Step 3: NLTK + ì»¤ìŠ¤í…€ ë¶ˆìš©ì–´ ì œê±°
- Step 4: TF-IDF ë²¡í„°í™” (unigram + bigram)
- Step 5: ì¤‘ë³µ í‚¤ì›Œë“œ í•„í„°ë§ (ê¸´ í‚¤ì›Œë“œì— í¬í•¨ëœ ì§§ì€ í‚¤ì›Œë“œ ì œê±°)
- Step 6: ìƒìœ„ 3ê°œ í‚¤ì›Œë“œ ë°˜í™˜

Version: 2.0
Author: SKN20-3rd-2TEAM
"""

import os
import json
import logging
import requests
from bs4 import BeautifulSoup
from typing import List, Dict
from datetime import datetime
import re
import time

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0 Safari/537.36"
    )
}

def get_with_retry(
    url: str,
    max_retries: int = 6,      
    initial_wait: float = 2.0, # ì²« ì‹¤íŒ¨ ì´í›„ ëŒ€ê¸°
    backoff: float = 6.0,
    timeout: float = 12.0,
):
    wait = initial_wait

    for attempt in range(1, max_retries + 1):
        logging.info(f"[HTTP] GET {url} (try {attempt}/{max_retries})")

        try:
            resp = requests.get(url, headers=HEADERS, timeout=timeout)
        except Exception as e:
            logging.warning(f"[HTTP] ì˜ˆì™¸, retry... {e}")
            time.sleep(wait)
            wait *= backoff
            continue

        # 429: ë„ˆë¬´ ìì£¼ ìš”ì²­í–ˆë‹¤ëŠ” ëœ» â†’ ì§§ê²Œ ëª‡ ë²ˆë§Œ ì¬ì‹œë„
        if resp.status_code == 429:
            ra = resp.headers.get("Retry-After")
            wait_time = float(ra) if ra else wait
            logging.warning(f"[429] Too Many Requests, sleep {wait_time:.1f}s í›„ ì¬ì‹œë„")
            time.sleep(wait_time)
            wait *= backoff
            continue

        # ì •ìƒ ì‘ë‹µ
        if 200 <= resp.status_code < 300:
            return resp

        # ê¸°íƒ€ ì—ëŸ¬ ì½”ë“œ
        logging.warning(f"[HTTP {resp.status_code}] retry..")
        time.sleep(wait)
        wait *= backoff

    logging.error(f"[FAIL] ìš”ì²­ ì‹¤íŒ¨(ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼): {url}")
    return None



# TF-IDF ë° NLTK
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# NLTK ë¶ˆìš©ì–´ ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ 1íšŒ)
try:
    nltk.data.find('corpora/stopwords')
    nltk.data.find('corpora/wordnet')
    nltk.data.find('corpora/omw-1.4')
except LookupError:
    nltk.download('stopwords', quiet=True)
    nltk.download('wordnet', quiet=True)
    nltk.download('omw-1.4', quiet=True)

# ì„¤ì •
TAG_COUNT = 3
CRAWLER_VERSION = "2.0"

def extract_keywords_tfidf_nltk(text: str, top_n: int = 3) -> List[str]:
    """
    Lemmatization â†’ TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ

    ì „ì²˜ë¦¬ í›„ TF-IDFë¥¼ ì ìš©í•˜ì—¬ ë” ì •í™•í•œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

    ì²˜ë¦¬ ìˆœì„œ:
        1. í† í°í™”: 3ê¸€ì ì´ìƒ ì˜ë¬¸ ë‹¨ì–´ë§Œ ì¶”ì¶œ
        2. Lemmatization: WordNetLemmatizerë¡œ ë‹¨ì–´ ê¸°ë³¸í˜• ë³€í™˜
           - "training" â†’ "train"
           - "models" â†’ "model"
        3. ë¶ˆìš©ì–´ ì œê±°: NLTK stopwords + ë…¼ë¬¸ ë„ë©”ì¸ íŠ¹í™” ë¶ˆìš©ì–´
        4. TF-IDF ë²¡í„°í™”: unigram(1-gram) + bigram(2-gram)
        5. í‚¤ì›Œë“œ ì¶”ì¶œ: TF-IDF ì ìˆ˜ ìƒìœ„ Nê°œ ì„ íƒ
        6. ì¤‘ë³µ í•„í„°ë§: ê¸´ í‚¤ì›Œë“œì— í¬í•¨ëœ ì§§ì€ í‚¤ì›Œë“œ ì œê±°
           - ["attention", "self attention"] â†’ ["self attention"]

    Args:
        text: ë…¼ë¬¸ Abstract í…ìŠ¤íŠ¸
        top_n: ì¶”ì¶œí•  í‚¤ì›Œë“œ ê°œìˆ˜ (ê¸°ë³¸ê°’: 3)

    Returns:
        List[str]: TF-IDF ì ìˆ˜ê°€ ë†’ì€ ìƒìœ„ Nê°œ í‚¤ì›Œë“œ (lemmatized & filtered)

    Examples:
        >>> abstract = "We trained multiple neural networks using transformers."
        >>> extract_keywords_tfidf_nltk(abstract, top_n=3)
        ['transformer', 'neural network', 'train']

    Note:
        - 10ë‹¨ì–´ ë¯¸ë§Œì˜ ì§§ì€ í…ìŠ¤íŠ¸ëŠ” ê¸°ë³¸ í‚¤ì›Œë“œ ë°˜í™˜
        - ì „ì²˜ë¦¬ í›„ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ í‚¤ì›Œë“œ ë°˜í™˜
    """
    # 1. í…ìŠ¤íŠ¸ ê²€ì¦
    if not text or len(text.split()) < 10:
        logging.warning("[WARNING] Abstractê°€ ë„ˆë¬´ ì§§ìŒ (< 10 words), ê¸°ë³¸ í‚¤ì›Œë“œ ë°˜í™˜")
        return [f"keyword{i+1}" for i in range(top_n)]

    try:
        lemmatizer = WordNetLemmatizer()

        # 2. NLTK ì˜ì–´ ë¶ˆìš©ì–´ ë¡œë“œ
        nltk_stopwords = set(stopwords.words('english'))

        # 3. ë„ë©”ì¸ íŠ¹í™” ë¶ˆìš©ì–´ ì¶”ê°€ (ì„ íƒì )
        custom_stopwords = {
            "the", "a", "an", "and", "or", "but", "in", "on", "at", "to", "for",
            "of", "with", "by", "from", "as", "is", "was", "are", "been", "be",
            "have", "has", "had", "do", "does", "did", "will", "would", "could",
            "should", "may", "might", "can", "this", "that", "these", "those",
            "we", "our", "their", "they", "it", "its", "which", "who", "when",
            "where", "why", "how", "what", "if", "than", "such", "into", "through",
            # ë…¼ë¬¸ì—ì„œ ë„ˆë¬´ ë§ì´ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë“¤
            "paper", "propose", "present", "show", "demonstrate", "using", "used",
            "approach", "method", "model", "based", "results", "work",
            "task", "tasks", "result", "results", "data"
        }
        all_stopwords = nltk_stopwords.union(custom_stopwords)

        # 4. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬: Lemmatization + NLTK ë¶ˆìš©ì–´ ì œê±°
        # 4-1. í† í°í™” (3ê¸€ì ì´ìƒ ì˜ë¬¸ë§Œ)
        tokens = re.findall(r'\b[a-zA-Z]{3,}\b', text.lower())

        # 4-2. Lemmatization ì ìš©
        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]

        # 4-3. ë¶ˆìš©ì–´ ì œê±°
        filtered_tokens = [token for token in lemmatized_tokens if token not in all_stopwords]

        # 4-4. ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ì¬êµ¬ì„±
        preprocessed_text = ' '.join(filtered_tokens)

        if not preprocessed_text.strip():
            logging.warning("[WARNING] ì „ì²˜ë¦¬ í›„ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŒ, ê¸°ë³¸ í‚¤ì›Œë“œ ë°˜í™˜")
            return [f"keyword{i+1}" for i in range(top_n)]

        logging.debug(f"[PREPROCESSED] Original length: {len(text.split())} â†’ Filtered: {len(filtered_tokens)}")

        # 5. ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ë¡œ TF-IDF Vectorizer ìƒì„±
        vectorizer = TfidfVectorizer(
            ngram_range=(1, 2),              # 1~2ê·¸ë¨ (unigram + bigram)
            max_features=1000,               # ì–´íœ˜ í¬ê¸° ì œí•œ
            lowercase=False,                 # ì´ë¯¸ ì†Œë¬¸ì ì²˜ë¦¬ë¨
            token_pattern=r'\b[a-z]{3,}\b'   # 3ê¸€ì ì´ìƒ ì˜ë¬¸ë§Œ
        )

        # 6. TF-IDF í–‰ë ¬ ìƒì„±
        tfidf_matrix = vectorizer.fit_transform([preprocessed_text])
        feature_names = vectorizer.get_feature_names_out()
        scores = tfidf_matrix.toarray()[0]

        # 7. TF-IDF ì ìˆ˜ë¡œ ì •ë ¬ (ë‚´ë¦¼ì°¨ìˆœ)
        idx_score_pairs = [(i, score) for i, score in enumerate(scores) if score > 0]

        if not idx_score_pairs:
            logging.warning("[WARNING] TF-IDF ì ìˆ˜ê°€ ëª¨ë‘ 0, ê¸°ë³¸ í‚¤ì›Œë“œ ë°˜í™˜")
            return [f"keyword{i+1}" for i in range(top_n)]

        idx_score_pairs.sort(key=lambda x: x[1], reverse=True)
        top_indices = [i for i, _ in idx_score_pairs[:top_n]]

        # 8. ìƒìœ„ Nê°œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì´ë¯¸ lemmatizationì´ ì ìš©ëœ ìƒíƒœ)
        keywords = [feature_names[i] for i in top_indices]

        logging.info(f"[KEYWORDS_RAW] {keywords} (lemmatized)")

        # 8-1. ì¤‘ë³µ ì œê±°: ë” ê¸´ í‚¤ì›Œë“œì— í¬í•¨ë˜ëŠ” ì§§ì€ í‚¤ì›Œë“œ ì œê±°
        # ì˜ˆ: ["attention", "self attention"] â†’ ["self attention"] (attention ì œê±°)
        filtered_keywords = []
        keywords_sorted = sorted(keywords, key=len, reverse=True)  # ê¸´ ê²ƒë¶€í„° ì •ë ¬

        for keyword in keywords_sorted:
            # í˜„ì¬ í‚¤ì›Œë“œê°€ ì´ë¯¸ ì„ íƒëœ ë” ê¸´ í‚¤ì›Œë“œì— í¬í•¨ë˜ëŠ”ì§€ í™•ì¸
            is_substring = False
            for selected in filtered_keywords:
                # ë‹¨ì–´ ê²½ê³„ ì²´í¬: "attention"ì´ "self attention"ì— ë‹¨ì–´ë¡œ í¬í•¨ë˜ëŠ”ì§€
                if keyword in selected.split():
                    is_substring = True
                    logging.debug(f"[REMOVED] '{keyword}' (í¬í•¨ë¨: '{selected}')")
                    break

            if not is_substring:
                filtered_keywords.append(keyword)

        # ì›ë˜ ìˆœì„œ(TF-IDF ì ìˆ˜ ìˆœ) ìœ ì§€
        keywords = [kw for kw in keywords if kw in filtered_keywords]

        logging.info(f"[KEYWORDS_FILTERED] {keywords} (ì¤‘ë³µ ì œê±° ì™„ë£Œ)")

        # 9. ë¶€ì¡±í•œ í‚¤ì›Œë“œëŠ” ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
        while len(keywords) < top_n:
            keywords.append(f"keyword{len(keywords)+1}")

        return keywords[:top_n]

    except Exception as e:
        logging.error(f"[ERROR] TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return [ f"keyword{i+1}" for i in range(top_n) ]


def fetch_weekly_papers(year: int, week: int) -> List[Dict[str, str]]:
    """
    HuggingFace DailyPapers Weekly í˜ì´ì§€ì—ì„œ ë…¼ë¬¸ ëª©ë¡ ì¶”ì¶œ

    Args:
        year: ì—°ë„ (ì˜ˆ: 2025)
        week: ì£¼ì°¨ (1~52)

    Returns:
        List[Dict]: ë…¼ë¬¸ URLê³¼ ì œëª© ë¦¬ìŠ¤íŠ¸
    """
    week_str = f"{year}-W{week:02d}"
    weekly_url = f"https://huggingface.co/papers/week/{week_str}"

    logging.info(f"[FETCH] Weekly í˜ì´ì§€ ìš”ì²­: {weekly_url}")

    try:
        response = get_with_retry(weekly_url, timeout=10)
        if response is None:
            logging.error(f"[ERROR] Weekly í˜ì´ì§€ ìš”ì²­ ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼: {weekly_url}")
            return []

        soup = BeautifulSoup(response.content, 'html.parser')

        # ë…¼ë¬¸ ë§í¬ ì¶”ì¶œ (CSS Selector: a.line-clamp-3)
        paper_links = []
        for link in soup.select('a.line-clamp-3'):
            href = link.get('href')
            title = link.get_text(strip=True)

            if href:
                full_url = f"https://huggingface.co{href}"
                paper_links.append({
                    'title': title
                    , 'url': full_url
                })

        logging.info(f"[SUCCESS] ë…¼ë¬¸ {len(paper_links)}ê°œ ë°œê²¬")
        return paper_links

    except requests.exceptions.RequestException as e:
        logging.error(f"[ERROR] Weekly í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {e}")
        return []


def fetch_paper_details(paper_url: str) -> Dict[str, any]:
    """
    ê°œë³„ ë…¼ë¬¸ ìƒì„¸ í˜ì´ì§€ì—ì„œ Abstract, GitHub URL, Upvote ì¶”ì¶œ

    Args:
        paper_url: ë…¼ë¬¸ ìƒì„¸ í˜ì´ì§€ URL (ì˜ˆ: https://huggingface.co/papers/2511.18538)

    Returns:
        Dict: ë…¼ë¬¸ ìƒì„¸ ì •ë³´
            - abstract (str): ë…¼ë¬¸ ì´ˆë¡ (ì—¬ëŸ¬ <p> íƒœê·¸ ê²°í•©)
            - github_url (str): GitHub ë ˆí¬ì§€í† ë¦¬ URL (ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´)
            - upvote (int): ì¶”ì²œ ìˆ˜ (ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ 0)

    Note:
        - ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ì‹œ ë¹ˆ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜ (ëª¨ë“  ê°’ ë¹ˆ ë¬¸ìì—´/0)
        - Upvote CSS SelectorëŠ” HuggingFace í˜ì´ì§€ êµ¬ì¡° ë³€ê²½ ì‹œ ì¡°ì • í•„ìš”
    """
    try:
        response = get_with_retry(paper_url, timeout=10)
        if response is None:
            logging.error(f"[ERROR] ë…¼ë¬¸ ìƒì„¸ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨(ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼): {paper_url}")
            return {"abstract": "", "github_url": "", "upvote": 0}


        soup = BeautifulSoup(response.content, 'html.parser')

        # Abstract ì¶”ì¶œ (ì—¬ëŸ¬ <p> íƒœê·¸ë¥¼ ê²°í•©)
        abstract_section = soup.select_one('section div')
        abstract = ""
        if abstract_section:
            paragraphs = abstract_section.find_all('p')
            abstract = ' '.join([ p.get_text(strip=True) for p in paragraphs ])

        # GitHub URL ì¶”ì¶œ (ì„ íƒì )
        github_link = soup.select_one('a[href*="github.com"]')
        github_url = github_link['href'] if github_link else ""

        # Upvote ì¶”ì¶œ (CSS SelectorëŠ” ì‹¤ì œ í˜ì´ì§€ êµ¬ì¡°ì— ë”°ë¼ ì¡°ì • í•„ìš”)
        upvote = 0
        upvote_elem = soup.select_one('div.font-semibold.text-orange-500')  # ì‹¤ì œ í´ë˜ìŠ¤ëª… í™•ì¸ í•„ìš”
        if upvote_elem:
            upvote_text = upvote_elem.get_text(strip=True)
            # ìˆ«ìë§Œ ì¶”ì¶œ (ì˜ˆ: "123" ë˜ëŠ” "123 upvotes")
            upvote_match = re.search(r'\d+', upvote_text)
            if upvote_match:
                upvote = int(upvote_match.group())

        return {
            'abstract': abstract
            , 'github_url': github_url
            , 'upvote': upvote
        }

    except requests.exceptions.RequestException as e:
        logging.error(f"[ERROR] ë…¼ë¬¸ ìƒì„¸ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {paper_url} - {e}")
        return {
            'abstract': ""
            , 'github_url': ""
            ,'upvote': 0
        }


def save_paper_json(paper_data: Dict, year: int, week: int, index: int) -> str:
    """
    ë…¼ë¬¸ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥

    Args:
        paper_data: ë…¼ë¬¸ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
            - title (str): ë…¼ë¬¸ ì œëª©
            - abstract (str): ë…¼ë¬¸ ì´ˆë¡
            - github_url (str): GitHub URL
            - huggingface_url (str): HuggingFace ë…¼ë¬¸ URL
            - upvote (int): ì¶”ì²œ ìˆ˜
            - tags (List[str]): í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (3ê°œ)
        year: ì—°ë„ (ì˜ˆ: 2025)
        week: ì£¼ì°¨ (1~52)
        index: ë…¼ë¬¸ ë²ˆí˜¸ (0ë¶€í„° ì‹œì‘)

    Returns:
        str: ì €ì¥ëœ íŒŒì¼ ID (ì˜ˆ: doc2545001)

    File Structure:
        01_data/documents/{year}/{year}-W{week}/doc{YY}{ww}{NNN}.json

    JSON Format:
        {
          "context": "Abstract í…ìŠ¤íŠ¸...",
          "metadata": {
            "paper_name": "ë…¼ë¬¸ ì œëª©",
            "github_url": "GitHub URL",
            "huggingface_url": "HuggingFace URL",
            "upvote": 123,
            "tags": ["keyword1", "keyword2", "keyword3"]
          }
        }
    """
    week_str = f"{year}-W{week:02d}"

    # íŒŒì¼ëª… ìƒì„±: doc{YY}{ww}{NNN}.json
    doc_id = f"doc{year % 100:02d}{week:02d}{index+1:03d}"
    filename = f"{doc_id}.json"

    # ë””ë ‰í† ë¦¬ ìƒì„±
    save_dir = f"01_data/documents/{year}/{week_str}"
    os.makedirs(save_dir, exist_ok=True)

    # JSON ë°ì´í„° êµ¬ì¡°
    document = {
        "context": paper_data['abstract']
        , "metadata": {
            "paper_name": paper_data['title']
            , "github_url": paper_data['github_url']
            , "huggingface_url": paper_data['huggingface_url']
            , "upvote": paper_data['upvote']
            , "tags": paper_data['tags']
        }
    }

    # JSON íŒŒì¼ ì €ì¥
    file_path = os.path.join(save_dir, filename)
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(document, f, ensure_ascii=False, indent=2)

    logging.info(f"[SAVED] {filename}")
    return doc_id


def crawl_weekly_papers(year: int, week: int):
    """
    íŠ¹ì • ì£¼ì°¨ì˜ HuggingFace DailyPapers í¬ë¡¤ë§ ë©”ì¸ í•¨ìˆ˜

    ì „ì²´ í¬ë¡¤ë§ íŒŒì´í”„ë¼ì¸:
        1. Weekly í˜ì´ì§€ì—ì„œ ë…¼ë¬¸ ëª©ë¡ ì¶”ì¶œ
        2. ê° ë…¼ë¬¸ ìƒì„¸ í˜ì´ì§€ì—ì„œ Abstract, GitHub URL, Upvote ìˆ˜ì§‘
        3. Abstractì—ì„œ Lemmatization â†’ TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        4. JSON íŒŒì¼ë¡œ ì €ì¥ (01_data/documents/{year}/{year}-W{week}/)

    Args:
        year: ì—°ë„ (ì˜ˆ: 2025)
        week: ì£¼ì°¨ (1~52)

    Returns:
        None (ì„±ê³µ/ì‹¤íŒ¨ í†µê³„ëŠ” ë¡œê·¸ë¡œ ì¶œë ¥)

    Logs:
        - [START]: í¬ë¡¤ë§ ì‹œì‘
        - [FETCH]: Weekly í˜ì´ì§€ ìš”ì²­
        - [KEYWORDS]: ì¶”ì¶œëœ í‚¤ì›Œë“œ
        - [SAVED]: JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ
        - [DONE]: í¬ë¡¤ë§ ì™„ë£Œ í†µê³„ (ì´ ë…¼ë¬¸ ìˆ˜, ì„±ê³µ, ì‹¤íŒ¨)

    Note:
        - Abstractê°€ ì—†ëŠ” ë…¼ë¬¸ì€ ìë™ ìŠ¤í‚µ
        - ì €ì¥ ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ë¡œê·¸ ê¸°ë¡ í›„ ë‹¤ìŒ ë…¼ë¬¸ìœ¼ë¡œ ì§„í–‰
    """
    week_str = f"{year}-W{week:02d}"
    logging.info(f"\n{'='*60}")
    logging.info(f"[START] {week_str} í¬ë¡¤ë§ ì‹œì‘")
    logging.info(f"{'='*60}")

    # 1. Weekly í˜ì´ì§€ì—ì„œ ë…¼ë¬¸ ëª©ë¡ ì¶”ì¶œ
    papers = fetch_weekly_papers(year, week)

    if not papers:
        logging.warning(f"[WARNING] {week_str}ì— ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 2. ê° ë…¼ë¬¸ ì²˜ë¦¬
    success_count = 0
    fail_count = 0

    for index, paper_info in enumerate(papers):

        # ğŸ”¥ ì—¬ê¸°ì— ìˆì–´ì•¼ í•¨! (ë°”ê¹¥ ë£¨í”„ ì•ˆ / ë‚´ë¶€ forë¬¸ ì—†ìŒ)
        if index > 0 and index % 40 == 0:
            logging.info(f"[COOLDOWN] {index}ê°œ ì²˜ë¦¬ ì™„ë£Œ â†’ 160ì´ˆ íœ´ì‹")
            time.sleep(160)

        paper_url = paper_info['url']
        paper_title = paper_info['title']

        logging.info(f"\n[{index+1}/{len(papers)}] ì²˜ë¦¬ ì¤‘: {paper_title}")

        # 2-1. ë…¼ë¬¸ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
        details = fetch_paper_details(paper_url)
        time.sleep(2.0)

        if not details['abstract']:
            logging.warning(f"  [SKIP] Abstract ì—†ìŒ: {paper_title}")
            fail_count += 1
            continue

        # 2-2. TF-IDF + NLTKë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = extract_keywords_tfidf_nltk(details['abstract'], top_n=TAG_COUNT)
        logging.info(f"  [KEYWORDS] {keywords}")

        # 2-3. ë°ì´í„° ì €ì¥
        paper_data = {
            'title': paper_title
            , 'abstract': details['abstract']
            , 'github_url': details['github_url']
            , 'huggingface_url': paper_url
            , 'upvote': details['upvote']
            ,'tags': keywords
        }

        try:
            doc_id = save_paper_json(paper_data, year, week, index)
            success_count += 1
            logging.info(f"  [SUCCESS] {doc_id} ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            logging.error(f"  [ERROR] ì €ì¥ ì‹¤íŒ¨: {e}")
            fail_count += 1

    # 3. í†µê³„ ì¶œë ¥
    logging.info(f"\n{'='*60}")
    logging.info(f"[DONE] {week_str} í¬ë¡¤ë§ ì™„ë£Œ")
    logging.info(f"  - ì´ ë…¼ë¬¸ ìˆ˜: {len(papers)}")
    logging.info(f"  - ì„±ê³µ: {success_count}")
    logging.info(f"  - ì‹¤íŒ¨: {fail_count}")
    logging.info(f"{'='*60}")

if __name__ == "__main__":
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ë¡œê¹… ì„¤ì •
    log_dir = "././01_data/logs"
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [%(levelname)s] %(message)s',
        handlers=[
            # ë¡œê·¸ íŒŒì¼ ì´ë¦„ì€ datetime ì‚¬ìš©
            logging.FileHandler(f"{log_dir}/crawling_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log", encoding='utf-8'),
            logging.StreamHandler()
        ]
    )

    # í¬ë¡¤ë§ ì‹¤í–‰ (ì˜ˆì‹œ: 2025ë…„ 48ì£¼ì°¨)
    try:
        crawl_weekly_papers(year=2025, week=45)
    except Exception as e:
        logging.error(f"[ERROR] W{45:02d} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

    # ìµœì‹  ë°ì´í„° í¬ë¡¤ë§ ì‹¤í–‰
    # ë°°ì¹˜ ëŒ ë•Œ ì‚¬ìš©
    # try:
    #     current_date = datetime.now()
    #     current_year, current_week, _ = current_date.isocalendar()
    #     crawl_weekly_papers(year=current_year, week=current_week)
    # except Exception as e:
    #     logging.error(f"[ERROR] ìµœì‹  ë°ì´í„° í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")