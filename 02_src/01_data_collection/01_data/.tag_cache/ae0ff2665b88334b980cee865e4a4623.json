{
  "title": "INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization\n  Formats",
  "abstract": "A comprehensive comparison of low-precision floating-point and integer quantization in large language models reveals that fine-grained integer formats, especially MXINT8, offer superior accuracy and e",
  "tags": [
    "Efficient AI",
    "Model Architecture",
    "Training Methods"
  ],
  "timestamp": 1765199761.117727,
  "model": "gpt-3.5-turbo"
}