"""
HuggingFace Weekly Papers Crawler (Selenium ë²„ì „)

í¬ë¡¤ë§ íŒŒì´í”„ë¼ì¸:
1. https://huggingface.co/papers/week/2025-W46 í˜ì´ì§€ ì ‘ì†
2. í˜ì´ì§€ì—ì„œ ë…¼ë¬¸ ëª©ë¡ ë¡œë“œ (ë™ì  ë Œë”ë§ ëŒ€ê¸°)
3. ê° ë…¼ë¬¸ ì¹´ë“œ í´ë¦­í•˜ì—¬ ìƒì„¸ í˜ì´ì§€ ì ‘ì†
4. Abstract, ì œëª©, GitHub URL, Upvote ì¶”ì¶œ
5. Abstractì—ì„œ í‚¤ì›Œë“œ 3ê°œ ì¶”ì¶œ
6. doc{YY}{ww}{NNN}.json í˜•ì‹ìœ¼ë¡œ ê°œë³„ ì €ì¥
"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import json
from pathlib import Path
from typing import List, Dict
import re
from collections import Counter
import time


class HFWeeklyCrawler:
    """HuggingFace Weekly Papers í¬ë¡¤ëŸ¬ (Selenium)"""
    
    def __init__(self, base_dir: str = "SKN20-3rd-2TEAM/01_data/documents", headless: bool = True):
        """
        Args:
            base_dir: JSON íŒŒì¼ì„ ì €ì¥í•  ìµœìƒìœ„ ë””ë ‰í† ë¦¬
            headless: Trueë©´ ë¸Œë¼ìš°ì € ì°½ì„ í‘œì‹œí•˜ì§€ ì•ŠìŒ (ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰)
        """
        self.base_dir = Path(base_dir)
        self.headless = headless
        self.driver = None
        
    def init_driver(self):
        """Selenium WebDriver ì´ˆê¸°í™”"""
        options = webdriver.ChromeOptions()
        
        if self.headless:
            options.add_argument('--headless')
        
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920,1080')
        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        self.driver = webdriver.Chrome(options=options)
        self.wait = WebDriverWait(self.driver, 10)
        print("âœ… Selenium WebDriver ì´ˆê¸°í™” ì™„ë£Œ")
    
    def close_driver(self):
        """WebDriver ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            print("âœ… WebDriver ì¢…ë£Œ")
    
    def parse_week_url(self, url: str) -> tuple:
        """URLì—ì„œ ì—°ë„ì™€ ì£¼ì°¨ ì¶”ì¶œ
        
        Args:
            url: https://huggingface.co/papers/week/2025-W46 í˜•ì‹
            
        Returns:
            (year, week): (2025, 46)
        """
        pattern = r'/week/(\d{4})-W(\d{2})'
        match = re.search(pattern, url)
        if not match:
            raise ValueError(f"ì˜ëª»ëœ URL í˜•ì‹ì…ë‹ˆë‹¤: {url}\nì˜¬ë°”ë¥¸ í˜•ì‹: https://huggingface.co/papers/week/YYYY-WNN")
        
        year = int(match.group(1))
        week = int(match.group(2))
        return year, week
    
    def extract_keywords(self, abstract: str, top_n: int = 3) -> List[str]:
        """Abstractì—ì„œ ìƒìœ„ í‚¤ì›Œë“œ 3ê°œ ì¶”ì¶œ (TF ê¸°ë°˜)
        
        Args:
            abstract: ë…¼ë¬¸ ì´ˆë¡
            top_n: ì¶”ì¶œí•  í‚¤ì›Œë“œ ê°œìˆ˜ (ê¸°ë³¸ê°’: 3)
            
        Returns:
            ìƒìœ„ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        if not abstract:
            return []
        
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'been', 'be',
            'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',
            'should', 'may', 'might', 'can', 'this', 'that', 'these', 'those',
            'we', 'our', 'their', 'they', 'it', 'its', 'which', 'who', 'when',
            'where', 'why', 'how', 'what', 'if', 'than', 'such', 'into', 'through',
            'paper', 'propose', 'present', 'show', 'demonstrate', 'using', 'used',
            'approach', 'method', 'model', 'based', 'results', 'work'
        }
        
        # ë‹¨ì–´ ì¶”ì¶œ ë° í•„í„°ë§
        words = re.findall(r'\b[a-z]{3,}\b', abstract.lower())
        filtered_words = [w for w in words if w not in stopwords]
        
        # ë¹ˆë„ ê³„ì‚° ë° ìƒìœ„ Nê°œ ì¶”ì¶œ
        word_freq = Counter(filtered_words)
        keywords = [word for word, _ in word_freq.most_common(top_n)]
        
        return keywords
    
    def get_paper_urls(self, week_url: str) -> List[str]:
        """ì£¼ì°¨ í˜ì´ì§€ì—ì„œ ëª¨ë“  ë…¼ë¬¸ URL ì¶”ì¶œ
        
        Args:
            week_url: https://huggingface.co/papers/week/2025-W46
            
        Returns:
            ë…¼ë¬¸ ìƒì„¸ í˜ì´ì§€ URL ë¦¬ìŠ¤íŠ¸
        """
        print(f"ğŸ”„ í˜ì´ì§€ ë¡œë”© ì¤‘: {week_url}")
        self.driver.get(week_url)
        
        # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        time.sleep(3)
        
        # í˜ì´ì§€ë¥¼ ìŠ¤í¬ë¡¤í•˜ë©´ì„œ ëª¨ë“  ë…¼ë¬¸ ì¹´ë“œ ë¡œë“œ
        last_height = self.driver.execute_script("return document.body.scrollHeight")
        while True:
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            new_height = self.driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                break
            last_height = new_height
        
        # ë…¼ë¬¸ ë§í¬ ì¶”ì¶œ (community ì œì™¸)
        paper_urls = []
        
        try:
            # ë°©ë²• 1: article íƒœê·¸ ë‚´ì˜ ë§í¬
            articles = self.driver.find_elements(By.CSS_SELECTOR, "article a[href*='/papers/']")
            for article in articles:
                href = article.get_attribute('href')
                if href and '/papers/' in href and href not in paper_urls:
                    # /papers/week/, #community ê²½ë¡œ ì œì™¸
                    if '/papers/week/' not in href and '#community' not in href:
                        paper_urls.append(href)
        except NoSuchElementException:
            pass
        
        try:
            # ë°©ë²• 2: ì§ì ‘ /papers/ ë§í¬ ì°¾ê¸°
            links = self.driver.find_elements(By.CSS_SELECTOR, "a[href*='/papers/2']")
            for link in links:
                href = link.get_attribute('href')
                if href and href not in paper_urls:
                    # /papers/week/, #community ì œì™¸, ë…¼ë¬¸ ID í˜•ì‹ í™•ì¸
                    if '/papers/week/' not in href and '#community' not in href and re.search(r'/papers/\d{4}\.\d+', href):
                        paper_urls.append(href)
        except NoSuchElementException:
            pass
        
        # ì¤‘ë³µ ì œê±° ë° #communityê°€ í¬í•¨ëœ URL í•œë²ˆ ë” í•„í„°ë§
        paper_urls = [url for url in dict.fromkeys(paper_urls) if '#community' not in url]
        
        print(f"âœ… {len(paper_urls)}ê°œ ë…¼ë¬¸ URL ë°œê²¬")
        return paper_urls
    
    def extract_paper_info(self, paper_url: str) -> Dict:
        """ë…¼ë¬¸ ìƒì„¸ í˜ì´ì§€ì—ì„œ ì •ë³´ ì¶”ì¶œ
        
        Args:
            paper_url: ë…¼ë¬¸ ìƒì„¸ í˜ì´ì§€ URL
            
        Returns:
            {
                "context": "Abstract",
                "metadata": {
                    "paper_name": "ì œëª©",
                    "github_url": "...",
                    "huggingface_url": "...",
                    "upvote": 123,
                    "tags": ["k1", "k2", "k3"]
                }
            }
        """
        self.driver.get(paper_url)
        time.sleep(2)
        
        paper_data = {
            "context": "",
            "metadata": {
                "paper_name": "",
                "github_url": "",
                "huggingface_url": paper_url,
                "upvote": 0,
                "tags": []
            }
        }
        
        try:
            # ì œëª© ì¶”ì¶œ
            try:
                title_elem = self.wait.until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "h1"))
                )
                paper_data["metadata"]["paper_name"] = title_elem.text.strip()
            except TimeoutException:
                paper_data["metadata"]["paper_name"] = "Unknown Title"
            
            # Abstract ì¶”ì¶œ (ì—¬ëŸ¬ ì„ íƒì ì‹œë„)
            abstract = ""
            selectors = [
                # HuggingFace Papers í˜ì´ì§€ì˜ ì¼ë°˜ì ì¸ êµ¬ì¡°
                "div.pb-8.pr-4.md\\:pr-16",  # ë©”ì¸ ì»¨í…ì¸  ì˜ì—­
                "div[class*='prose']",
                "div.prose",
                "article div",
                "main div.text-lg",
                "div[class*='abstract']",
                "div[class*='Abstract']",
                "section[class*='abstract']",
                "p[class*='abstract']"
            ]
            
            for selector in selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    for elem in elements:
                        text = elem.text.strip()
                        # AbstractëŠ” ë³´í†µ ê¸¸ì´ê°€ 100ì ì´ìƒ
                        if text and len(text) > 100:
                            # "Abstract" ë‹¨ì–´ê°€ í¬í•¨ë˜ì–´ ìˆê±°ë‚˜ ì¶©ë¶„íˆ ê¸´ í…ìŠ¤íŠ¸
                            if 'abstract' in text.lower()[:100] or len(text) > 200:
                                abstract = text
                                # "Abstract" í—¤ë” ì œê±°
                                abstract = re.sub(r'^abstract\s*:?\s*', '', abstract, flags=re.IGNORECASE)
                                break
                    if abstract:
                        break
                except NoSuchElementException:
                    continue
            
            if not abstract:
                print(f"    âš ï¸  Abstractë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ (í˜ì´ì§€ êµ¬ì¡° í™•ì¸ í•„ìš”)")
            else:
                print(f"    ğŸ“„ Abstract ê¸¸ì´: {len(abstract)} ì")
            
            paper_data["context"] = abstract
            
            # GitHub URL ì¶”ì¶œ (ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´)
            github_url = ""
            try:
                github_links = self.driver.find_elements(By.CSS_SELECTOR, "a[href*='github.com']")
                if github_links:
                    github_url = github_links[0].get_attribute('href')
            except NoSuchElementException:
                pass
            
            paper_data["metadata"]["github_url"] = github_url
            
            # Upvote ì¶”ì¶œ
            upvote = 0
            try:
                # ì •í™•í•œ ì„ íƒì: <div class="font-semibold text-orange-500">117</div>
                upvote_elem = self.driver.find_element(By.CSS_SELECTOR, "body > div > main > div > section.pt-8.border-gray-100.md\:col-span-5.pt-6.lg\:pt-28.pb-24.md\:pl-6.md\:border-l > div.hidden.flex-wrap.items-start.gap-2.md\:flex > div > div > a > div > div")
                upvote_text = upvote_elem.text.strip()
                # ìˆ«ìë§Œ ì¶”ì¶œ
                numbers = re.findall(r'\d+', upvote_text)
                if numbers:
                    upvote = int(numbers[0])
                    print(f"    â­ Upvote: {upvote}")
            except (NoSuchElementException, ValueError) as e:
                print(f"    âš ï¸  Upvoteë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
            paper_data["metadata"]["upvote"] = upvote
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            if abstract:
                paper_data["metadata"]["tags"] = self.extract_keywords(abstract, top_n=3)
            
        except Exception as e:
            print(f"    âš ï¸  ì •ë³´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return paper_data
    
    def save_individual_papers(self, papers_data: List[Dict], year: int, week: int):
        """ê° ë…¼ë¬¸ì„ ê°œë³„ JSON íŒŒì¼ë¡œ ì €ì¥
        
        íŒŒì¼ í˜•ì‹: doc{YY}{ww}{NNN}.json
        ì˜ˆ: doc2546001.json (2025ë…„ 46ì£¼ì°¨ 1ë²ˆ)
        
        Args:
            papers_data: ë³€í™˜ëœ ë…¼ë¬¸ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            year: ì—°ë„
            week: ì£¼ì°¨
        """
        # ì¶œë ¥ ë””ë ‰í† ë¦¬: SKN20-3rd-2TEAM/01_data/documents/2025/2025-W46
        year_dir = self.base_dir / str(year)
        week_dir = year_dir / f"{year}-W{week:02d}"
        week_dir.mkdir(parents=True, exist_ok=True)
        
        # YY: ì—°ë„ ë§ˆì§€ë§‰ 2ìë¦¬ (2025 -> 25)
        yy = str(year)[-2:]
        
        print(f"\nğŸ’¾ JSON íŒŒì¼ ì €ì¥ ì¤‘...")
        for idx, paper in enumerate(papers_data, 1):
            # íŒŒì¼ëª…: doc{YY}{ww}{NNN}.json
            filename = f"doc{yy}{week:02d}{idx:03d}.json"
            filepath = week_dir / filename
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(paper, f, indent=2, ensure_ascii=False)
            
            title = paper['metadata']['paper_name'][:50]
            print(f"  âœ… [{idx}/{len(papers_data)}] {filename} - {title}...")
        
        print(f"\nâœ… ì´ {len(papers_data)}ê°œ ë…¼ë¬¸ ì €ì¥ ì™„ë£Œ: {week_dir}")
    
    def crawl_week(self, week_url: str):
        """ì£¼ì°¨ URLì˜ ë…¼ë¬¸ì„ í¬ë¡¤ë§í•˜ê³  JSON íŒŒì¼ë¡œ ì €ì¥
        
        ì „ì²´ íŒŒì´í”„ë¼ì¸:
        1. URLì—ì„œ year, week ì¶”ì¶œ
        2. ì£¼ì°¨ í˜ì´ì§€ì—ì„œ ëª¨ë“  ë…¼ë¬¸ URL ìˆ˜ì§‘
        3. ê° ë…¼ë¬¸ í˜ì´ì§€ë¥¼ ë°©ë¬¸í•˜ì—¬ ì •ë³´ ì¶”ì¶œ
        4. doc{YY}{ww}{NNN}.json í˜•ì‹ìœ¼ë¡œ ì €ì¥
        
        Args:
            week_url: HuggingFace weekly papers URL
                     ì˜ˆ: https://huggingface.co/papers/week/2025-W46
        """
        print("\n" + "="*70)
        print("ğŸ“š HuggingFace Weekly Papers Crawler (Selenium)")
        print("="*70)
        
        # STEP 1: URL íŒŒì‹±
        print(f"\nğŸ”— ì…ë ¥ URL: {week_url}")
        try:
            year, week = self.parse_week_url(week_url)
            print(f"ğŸ“… í¬ë¡¤ë§ ëŒ€ìƒ: {year}ë…„ {week}ì£¼ì°¨ ({year}-W{week:02d})")
        except ValueError as e:
            print(f"âŒ {e}")
            return
        
        # WebDriver ì´ˆê¸°í™”
        self.init_driver()
        
        try:
            # STEP 2: ë…¼ë¬¸ URL ëª©ë¡ ìˆ˜ì§‘
            print(f"\n{'â”€'*70}")
            print("STEP 1: ë…¼ë¬¸ URL ëª©ë¡ ìˆ˜ì§‘")
            print(f"{'â”€'*70}")
            paper_urls = self.get_paper_urls(week_url)
            
            if not paper_urls:
                print("âš ï¸  ë…¼ë¬¸ URLì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # STEP 3: ê° ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ
            print(f"\n{'â”€'*70}")
            print("STEP 2: ê° ë…¼ë¬¸ ì •ë³´ í¬ë¡¤ë§")
            print(f"{'â”€'*70}")
            papers_data = []
            
            for i, url in enumerate(paper_urls, 1):
                print(f"\n  [{i}/{len(paper_urls)}] í¬ë¡¤ë§ ì¤‘: {url}")
                try:
                    paper_info = self.extract_paper_info(url)
                    
                    # Abstract ìœ ë¬´ì™€ ê´€ê³„ì—†ì´ ëª¨ë‘ ì €ì¥
                    papers_data.append(paper_info)
                    title = paper_info['metadata']['paper_name'][:50]
                    tags = ', '.join(paper_info['metadata']['tags']) if paper_info['metadata']['tags'] else 'ì—†ìŒ'
                    print(f"    âœ… {title}... (íƒœê·¸: {tags})")
                    
                    time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
                    
                except Exception as e:
                    print(f"    âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    continue
            
            if not papers_data:
                print("âŒ ì¶”ì¶œëœ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # STEP 4: íŒŒì¼ ì €ì¥
            print(f"\n{'â”€'*70}")
            print("STEP 3: JSON íŒŒì¼ ì €ì¥")
            print(f"{'â”€'*70}")
            self.save_individual_papers(papers_data, year, week)
            
            # ìš”ì•½ í†µê³„
            print(f"\n{'='*70}")
            print("ğŸ“Š í¬ë¡¤ë§ ì™„ë£Œ ìš”ì•½")
            print(f"{'='*70}")
            print(f"  ğŸ“ ì €ì¥ ê²½ë¡œ: {self.base_dir / str(year) / f'{year}-W{week:02d}'}")
            print(f"  ğŸ“„ ì´ ë…¼ë¬¸ ìˆ˜: {len(papers_data)}")
            print(f"  â­ í‰ê·  Upvote: {sum(p['metadata']['upvote'] for p in papers_data) / len(papers_data):.1f}")
            print(f"  ğŸ”— GitHub URL í¬í•¨: {sum(1 for p in papers_data if p['metadata']['github_url'])}")
            print(f"  ğŸ“ íŒŒì¼ í˜•ì‹: doc{str(year)[-2:]}{week:02d}001.json ~ doc{str(year)[-2:]}{week:02d}{len(papers_data):03d}.json")
            print("="*70 + "\n")
            
        finally:
            # WebDriver ì¢…ë£Œ
            self.close_driver()


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” (headless=Falseë¡œ ì„¤ì •í•˜ë©´ ë¸Œë¼ìš°ì € ì°½ì´ ë³´ì„)
    crawler = HFWeeklyCrawler(
        base_dir="01_data/documents/2025/2025-W46",
        headless=True  # Falseë¡œ ë³€ê²½í•˜ë©´ ë¸Œë¼ìš°ì € ì°½ì´ í‘œì‹œë¨
    )
    
    # ë‹¨ì¼ ì£¼ì°¨ í¬ë¡¤ë§
    crawler.crawl_week("https://huggingface.co/papers/week/2025-W46")
    
    # ì—¬ëŸ¬ ì£¼ì°¨ë¥¼ í¬ë¡¤ë§í•˜ë ¤ë©´:
    # urls = [
    #     "https://huggingface.co/papers/week/2025-W44",
    #     "https://huggingface.co/papers/week/2025-W45",
    #     "https://huggingface.co/papers/week/2025-W46",
    # ]
    # for url in urls:
    #     crawler.crawl_week(url)
    #     time.sleep(2)  # ì„œë²„ ë¶€í•˜ ë°©ì§€


if __name__ == "__main__":
    main()