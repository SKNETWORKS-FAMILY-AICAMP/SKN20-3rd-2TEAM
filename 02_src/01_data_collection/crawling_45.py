import os
import time
import random
import re
import json
from collections import Counter
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
import logging
from datetime import datetime


# ====== NLTK ë¶ˆìš©ì–´ ì ìš© ======
import nltk
from nltk.corpus import stopwords as nltk_stopwords
nltk.download('stopwords')

# ê¸°ë³¸ ì˜ì–´ ë¶ˆìš©ì–´
stopwords = set(nltk_stopwords.words("english"))

# ë…¼ë¬¸/ë…¼ë¬¸ì‚¬ì´íŠ¸ íŠ¹í™” ë¶ˆìš©ì–´ ì¶”ê°€
extra_stopwords = {
    "introduction", "method", "result", "figure", "table",
    "dataset", "experiment", "paper", "approach", "related", "work"
}
stopwords.update(extra_stopwords)

# ====== ì„¤ì • ======
base_year = 2025
start_week = 45
wait_time = 7
max_retry_per_article = 4
retry_click = 6

# ====== ë¡œê¹… ì„¤ì • ======
# ====== ë¡œê¹… íŒŒì¼ ì´ë¦„ ìƒì„± ======
current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
log_week_str = f"{base_year}-W{start_week:02d}"
log_file = f"crawling_{log_week_str}_{current_time}.log"

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_file, mode='w', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logging.info(f"ðŸš€ í¬ë¡¤ë§ ì‹œìž‘ â€” ë¡œê·¸íŒŒì¼: {log_file}")

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_file, mode='w', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logging.info("ðŸš€ í¬ë¡¤ë§ ì‹œìž‘")

# ====== ì›¹ ë“œë¼ì´ë²„ ì‹¤í–‰ ======
options = webdriver.ChromeOptions()
# options.add_argument("--headless=new")
options.add_argument("--disable-gpu")
options.add_argument("user-agent=Mozilla/5.0")
driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

# ====== ì´ˆê¸° ì£¼ì°¨ URL ======
week = start_week
week_url = f"https://huggingface.co/papers/week/{base_year}-W{week:02d}"
file_index = int(str(base_year)[-2:] + f"{week:02d}" + "001")

# ====== í†µê³„ ======
total_articles = 0
success_count = 0
fail_count = 0

# ====== í¬ë¡¤ë§ ë£¨í”„ ======
while True:
    logging.info(f"ðŸ”¹ Crawling week URL: {week_url}")
    folder = f"{base_year}-W{week:02d}"
    os.makedirs(folder, exist_ok=True)
    time.sleep(random.uniform(3, 6))

    # ì•„í‹°í´ ë§í¬ ì¶”ì¶œ
    try:
        driver.get(week_url)
        WebDriverWait(driver, wait_time).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "article h3 a"))
        )
        articles = driver.find_elements(By.CSS_SELECTOR, "article h3 a")
        article_urls = [a.get_attribute("href") for a in articles]
        logging.info(f"ðŸ“ {len(article_urls)} articles found")
    except Exception as e:
        logging.error(f"âŒ No articles found or page error: {e}")
        break

    total_articles += len(article_urls)

    # ê° ì•„í‹°í´ í¬ë¡¤ë§
    for link in article_urls:
        article_success = False
        for attempt in range(1, max_retry_per_article + 1):
            try:
                driver.get(link)
                time.sleep(random.uniform(3, 6))

                # ì œëª©
                try:
                    paper_name = WebDriverWait(driver, wait_time).until(
                        EC.presence_of_element_located((By.TAG_NAME, "h1"))
                    ).text.strip()
                except:
                    paper_name = "Unknown_Title"

                # Abstract
                try:
                    abstract_div = WebDriverWait(driver, wait_time).until(
                        EC.presence_of_element_located(
                            (By.CSS_SELECTOR, "div.pb-8.pr-4.md\\:pr-16 > div")
                        )
                    )
                    ps = abstract_div.find_elements(By.TAG_NAME, "p")
                    page_content = "\n".join([p.text.strip() for p in ps]) if ps else abstract_div.text.strip()
                except:
                    page_content = ""

                # Upvote
                try:
                    upvote_elem = WebDriverWait(driver, wait_time).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR,
                            "section.pt-8 div.hidden.flex-wrap.items-start.gap-2.md\\:flex a div div"
                        ))
                    )
                    upvote_match = re.search(r"\d+", upvote_elem.text.strip())
                    upvote = int(upvote_match.group()) if upvote_match else 0
                except:
                    upvote = 0

                # GitHub ë§í¬
                try:
                    github_url = driver.find_element(By.XPATH, "//a[contains(@href,'github.com')]").get_attribute("href")
                except:
                    github_url = ""

                huggingface_url = link

                # íƒœê·¸ ì¶”ì¶œ
                words = re.findall(r'\b\w+\b', page_content.lower())
                filtered = [w for w in words if w not in stopwords and len(w) > 2]
                counter = Counter(filtered)
                tags = [tag for tag, _ in counter.most_common(3)]
                while len(tags) < 3:
                    tags.append("")

                # JSON êµ¬ì¡°
                json_data = {
                    "content": page_content,
                    "metadata": {
                        "paper_name": paper_name,
                        "github_url": github_url,
                        "huggingface_url": huggingface_url,
                        "upvote": upvote,
                        "tags": tags
                    }
                }

                # ì €ìž¥
                doc_name = f"doc{file_index}.json"
                file_path = os.path.join(folder, doc_name)
                with open(file_path, "w", encoding="utf-8") as f:
                    json.dump(json_data, f, ensure_ascii=False, indent=4)

                logging.info(f"âœ… Saved {file_path}")
                file_index += 1
                success_count += 1
                article_success = True
                break

            except Exception as e:
                logging.warning(f"âš ï¸ Retry {attempt}/{max_retry_per_article} failed for {link}, error: {e}")
                time.sleep(3)

        if not article_success:
            logging.error(f"âŒ Failed to crawl article: {link}")
            fail_count += 1

    # ====== ë‹¤ìŒ ì£¼ ë²„íŠ¼ í´ë¦­ (XPath ì‚¬ìš©) ======
    clicked = False
    for attempt in range(retry_click):
        try:
            driver.get(week_url)  # ì£¼ì°¨ ë¦¬ìŠ¤íŠ¸ íŽ˜ì´ì§€ë¡œ ì´ë™
            next_btn = WebDriverWait(driver, wait_time).until(
                EC.element_to_be_clickable((By.XPATH, "/html/body/div[1]/main/div[2]/section/div[1]/div[4]/div/div[2]/a[2]"))
            )
            next_btn.click()
            time.sleep(random.uniform(3, 6))
            week += 1
            week_url = driver.current_url
            file_index = int(str(base_year)[-2:] + f"{week:02d}" + "001")
            clicked = True
            logging.info(f"âž¡ Moving to next week: {week_url}")
            break
        except:
            logging.warning(f"âš ï¸ Next button click attempt {attempt+1}/{retry_click} failed")
            time.sleep(2)

    if not clicked:
        logging.info("âž¡ No more weeks. Crawling finished.")
        break

driver.quit()

# ====== ìµœì¢… í†µê³„ ======
logging.info("ðŸŽ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
logging.info(f"ì´ ì•„í‹°í´ ìˆ˜: {total_articles}")
logging.info(f"ì„±ê³µ: {success_count}")
logging.info(f"ì‹¤íŒ¨: {fail_count}")
