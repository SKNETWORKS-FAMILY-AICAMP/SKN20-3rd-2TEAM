# HuggingFace DailyPapers í¬ë¡¤ë§ ì‹œìŠ¤í…œ

## ğŸ“‹ ê°œìš”

ë³¸ ìŠ¤í¬ë¦½íŠ¸ëŠ” HuggingFaceì˜ DailyPapers ì›¹ì‚¬ì´íŠ¸ì—ì„œ AI/ML ê´€ë ¨ ë…¼ë¬¸ ì •ë³´ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ëŠ” í¬ë¡¤ëŸ¬ì…ë‹ˆë‹¤. ì£¼ê°„ ë‹¨ìœ„ë¡œ ê²Œì‹œëœ ë…¼ë¬¸ë“¤ì˜ ë©”íƒ€ë°ì´í„°ì™€ ì´ˆë¡ì„ ìˆ˜ì§‘í•˜ì—¬ êµ¬ì¡°í™”ëœ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

### ì£¼ìš” ê¸°ëŠ¥
- HuggingFace Weekly Papers í˜ì´ì§€ì—ì„œ ë…¼ë¬¸ ëª©ë¡ ìë™ ì¶”ì¶œ
- ê° ë…¼ë¬¸ì˜ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ (ì œëª©, ì €ì, ì´ˆë¡, GitHub ë§í¬, ì¶”ì²œ ìˆ˜)
- KeyBERT / TF-IDFë¥¼ í™œìš©í•œ ìë™ í‚¤ì›Œë“œ ì¶”ì¶œ ë¹„êµ
- ì£¼ì°¨ë³„/ì—°ë„ë³„ êµ¬ì¡°í™”ëœ JSON íŒŒì¼ ì €ì¥
- ì¬ì‹œë„ ë¡œì§ ë° Rate Limiting ì²˜ë¦¬

---

## ğŸ—‚ï¸ ë°ì´í„° êµ¬ì¡°

### ì €ì¥ ë””ë ‰í† ë¦¬ êµ¬ì¡°
```
01_data/
â””â”€â”€ documents/
    â””â”€â”€ {year}/
        â””â”€â”€ {year}-W{week}/
            â”œâ”€â”€ doc{YY}{ww}{001}.json
            â”œâ”€â”€ doc{YY}{ww}{002}.json
            â””â”€â”€ ...
```

**ì˜ˆì‹œ**: `01_data/documents/2025/2025-W45/doc2545001.json`

### JSON íŒŒì¼ í¬ë§·

**í˜„ì¬ ë²„ì „ (KeyBERT)**
```json
{
  "context": "ë…¼ë¬¸ì˜ Abstract ì „ë¬¸...",
  "metadata": {
    "title": "ë…¼ë¬¸ ì œëª©",
    "authors": ["ì €ì1", "ì €ì2", "ì €ì3"],
    "publication_year": 2025,
    "github_url": "https://github.com/...",
    "huggingface_url": "https://huggingface.co/papers/...",
    "upvote": 123,
    "tags": ["keyword1", "keyword2", "keyword3"]
  }
}
```

**ì´ˆê¸° ë²„ì „ (TF-IDF)**
```json
{
  "context": "ë…¼ë¬¸ì˜ Abstract ì „ë¬¸...",
  "metadata": {
    "paper_name": "ë…¼ë¬¸ ì œëª©",
    "github_url": "https://github.com/...",
    "huggingface_url": "https://huggingface.co/papers/...",
    "upvote": 123,
    "tag1": "keyword1",
    "tag2": "keyword2",
    "tag3": "keyword3"
  }
}
```

**ì£¼ìš” ì°¨ì´ì **
- í˜„ì¬ ë²„ì „: `tags` ë°°ì—´ ì‚¬ìš©, `authors` ë° `publication_year` í•„ë“œ ì¶”ê°€
- ì´ˆê¸° ë²„ì „: ê°œë³„ `tag1`, `tag2`, `tag3` í•„ë“œ ì‚¬ìš©, `paper_name` í•„ë“œëª…

---

## ğŸ”§ ì£¼ìš” í•¨ìˆ˜ ì„¤ëª…

### 1. `extract_keywords(text: str, top_n: int = 3) -> List[str]`

**ëª©ì **: KeyBERTë¥¼ ì‚¬ìš©í•˜ì—¬ ë…¼ë¬¸ ì´ˆë¡ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ìë™ ì¶”ì¶œ

**ë§¤ê°œë³€ìˆ˜**
- `text`: ë…¼ë¬¸ ì´ˆë¡ í…ìŠ¤íŠ¸
- `top_n`: ì¶”ì¶œí•  í‚¤ì›Œë“œ ê°œìˆ˜ (ê¸°ë³¸ê°’: 3)

**ë°˜í™˜ê°’**: ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: `["transformer", "attention mechanism", "nlp"]`)

**íŠ¹ì§•**
- KeyBERTì˜ MaxSum ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©
- 1~2ë‹¨ì–´ êµ¬ë¬¸(n-gram) ì¶”ì¶œ
- ì»¤ìŠ¤í…€ ë¶ˆìš©ì–´ í•„í„°ë§ ì ìš©
- ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜ (`keyword1`, `keyword2`, ...)

---

## ğŸ·ï¸ í‚¤ì›Œë“œ ì¶”ì¶œ ë°©ë²•ë¡ 

ë³¸ í”„ë¡œì íŠ¸ì—ì„œëŠ” ë…¼ë¬¸ì˜ í•µì‹¬ ê°œë…ì„ íŒŒì•…í•˜ê¸° ìœ„í•´ ë‘ ê°€ì§€ í‚¤ì›Œë“œ ì¶”ì¶œ ë°©ì‹ì„ ì ìš©í•´ ë³´ì•˜ìŠµë‹ˆë‹¤.

### KeyBERT ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ

**ì•Œê³ ë¦¬ì¦˜**: KeyBERTì˜ BERT ì„ë² ë”© + MaxSum ë‹¤ì–‘ì„± ì•Œê³ ë¦¬ì¦˜

**ì¶”ì¶œ ê³¼ì •**
1. BERT ëª¨ë¸ë¡œ ë¬¸ì„œì™€ í›„ë³´ í‚¤ì›Œë“œì˜ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°
2. MaxSum ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì˜ë¯¸ì ìœ¼ë¡œ ë‹¤ì–‘í•œ í‚¤ì›Œë“œ ì„ íƒ
3. ì»¤ìŠ¤í…€ ë¶ˆìš©ì–´ í•„í„°ë§ ì ìš©

**ì¥ì **
- ë¬¸ë§¥ì„ ê³ ë ¤í•œ ì˜ë¯¸ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
- ì‚¬ì „ í•™ìŠµëœ BERT ëª¨ë¸ í™œìš©ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ
- ê°„ê²°í•œ ì½”ë“œë¡œ ê³ í’ˆì§ˆ í‚¤ì›Œë“œ í™•ë³´

**ì˜ˆì‹œ ê²°ê³¼**
```python
Abstract: "We propose a novel transformer architecture using self-attention..."
Keywords: ["transformer architecture", "self attention", "neural network"]
```

---

### TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ

**ì•Œê³ ë¦¬ì¦˜**: Lemmatization â†’ TF-IDF ë²¡í„°í™” â†’ ì¤‘ë³µ í•„í„°ë§

**ì¶”ì¶œ ê³¼ì •**

1. í† í°í™”: ì†Œë¬¸ì ë³€í™˜í›„ ì •ê·œì‹ìœ¼ë¡œ 3ê¸€ì ì´ìƒ ì˜ë¬¸ ë‹¨ì–´ ì¶”ì¶œ
2. Lemmatization (í˜•íƒœì†Œ ì •ê·œí™”): WordNetLemmatizer ì‚¬ìš©
   - ì˜ˆ: "training" â†’ "train", "models" â†’ "model"
3. ë¶ˆìš©ì–´ ì œê±° (NLTK): ì˜ì–´ stopwordsì™€ ë…¼ë¬¸ íŠ¹í™” ë¶ˆìš©ì–´ë¥¼ ì œê±°
4. TF-IDF ë²¡í„°í™”: n-gram ë²”ìœ„: (1, 2)
5. ì ìˆ˜ ê¸°ë°˜ ì •ë ¬: TF-IDF ì ìˆ˜ ìƒìœ„ Nê°œ ì„ íƒ
6. ì¤‘ë³µ í•„í„°ë§: ê¸´ í‚¤ì›Œë“œì— í¬í•¨ëœ ì§§ì€ í‚¤ì›Œë“œ ì œê±° í›„ í‚¤ì›Œë“œë¥¼ 3ê°œë¡œ ë§ì¶¤
   - ì˜ˆ: ["attention", "self attention"] â†’ ["self attention"]

**ì¥ì **
- í†µê³„ ê¸°ë°˜ ì ‘ê·¼ìœ¼ë¡œ í•´ì„ ê°€ëŠ¥ì„± ë†’ìŒ
- Lemmatizationìœ¼ë¡œ ë‹¨ì–´ ë³€í˜• ë¬¸ì œ í•´ê²°

**ì˜ˆì‹œ ê²°ê³¼**
```python
Abstract: "We trained multiple neural networks using transformers..."
Keywords: ["transformer", "neural network", "train"]
# "training" â†’ "train" (lemmatized)
# "networks" â†’ "network" (lemmatized)
```

**ì£¼ìš” ì°¨ì´ì **
- **ì „ì²˜ë¦¬**: í˜•íƒœì†Œ ì •ê·œí™”ë¡œ ë‹¨ì–´ ê¸°ë³¸í˜• ì¶”ì¶œ
- **í•„í„°ë§**: 2ë‹¨ê³„ ì¤‘ë³µ ì œê±° (ë¶ˆìš©ì–´ + substring í•„í„°)

---

### 2. `get_with_retry(url: str, max_retries: int = 3)`

**ëª©ì **: ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë° Rate Limitingì„ ì²˜ë¦¬í•˜ëŠ” ì•ˆì •ì ì¸ HTTP ìš”ì²­ í•¨ìˆ˜

**ë§¤ê°œë³€ìˆ˜**
- `url`: ìš”ì²­í•  URL
- `max_retries`: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸ê°’: 3)

**ë°˜í™˜ê°’**: `requests.Response` ê°ì²´ ë˜ëŠ” `None` (ì‹¤íŒ¨ ì‹œ)

**íŠ¹ì§•**
- HTTP 429 (Too Many Requests) ì—ëŸ¬ ëŒ€ì‘
- ì¬ì‹œë„ ê°„ 2ì´ˆ ëŒ€ê¸° (ê¸°ë³¸), 429 ì—ëŸ¬ ì‹œ 5ì´ˆ ëŒ€ê¸°
- íƒ€ì„ì•„ì›ƒ 10ì´ˆ ì„¤ì •
- User-Agent í—¤ë” ìë™ ì„¤ì •

---

### 3. `fetch_weekly_papers(year: int, week: int) -> List[Dict[str, str]]`

**ëª©ì **: HuggingFace Weekly Papers í˜ì´ì§€ì—ì„œ í•´ë‹¹ ì£¼ì°¨ ë…¼ë¬¸ ëª©ë¡ ì¶”ì¶œ

**ë§¤ê°œë³€ìˆ˜**
- `year`: ì—°ë„ (ì˜ˆ: 2025)
- `week`: ì£¼ì°¨ (1~52)

**ë°˜í™˜ê°’**: ë…¼ë¬¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
```python
[
    {"title": "ë…¼ë¬¸ ì œëª©", "url": "https://huggingface.co/papers/..."},
    ...
]
```

**í¬ë¡¤ë§ ëŒ€ìƒ**
- URL íŒ¨í„´: `https://huggingface.co/papers/week/{year}-W{week:02d}`
- CSS Selector: `a.line-clamp-3` (ë…¼ë¬¸ ì œëª© ë§í¬)

---

### 4. `fetch_paper_details(paper_url: str) -> Dict[str, any]`

**ëª©ì **: ê°œë³„ ë…¼ë¬¸ ìƒì„¸ í˜ì´ì§€ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ

**ë§¤ê°œë³€ìˆ˜**
- `paper_url`: ë…¼ë¬¸ HuggingFace URL

**ë°˜í™˜ê°’**: ë…¼ë¬¸ ìƒì„¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬
```python
{
    "context": "Abstract ì „ë¬¸",
    "authors": ["ì €ì1", "ì €ì2"],
    "github_url": "GitHub URL",
    "upvote": 123
}
```

**ì¶”ì¶œ ë°ì´í„°**
1. **Abstract**: `section div` ë‚´ ëª¨ë“  `<p>` íƒœê·¸ ê²°í•©
2. **Authors**: CSS Selectorë¡œ ì €ì ë§í¬ ì¶”ì¶œ
3. **GitHub URL**: `href*="github.com"` ì†ì„± ê²€ìƒ‰
4. **Upvote**: `div.font-semibold.text-orange-500` ë‚´ ìˆ«ì íŒŒì‹±

---

### 5. `save_paper_json(paper_data: Dict, year: int, week: int, index: int) -> str`

**ëª©ì **: í¬ë¡¤ë§í•œ ë…¼ë¬¸ ë°ì´í„°ë¥¼ êµ¬ì¡°í™”ëœ JSON íŒŒì¼ë¡œ ì €ì¥<br>
**JSONì„ ì„ íƒí•œ ì´ìœ **: tagë¥¼ ì‚¬ìš© í•  ê²ƒì´ê¸° ë•Œë¬¸ì— êµ¬ì¡°í™” ëœ ë°ì´í„°ê°€ í•„ìš”

**ë§¤ê°œë³€ìˆ˜**
- `paper_data`: ë…¼ë¬¸ ì „ì²´ ë°ì´í„° (ì œëª©, ì´ˆë¡, ì €ì, URL ë“±)
- `year`: ì—°ë„
- `week`: ì£¼ì°¨
- `index`: ë…¼ë¬¸ ì¸ë±ìŠ¤ (0ë¶€í„° ì‹œì‘)

**ë°˜í™˜ê°’**: ì €ì¥ëœ ë¬¸ì„œ ID (ì˜ˆ: `doc2545001`)

**íŒŒì¼ëª… ê·œì¹™**
- í˜•ì‹: `doc{YY}{ww}{NNN}.json`
- ì˜ˆì‹œ: `doc2545001.json` = 2025ë…„ 45ì£¼ì°¨ 1ë²ˆì§¸ ë…¼ë¬¸

---

### 6. `crawl_weekly_papers(year: int, week: int)`

**ëª©ì **: íŠ¹ì • ì£¼ì°¨ì˜ ì „ì²´ í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ (ë©”ì¸ í•¨ìˆ˜)

**ë§¤ê°œë³€ìˆ˜**
- `year`: ì—°ë„
- `week`: ì£¼ì°¨

**ì‹¤í–‰ íë¦„**
```
1. Weekly í˜ì´ì§€ì—ì„œ ë…¼ë¬¸ ëª©ë¡ ì¶”ì¶œ (fetch_weekly_papers)
   â†“
2. ê° ë…¼ë¬¸ì— ëŒ€í•´:
   a. ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ (fetch_paper_details)
   b. KeyBERTë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ (extract_keywords)
   c. JSON íŒŒì¼ ì €ì¥ (save_paper_json)
   â†“
3. ì„±ê³µ/ì‹¤íŒ¨ í†µê³„ ì¶œë ¥
```

**Rate Limiting ì „ëµ**
- ê° ë…¼ë¬¸ ì²˜ë¦¬ í›„ 2ì´ˆ ëŒ€ê¸°
- 40ê°œ ë…¼ë¬¸ë§ˆë‹¤ 160ì´ˆ íœ´ì‹ (429 ì—ëŸ¬ ë°©ì§€)

---

## âš™ï¸ ì„¤ì • ë° ìƒìˆ˜

### HTTP í—¤ë”
```python
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0 Safari/537.36"
}
```
- ë¸Œë¼ìš°ì € ìš”ì²­ ìœ„ì¥ìœ¼ë¡œ ì°¨ë‹¨ ë°©ì§€

### ì»¤ìŠ¤í…€ ë¶ˆìš©ì–´
```python
custom_stopwords = {
    "the", "a", "an", "and", "or", "but", ...,  # ì¼ë°˜ ë¶ˆìš©ì–´
    "paper", "propose", "present", "show", ...  # ë…¼ë¬¸ íŠ¹í™” ë¶ˆìš©ì–´
}
```
- ì¼ë°˜ ì˜ì–´ ë¶ˆìš©ì–´ + ë…¼ë¬¸ì—ì„œ ìì£¼ ë“±ì¥í•˜ëŠ” ì¼ë°˜ì  ìš©ì–´ ì œê±°
- KeyBERT í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œ ë…¸ì´ì¦ˆ ê°ì†Œ

---

## ğŸ“¦ ì˜ì¡´ì„±

### í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ (KeyBERT ë²„ì „)
```python
import os, json, re, time, requests
from typing import List, Dict
from bs4 import BeautifulSoup
from tqdm import tqdm
import nltk
from keybert import KeyBERT
```

### í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ (TF-IDF ë²„ì „)
```python
import os, json, re, time, requests
from typing import List, Dict
from bs4 import BeautifulSoup
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
```

### NLTK ë°ì´í„°
- `corpora/stopwords`: ë¶ˆìš©ì–´ ì‚¬ì „
- `corpora/wordnet`: í˜•íƒœì†Œ ë¶„ì„ìš© (Lemmatization)
- `corpora/omw-1.4`: Open Multilingual Wordnet (TF-IDF ë²„ì „)
- ì´ˆê¸° ì‹¤í–‰ ì‹œ ìë™ ë‹¤ìš´ë¡œë“œ

---

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### ê¸°ë³¸ ì‹¤í–‰
```python
if __name__ == "__main__":
    # 2025ë…„ 47~49ì£¼ì°¨ í¬ë¡¤ë§
    for week in range(47, 50):
        try:
            crawl_weekly_papers(year=2025, week=week)
        except Exception as e:
            print(f"[FATAL] W{week:02d} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
```

### ì»¤ìŠ¤í„°ë§ˆì´ì§•
```python
# ë‹¨ì¼ ì£¼ì°¨ í¬ë¡¤ë§
crawl_weekly_papers(year=2025, week=45)

# ì—¬ëŸ¬ ì£¼ì°¨ í¬ë¡¤ë§
for week in range(45, 50):
    crawl_weekly_papers(year=2025, week=week)
```

---

## âš ï¸ ì£¼ì˜ì‚¬í•­

1. **Rate Limiting**
   - 40ê°œ ë…¼ë¬¸ë§ˆë‹¤ 160ì´ˆ íœ´ì‹ (í•˜ë“œì½”ë”©ë¨)
   - ë„ˆë¬´ ë¹ ë¥¸ ìš”ì²­ ì‹œ 429 ì—ëŸ¬ ë°œìƒ ê°€ëŠ¥

2. **ë„¤íŠ¸ì›Œí¬ ì•ˆì •ì„±**
   - ì¬ì‹œë„ ë¡œì§ ìˆì§€ë§Œ, ì¥ê¸° ì‹¤í–‰ ì‹œ ëª¨ë‹ˆí„°ë§ í•„ìš”
   - ì‹¤íŒ¨í•œ ë…¼ë¬¸ì€ ë¡œê·¸ì— ê¸°ë¡ë˜ë‚˜ ì¬ì‹œë„í•˜ì§€ ì•ŠìŒ

3. **ë°ì´í„° í’ˆì§ˆ**
   - Abstractê°€ ì—†ëŠ” ë…¼ë¬¸ì€ ìë™ ìŠ¤í‚µ
   - í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš© (`keyword1`, `keyword2`, ...)

---

## ğŸ” ë¬¸ì œ í•´ê²°

### ìì£¼ ë°œìƒí•˜ëŠ” ì˜¤ë¥˜

**1. 429 Too Many Requests**
- ì›ì¸: Rate Limit ì´ˆê³¼
- í•´ê²°: `time.sleep()` ì‹œê°„ ì¦ê°€ ë˜ëŠ” íœ´ì‹ ë¹ˆë„ ì¡°ì •

**2. íƒ€ì„ì•„ì›ƒ ì˜¤ë¥˜**
- ì›ì¸: ë„¤íŠ¸ì›Œí¬ ë¶ˆì•ˆì •
- í•´ê²°: `timeout` ê°’ ì¦ê°€ (í˜„ì¬ 10ì´ˆ)

**3. í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨**
- ì›ì¸: Abstractê°€ ë„ˆë¬´ ì§§ê±°ë‚˜ íŠ¹ìˆ˜ë¬¸ìë§Œ í¬í•¨
- í•´ê²°: ìë™ìœ¼ë¡œ ê¸°ë³¸ê°’ ì‚¬ìš© (ì •ìƒ ë™ì‘)

**4. ì €ì¥ ê²½ë¡œ ì˜¤ë¥˜**
- ì›ì¸: ë””ë ‰í† ë¦¬ ê¶Œí•œ ë¬¸ì œ
- í•´ê²°: `os.makedirs(exist_ok=True)` í™•ì¸ ë˜ëŠ” ìˆ˜ë™ ìƒì„±

---

## ğŸ“Œ ì°¸ê³  ì‚¬í•­

### íŒŒì¼ ìœ„ì¹˜
- **KeyBERT ë²„ì „**: `02_src/01_data_collection/crawling.py`
- **TF-IDF ë²„ì „**: `02_src/01_data_collection/past/crawling_past.py`

### í¬ë¡¤ë§ ì •ë³´
- í¬ë¡¤ë§ ëŒ€ìƒ: https://huggingface.co/papers/week/{year}-W{week}
- ë¡œë´‡ ë°°ì œ í‘œì¤€(robots.txt) ì¤€ìˆ˜
- í•™ìˆ  ëª©ì  ë°ì´í„° ìˆ˜ì§‘ìš©

### ê¸°ìˆ  ìŠ¤íƒ
- **í‚¤ì›Œë“œ ì¶”ì¶œ**: KeyBERT / TF-IDF + Lemmatization
- **ì›¹ ìŠ¤í¬ë˜í•‘**: BeautifulSoup4 + requests
- **ë°ì´í„° ì €ì¥**: JSON íŒŒì¼ ì‹œìŠ¤í…œ
- **ìì—°ì–´ ì²˜ë¦¬**: NLTK, scikit-learn
