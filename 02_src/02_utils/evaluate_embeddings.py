"""
ì„ë² ë”© ëª¨ë¸ ë° ì²­í¬ ì „ëµ ë¹„êµ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

ì—¬ëŸ¬ ì„ë² ë”© ëª¨ë¸ê³¼ ì²­í¬ ì „ëµ ì¡°í•©ì„ ë¹„êµí•˜ì—¬ ìµœì ì˜ ì¡°í•©ì„ ì„ íƒí•©ë‹ˆë‹¤.

í‰ê°€ ëª¨ë¸:
1. sentence-transformers/all-MiniLM-L6-v2 (384 dim, ë¹ ë¦„)
2. sentence-transformers/all-mpnet-base-v2 (768 dim, ê³ í’ˆì§ˆ)
3. sentence-transformers/msmarco-MiniLM-L-6-v3 (384 dim, ê²€ìƒ‰ ìµœì í™”)
4. sentence-transformers/allenai-specter (768 dim, scientific papers íŠ¹í™”)
5. OpenAI text-embedding-3-small (1536 dim)
6. BAAI/bge-m3 (1024 dim, ë‹¤êµ­ì–´)
7. paraphrase-multilingual-mpnet-base-v2 (768 dim, ë‹¤êµ­ì–´)

í‰ê°€ ì²­í¬ ì „ëµ:
- ìë™ìœ¼ë¡œ 01_data/chunks/ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  *_C.pkl íŒŒì¼ íƒìƒ‰
- ë‹¤ì–‘í•œ chunk_sizeì™€ overlap ì¡°í•© í‰ê°€

í‰ê°€ ë©”íŠ¸ë¦­:
- Hit Rate@K: ìƒìœ„ Kê°œ ê²°ê³¼ ì¤‘ ìµœì†Œ 1ê°œì˜ ê´€ë ¨ ë¬¸ì„œ í¬í•¨ ë¹„ìœ¨
- MRR (Mean Reciprocal Rank): ì²« ë²ˆì§¸ ê´€ë ¨ ë¬¸ì„œì˜ ì—­ìˆœìœ„ í‰ê· 
- NDCG@K: ìˆœìœ„ë¥¼ ê³ ë ¤í•œ ê²€ìƒ‰ í’ˆì§ˆ ì¸¡ì •
- Avg Time: ì¿¼ë¦¬ë‹¹ í‰ê·  ê²€ìƒ‰ ì‹œê°„

í‰ê°€ ë°©ë²•:
- í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ 10ê°œì— ëŒ€í•œ ê²€ìƒ‰ ì„±ëŠ¥ ì¸¡ì •
- Cosine similarity ê¸°ë°˜ ê²€ìƒ‰
- ê° ëª¨ë¸ Ã— ì²­í¬ ì „ëµ ì¡°í•©ì— ëŒ€í•´ í‰ê°€ ìˆ˜í–‰

ì¶œë ¥:
- í„°ë¯¸ë„ì— ê²°ê³¼ í…Œì´ë¸” ì¶œë ¥
- embedding_evaluation_report.md ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìë™ ìƒì„±
  - ì „ì²´ ê²°ê³¼ í‘œ
  - ìµœê³  ì„±ëŠ¥ ì¡°í•© (NDCG@10, Hit Rate@10, MRR, ì†ë„ ê¸°ì¤€)
  - ì²­í¬ ì „ëµë³„/ëª¨ë¸ë³„ ë¹„êµ
  - í”„ë¡œë•ì…˜ ì‚¬ìš© ê¶Œì¥ì‚¬í•­
"""

import os
import pickle
import time
import logging
from typing import List, Dict
from dataclasses import dataclass

import numpy as np
import torch
from sklearn.metrics.pairwise import cosine_similarity
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import OpenAIEmbeddings
from transformers import AutoModel
from dotenv import load_dotenv

# Load environment variables
load_dotenv()


# ==================== ë¡œê¹… ì„¤ì • ====================

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


# ==================== í‰ê°€ìš© ë°ì´í„° í´ë˜ìŠ¤ ====================


@dataclass
class TestQuery:
    """í‰ê°€ìš© í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬"""

    query: str
    relevant_keywords: List[str]  # ê´€ë ¨ í‚¤ì›Œë“œ (metadata.tagsì—ì„œ ë§¤ì¹­)
    relevant_doc_ids: List[str] = None  # íŠ¹ì • ë¬¸ì„œ ID (optional)

    def __repr__(self):
        return f"TestQuery(query='{self.query[:50]}...', keywords={self.relevant_keywords})"


@dataclass
class EvaluationResult:
    """ëª¨ë¸ í‰ê°€ ê²°ê³¼"""

    model_name: str
    chunk_strategy: str  # e.g., "100_15"
    hit_rate_at_5: float
    hit_rate_at_10: float
    mrr: float  # Mean Reciprocal Rank
    ndcg_at_5: float
    ndcg_at_10: float
    avg_time: float  # í‰ê·  ê²€ìƒ‰ ì‹œê°„ (ì´ˆ)

    def __repr__(self):
        return (
            f"EvaluationResult(model='{self.model_name}', "
            f"chunk={self.chunk_strategy}, "
            f"HR@5={self.hit_rate_at_5:.3f}, HR@10={self.hit_rate_at_10:.3f}, "
            f"MRR={self.mrr:.3f}, NDCG@5={self.ndcg_at_5:.3f}, NDCG@10={self.ndcg_at_10:.3f}, "
            f"time={self.avg_time:.2f}s)"
        )


# ==================== í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì •ì˜ ====================
# ì‹¤ì œ í´ëŸ¬ìŠ¤í„° ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ê°œì„ ëœ TEST_QUERIES ì‚¬ìš©

from improved_test_queries import IMPROVED_TEST_QUERIES as TEST_QUERIES

# í´ëŸ¬ìŠ¤í„° ì»¤ë²„ë¦¬ì§€: 18ê°œ í´ëŸ¬ìŠ¤í„° ì¤‘ 18ê°œ (100%)
# ì£¼ìš” ê³ í’ˆì§ˆ í´ëŸ¬ìŠ¤í„° í¬í•¨:
#   - Cluster 17 (LLM training, avg_upvote: 44.3)
#   - Cluster 11 (Attention, avg_upvote: 42.9)
#   - Cluster 5 (Long context, avg_upvote: 38.7)
#   - Cluster 15 (Video/multimodal, avg_upvote: 39.6)
# ì´ 14ê°œ ì¿¼ë¦¬ë¡œ ëª¨ë“  í´ëŸ¬ìŠ¤í„° ì»¤ë²„

# ==================== ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ====================


def init_models() -> Dict[str, any]:
    """
    7ê°œ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”

    Returns:
        Dict[str, embedding_model]: ëª¨ë¸ëª… -> ì„ë² ë”© ëª¨ë¸ ê°ì²´
    """
    models = {}
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    logger.info("[ëª¨ë¸ ì´ˆê¸°í™”] 7ê°œ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì‹œì‘...")
    logger.info(f"[GPU ì‚¬ìš© ì—¬ë¶€] {device}")

    # 1. all-MiniLM-L6-v2 (384 dim, ë¹ ë¦„)
    try:
        models["MiniLM-L6"] = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs={"device": device},
            encode_kwargs={"normalize_embeddings": True},
        )
        logger.info("âœ“ MiniLM-L6-v2 ë¡œë”© ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âœ— MiniLM-L6-v2 ë¡œë”© ì‹¤íŒ¨: {e}")

    # 2. all-mpnet-base-v2 (768 dim, ë†’ì€ í’ˆì§ˆ)
    try:
        models["MPNet"] = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-mpnet-base-v2",
            model_kwargs={"device": device},
            encode_kwargs={"normalize_embeddings": True},
        )
        logger.info("âœ“ MPNet-base-v2 ë¡œë”© ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âœ— MPNet-base-v2 ë¡œë”© ì‹¤íŒ¨: {e}")

    # 3. msmarco-MiniLM-L-6-v3 (384 dim, ê²€ìƒ‰ ìµœì í™”)
    try:
        models["MsMarco"] = HuggingFaceEmbeddings(
            model_name="sentence-transformers/msmarco-MiniLM-L-6-v3",
            model_kwargs={"device": device},
            encode_kwargs={"normalize_embeddings": True},
        )
        logger.info("âœ“ MsMarco-MiniLM ë¡œë”© ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âœ— MsMarco-MiniLM ë¡œë”© ì‹¤íŒ¨: {e}")

    # 4. allenai-specter (768 dim, scientific papers íŠ¹í™”)
    try:
        models["SPECTER"] = HuggingFaceEmbeddings(
            model_name="sentence-transformers/allenai-specter",
            model_kwargs={"device": device},
            encode_kwargs={"normalize_embeddings": True},
        )
        logger.info("âœ“ SPECTER (scientific) ë¡œë”© ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âœ— SPECTER ë¡œë”© ì‹¤íŒ¨: {e}")

    # 5. OpenAI text-embedding-3-small (1536 dim)
    try:
        if os.getenv("OPENAI_API_KEY"):
            models["OpenAI-small"] = OpenAIEmbeddings(model="text-embedding-3-small")
            logger.info("âœ“ OpenAI text-embedding-3-small ë¡œë”© ì™„ë£Œ")
        else:
            logger.warning("âœ— OPENAI_API_KEY not found, skipping OpenAI model")
    except Exception as e:
        logger.error(f"âœ— OpenAI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")

    # 6. BAAI/bge-m3 (1024 dim, ì¤‘êµ­ì–´ ë° ì˜ì–´ ì§€ì›)
    try:
        models["BGE-M3"] = HuggingFaceEmbeddings(
            model_name="BAAI/bge-m3",
            model_kwargs={"device": device},
            encode_kwargs={"normalize_embeddings": True},
        )
        logger.info("âœ“ BGE-M3 ë¡œë”© ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âœ— BGE-M3 ë¡œë”© ì‹¤íŒ¨: {e}")

    # 7. paraphrase-multilingual-mpnet-base-v2 (768 dim, ë‹¤êµ­ì–´ ì§€ì›)
    try:
        models["Paraphrase-Multi"] = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
            model_kwargs={"device": device},
            encode_kwargs={"normalize_embeddings": True},
        )
        logger.info("âœ“ Paraphrase-Multilingual ë¡œë”© ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âœ— Paraphrase-Multilingual ë¡œë”© ì‹¤íŒ¨: {e}")

    logger.info(f"[ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ] {len(models)}ê°œ ëª¨ë¸ ì¤€ë¹„ë¨")
    return models


# ==================== í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° ====================


def calculate_hit_rate_at_k(
    relevant_indices: List[int], retrieved_indices: List[int], k: int
) -> float:
    """
    Hit Rate@k ê³„ì‚°
    ìµœì†Œ 1ê°œì˜ ê´€ë ¨ ë¬¸ì„œê°€ ìƒìœ„ kê°œì— í¬í•¨ë˜ë©´ 1, ì•„ë‹ˆë©´ 0

    Args:
        relevant_indices: ê´€ë ¨ ë¬¸ì„œ ì¸ë±ìŠ¤ ëª©ë¡
        retrieved_indices: ê²€ìƒ‰ëœ ë¬¸ì„œ ì¸ë±ìŠ¤ ëª©ë¡ (ìƒìœ„ kê°œ)
        k: ìƒìœ„ kê°œ

    Returns:
        float: Hit Rate@k ì ìˆ˜ (0 or 1)
    """
    if not relevant_indices:
        return 0.0

    retrieved_k = set(retrieved_indices[:k])
    relevant_set = set(relevant_indices)

    # êµì§‘í•©ì´ ìˆìœ¼ë©´ 1, ì—†ìœ¼ë©´ 0
    return 1.0 if len(retrieved_k & relevant_set) > 0 else 0.0


def calculate_mrr(relevant_indices: List[int], retrieved_indices: List[int]) -> float:
    """
    Mean Reciprocal Rank ê³„ì‚°

    Args:
        relevant_indices: ê´€ë ¨ ë¬¸ì„œ ì¸ë±ìŠ¤ ëª©ë¡
        retrieved_indices: ê²€ìƒ‰ëœ ë¬¸ì„œ ì¸ë±ìŠ¤ ëª©ë¡

    Returns:
        float: MRR ì ìˆ˜
    """
    relevant_set = set(relevant_indices)

    for rank, idx in enumerate(retrieved_indices, 1):
        if idx in relevant_set:
            return 1.0 / rank

    return 0.0


def calculate_ndcg_at_k(
    relevant_indices: List[int], retrieved_indices: List[int], k: int
) -> float:
    """
    Normalized Discounted Cumulative Gain@k ê³„ì‚°

    Args:
        relevant_indices: ê´€ë ¨ ë¬¸ì„œ ì¸ë±ìŠ¤ ëª©ë¡
        retrieved_indices: ê²€ìƒ‰ëœ ë¬¸ì„œ ì¸ë±ìŠ¤ ëª©ë¡
        k: ìƒìœ„ kê°œ

    Returns:
        float: NDCG@k ì ìˆ˜
    """
    if not relevant_indices:
        return 0.0

    relevant_set = set(relevant_indices)
    retrieved_k = retrieved_indices[:k]

    # DCG ê³„ì‚° (relevanceëŠ” binary: relevant=1, irrelevant=0)
    dcg = 0.0
    for i, idx in enumerate(retrieved_k, 1):
        if idx in relevant_set:
            # rel_i / log2(i+1)
            dcg += 1.0 / np.log2(i + 1)

    # IDCG ê³„ì‚° (ì´ìƒì ì¸ ìˆœì„œ: ëª¨ë“  relevant ë¬¸ì„œê°€ ì•ì—)
    idcg = 0.0
    for i in range(1, min(len(relevant_indices), k) + 1):
        idcg += 1.0 / np.log2(i + 1)

    if idcg == 0.0:
        return 0.0

    return dcg / idcg


# ==================== í´ëŸ¬ìŠ¤í„° ë©”íƒ€ë°ì´í„° ë¡œë”© ====================

# ì „ì—­ ë³€ìˆ˜ë¡œ í´ëŸ¬ìŠ¤í„° ë©”íƒ€ë°ì´í„° ìºì‹±
_cluster_assignments = None
_cluster_metadata = None


def load_cluster_metadata():
    """í´ëŸ¬ìŠ¤í„° ë©”íƒ€ë°ì´í„°ë¥¼ ë¡œë“œ (í•œ ë²ˆë§Œ ë¡œë“œ)"""
    global _cluster_assignments, _cluster_metadata

    if _cluster_assignments is not None and _cluster_metadata is not None:
        return _cluster_assignments, _cluster_metadata

    import json
    from pathlib import Path

    PROJECT_ROOT = Path(__file__).parent.parent.parent

    # cluster_assignments.json ë¡œë“œ
    assignments_path = PROJECT_ROOT / "01_data" / "clusters" / "cluster_assignments.json"
    with open(assignments_path, "r", encoding="utf-8") as f:
        data = json.load(f)
        _cluster_assignments = data["assignments"]

    # cluster_metadata.json ë¡œë“œ
    metadata_path = PROJECT_ROOT / "01_data" / "clusters" / "cluster_metadata.json"
    with open(metadata_path, "r", encoding="utf-8") as f:
        data = json.load(f)
        _cluster_metadata = data["clusters"]

    logger.info(f"[CLUSTER] í´ëŸ¬ìŠ¤í„° ë©”íƒ€ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(_cluster_assignments)} documents, {len(_cluster_metadata)} clusters")

    return _cluster_assignments, _cluster_metadata


# ==================== ê²€ìƒ‰ ë° í‰ê°€ ====================


def find_relevant_docs(chunks: List[Document], test_query: TestQuery) -> List[int]:
    """
    í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œ ì¸ë±ìŠ¤ ì°¾ê¸°

    í´ëŸ¬ìŠ¤í„° ë©”íƒ€ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ì„± íŒë‹¨:
    1. ë¬¸ì„œì˜ doc_idë¡œ cluster_id ì°¾ê¸°
    2. cluster_idë¡œ cluster keywords ê°€ì ¸ì˜¤ê¸°
    3. test_queryì˜ relevant_keywordsì™€ cluster keywords ë§¤ì¹­

    Args:
        chunks: ì „ì²´ ë¬¸ì„œ ì²­í¬
        test_query: í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬

    Returns:
        List[int]: ê´€ë ¨ ë¬¸ì„œ ì¸ë±ìŠ¤ ëª©ë¡
    """
    # í´ëŸ¬ìŠ¤í„° ë©”íƒ€ë°ì´í„° ë¡œë“œ
    cluster_assignments, cluster_metadata = load_cluster_metadata()

    relevant_indices = []

    for idx, doc in enumerate(chunks):
        doc_id = doc.metadata.get("doc_id", "")

        # ë¬¸ì„œì˜ ì²­í¬ ID ì œê±° (doc2549001_chunk_0 -> doc2549001)
        if "_chunk_" in doc_id:
            base_doc_id = doc_id.split("_chunk_")[0]
        else:
            base_doc_id = doc_id

        # cluster_id ê°€ì ¸ì˜¤ê¸°
        cluster_id = cluster_assignments.get(base_doc_id, None)

        if cluster_id is None:
            continue

        # cluster keywords ê°€ì ¸ì˜¤ê¸°
        cluster_info = cluster_metadata.get(str(cluster_id), {})
        cluster_keywords = cluster_info.get("keywords", [])

        # ê´€ë ¨ í‚¤ì›Œë“œê°€ cluster keywords ë‚´ ë¶€ë¶„ ë¬¸ìì—´ë¡œ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if any(
            keyword.lower() in cluster_keyword.lower()
            for keyword in test_query.relevant_keywords
            for cluster_keyword in cluster_keywords
        ):
            relevant_indices.append(idx)

    return relevant_indices


def retrieve_top_k(
    query_embedding: np.ndarray, doc_embeddings: np.ndarray, k: int = 10
) -> List[int]:
    """
    Cosine similarity ê¸°ë°˜ ìƒìœ„ kê°œ ë¬¸ì„œ ì¸ë±ìŠ¤ ê²€ìƒ‰

    Args:
        query_embedding: ì¿¼ë¦¬ ì„ë² ë”© (1, dim)
        doc_embeddings: ë¬¸ì„œ ì„ë² ë”© (n, dim)
        k: ìƒìœ„ kê°œ

    Returns:
        List[int]: ìƒìœ„ kê°œ ë¬¸ì„œ ì¸ë±ìŠ¤
    """
    # Cosine similarity ê³„ì‚°
    similarities = cosine_similarity(query_embedding, doc_embeddings)[0]

    # ìƒìœ„ kê°œ ì¸ë±ìŠ¤
    top_k_indices = np.argsort(similarities)[::-1][:k]

    return top_k_indices.tolist()


def evaluate_model(
    model_name: str,
    embedding_model: any,
    chunks: List[Document],
    test_queries: List[TestQuery],
    chunk_strategy: str,
    top_k: int = 10,
) -> EvaluationResult:
    """
    ë‹¨ì¼ ëª¨ë¸ í‰ê°€

    Args:
        model_name: ëª¨ë¸ëª…
        embedding_model: ì„ë² ë”© ëª¨ë¸
        chunks: ì „ì²´ ë¬¸ì„œ ì²­í¬
        test_queries: í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ëª©ë¡
        chunk_strategy: ì²­í¬ ì „ëµ (e.g., "100_15")
        top_k: ìƒìœ„ kê°œ ê²€ìƒ‰

    Returns:
        EvaluationResult: í‰ê°€ ê²°ê³¼
    """
    logger.info(f"\n[í‰ê°€ ì‹œì‘] {model_name} (chunk: {chunk_strategy})")

    # 1. ëª¨ë“  ë¬¸ì„œ ì„ë² ë”© ìƒì„±
    logger.info(f"  - ë¬¸ì„œ ì„ë² ë”© ìƒì„± ì¤‘... ({len(chunks)}ê°œ)")
    start_time = time.time()

    try:
        # ë°°ì¹˜ë¡œ ì„ë² ë”© (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
        batch_size = 100
        all_embeddings = []

        for i in range(0, len(chunks), batch_size):
            batch_docs = chunks[i : i + batch_size]
            batch_texts = [doc.page_content for doc in batch_docs]

            if hasattr(embedding_model, "embed_documents"):
                batch_embeddings = embedding_model.embed_documents(batch_texts)
            else:
                # OpenAIì˜ ê²½ìš°
                batch_embeddings = [
                    embedding_model.embed_query(text) for text in batch_texts
                ]

            all_embeddings.extend(batch_embeddings)

            if (i // batch_size + 1) % 10 == 0:
                logger.info(f"    ì§„í–‰: {i+len(batch_docs)}/{len(chunks)}")

        doc_embeddings = np.array(all_embeddings)
        embedding_time = time.time() - start_time
        logger.info(f"  - ì„ë² ë”© ì™„ë£Œ ({embedding_time:.2f}ì´ˆ)")

    except Exception as e:
        logger.error(f"  âœ— ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
        return EvaluationResult(model_name, chunk_strategy, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)

    # 2. ê° í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ í‰ê°€
    hit_rate_5_scores = []
    hit_rate_10_scores = []
    mrr_scores = []
    ndcg_5_scores = []
    ndcg_10_scores = []
    query_times = []

    for query_idx, test_query in enumerate(test_queries, 1):
        logger.info(
            f"  - ì¿¼ë¦¬ {query_idx}/{len(test_queries)}: '{test_query.query[:50]}...'"
        )

        # ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°
        relevant_indices = find_relevant_docs(chunks, test_query)

        if not relevant_indices:
            logger.warning(f"    ê²½ê³ : ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ")
            continue

        logger.info(f"    ê´€ë ¨ ë¬¸ì„œ: {len(relevant_indices)}ê°œ")

        # ì¿¼ë¦¬ ì„ë² ë”© ë° ê²€ìƒ‰
        try:
            start_time = time.time()
            query_embedding = np.array([embedding_model.embed_query(test_query.query)])
            retrieved_indices = retrieve_top_k(query_embedding, doc_embeddings, k=top_k)
            query_time = time.time() - start_time
            query_times.append(query_time)

            # ë©”íŠ¸ë¦­ ê³„ì‚°
            hit_rate_5 = calculate_hit_rate_at_k(relevant_indices, retrieved_indices, k=5)
            hit_rate_10 = calculate_hit_rate_at_k(relevant_indices, retrieved_indices, k=10)
            mrr = calculate_mrr(relevant_indices, retrieved_indices)
            ndcg_5 = calculate_ndcg_at_k(relevant_indices, retrieved_indices, k=5)
            ndcg_10 = calculate_ndcg_at_k(relevant_indices, retrieved_indices, k=10)

            hit_rate_5_scores.append(hit_rate_5)
            hit_rate_10_scores.append(hit_rate_10)
            mrr_scores.append(mrr)
            ndcg_5_scores.append(ndcg_5)
            ndcg_10_scores.append(ndcg_10)

            logger.info(
                f"    HR@5={hit_rate_5:.3f}, HR@10={hit_rate_10:.3f}, MRR={mrr:.3f}, "
                f"NDCG@5={ndcg_5:.3f}, NDCG@10={ndcg_10:.3f}"
            )

        except Exception as e:
            logger.error(f"    âœ— ì¿¼ë¦¬ í‰ê°€ ì‹¤íŒ¨: {e}")
            continue

    # 3. í‰ê·  ê³„ì‚°
    avg_hit_rate_5 = np.mean(hit_rate_5_scores) if hit_rate_5_scores else 0.0
    avg_hit_rate_10 = np.mean(hit_rate_10_scores) if hit_rate_10_scores else 0.0
    avg_mrr = np.mean(mrr_scores) if mrr_scores else 0.0
    avg_ndcg_5 = np.mean(ndcg_5_scores) if ndcg_5_scores else 0.0
    avg_ndcg_10 = np.mean(ndcg_10_scores) if ndcg_10_scores else 0.0
    avg_time = np.mean(query_times) if query_times else 0.0

    result = EvaluationResult(
        model_name=model_name,
        chunk_strategy=chunk_strategy,
        hit_rate_at_5=avg_hit_rate_5,
        hit_rate_at_10=avg_hit_rate_10,
        mrr=avg_mrr,
        ndcg_at_5=avg_ndcg_5,
        ndcg_at_10=avg_ndcg_10,
        avg_time=avg_time,
    )

    logger.info(f"[í‰ê°€ ì™„ë£Œ] {result}")
    return result


# ==================== ì²­í¬ ì „ëµ ê´€ë¦¬ ====================


def find_chunk_files(chunks_dir: str = "01_data/chunks") -> List[str]:
    """
    chunks ë””ë ‰í† ë¦¬ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì²­í¬ íŒŒì¼ ì°¾ê¸°
    _C suffix (clustering ì ìš©) íŒŒì¼ë§Œ ì‚¬ìš©

    Args:
        chunks_dir: ì²­í¬ ë””ë ‰í† ë¦¬ ê²½ë¡œ

    Returns:
        List[str]: ì²­í¬ íŒŒì¼ ê²½ë¡œ ëª©ë¡
    """
    import glob

    pattern = os.path.join(chunks_dir, "chunks_*_*_C.pkl")
    chunk_files = glob.glob(pattern)

    logger.info(f"[ì²­í¬ íŒŒì¼ íƒìƒ‰] {len(chunk_files)}ê°œ íŒŒì¼ ë°œê²¬")
    for f in chunk_files:
        logger.info(f"  - {os.path.basename(f)}")

    return sorted(chunk_files)


def extract_chunk_strategy(chunk_file_path: str) -> str:
    """
    ì²­í¬ íŒŒì¼ëª…ì—ì„œ ì „ëµ ì¶”ì¶œ (e.g., "chunks_100_15_C.pkl" -> "100_15")

    Args:
        chunk_file_path: ì²­í¬ íŒŒì¼ ê²½ë¡œ

    Returns:
        str: ì²­í¬ ì „ëµ (size_overlap)
    """
    import re

    basename = os.path.basename(chunk_file_path)
    match = re.search(r"chunks_(\d+)_(\d+)_C\.pkl", basename)
    if match:
        return f"{match.group(1)}_{match.group(2)}"
    return "unknown"


# ==================== ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„± ====================


def generate_markdown_report(
    results: List[EvaluationResult], output_path: str = "embedding_evaluation_report.md"
) -> None:
    """
    í‰ê°€ ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œë¡œ ìƒì„±

    Args:
        results: í‰ê°€ ê²°ê³¼ ëª©ë¡
        output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
    """
    from datetime import datetime

    with open(output_path, "w", encoding="utf-8") as f:
        # í—¤ë”
        f.write("# ì„ë² ë”© ëª¨ë¸ ë° ì²­í¬ ì „ëµ í‰ê°€ ë³´ê³ ì„œ\n\n")
        f.write(f"**ìƒì„± ë‚ ì§œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write("---\n\n")

        # í‰ê°€ ê°œìš”
        f.write("## 1. í‰ê°€ ê°œìš”\n\n")
        f.write("### í‰ê°€ ëª¨ë¸\n\n")
        unique_models = sorted(set(r.model_name for r in results))
        for i, model in enumerate(unique_models, 1):
            f.write(f"{i}. {model}\n")

        f.write("\n### í‰ê°€ ì²­í¬ ì „ëµ\n\n")
        unique_strategies = sorted(set(r.chunk_strategy for r in results))
        for strategy in unique_strategies:
            size, overlap = strategy.split("_")
            f.write(f"- **{strategy}**: chunk_size={size}, overlap={overlap}\n")

        f.write("\n### í‰ê°€ ë©”íŠ¸ë¦­\n\n")
        f.write("- **Hit Rate@K**: ìƒìœ„ Kê°œ ê²°ê³¼ ì¤‘ ìµœì†Œ 1ê°œì˜ ê´€ë ¨ ë¬¸ì„œ í¬í•¨ ë¹„ìœ¨ (0~1)\n")
        f.write("- **MRR (Mean Reciprocal Rank)**: ì²« ë²ˆì§¸ ê´€ë ¨ ë¬¸ì„œì˜ ì—­ìˆœìœ„ í‰ê· \n")
        f.write(
            "- **NDCG@K (Normalized Discounted Cumulative Gain)**: ìˆœìœ„ë¥¼ ê³ ë ¤í•œ ê²€ìƒ‰ í’ˆì§ˆ (0~1)\n"
        )
        f.write("- **Avg Time**: ì¿¼ë¦¬ë‹¹ í‰ê·  ê²€ìƒ‰ ì‹œê°„ (ì´ˆ)\n\n")

        f.write("---\n\n")

        # ì „ì²´ ê²°ê³¼ í‘œ
        f.write("## 2. ì „ì²´ í‰ê°€ ê²°ê³¼\n\n")
        f.write("| ëª¨ë¸ | ì²­í¬ ì „ëµ | HR@5 | HR@10 | MRR | NDCG@5 | NDCG@10 | Avg Time(s) |\n")
        f.write("|------|-----------|------|-------|-----|--------|---------|-------------|\n")

        for result in results:
            f.write(
                f"| {result.model_name} | {result.chunk_strategy} | "
                f"{result.hit_rate_at_5:.3f} | {result.hit_rate_at_10:.3f} | "
                f"{result.mrr:.3f} | {result.ndcg_at_5:.3f} | {result.ndcg_at_10:.3f} | "
                f"{result.avg_time:.2f} |\n"
            )

        f.write("\n---\n\n")

        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
        f.write("## 3. ìµœê³  ì„±ëŠ¥ ì¡°í•©\n\n")

        # NDCG@10 ê¸°ì¤€
        best_ndcg10 = max(results, key=lambda x: x.ndcg_at_10)
        f.write("### NDCG@10 ê¸°ì¤€ (ì¢…í•© ê²€ìƒ‰ í’ˆì§ˆ)\n\n")
        f.write(f"- **ëª¨ë¸**: {best_ndcg10.model_name}\n")
        f.write(f"- **ì²­í¬ ì „ëµ**: {best_ndcg10.chunk_strategy}\n")
        f.write(f"- **NDCG@10**: {best_ndcg10.ndcg_at_10:.3f}\n")
        f.write(f"- **HR@10**: {best_ndcg10.hit_rate_at_10:.3f}\n")
        f.write(f"- **MRR**: {best_ndcg10.mrr:.3f}\n")
        f.write(f"- **í‰ê·  ê²€ìƒ‰ ì‹œê°„**: {best_ndcg10.avg_time:.2f}ì´ˆ\n\n")

        # Hit Rate@10 ê¸°ì¤€
        best_hr10 = max(results, key=lambda x: x.hit_rate_at_10)
        f.write("### Hit Rate@10 ê¸°ì¤€ (ê²€ìƒ‰ ì¬í˜„ìœ¨)\n\n")
        f.write(f"- **ëª¨ë¸**: {best_hr10.model_name}\n")
        f.write(f"- **ì²­í¬ ì „ëµ**: {best_hr10.chunk_strategy}\n")
        f.write(f"- **HR@10**: {best_hr10.hit_rate_at_10:.3f}\n")
        f.write(f"- **NDCG@10**: {best_hr10.ndcg_at_10:.3f}\n")
        f.write(f"- **MRR**: {best_hr10.mrr:.3f}\n\n")

        # MRR ê¸°ì¤€
        best_mrr = max(results, key=lambda x: x.mrr)
        f.write("### MRR ê¸°ì¤€ (ì²« ë²ˆì§¸ ê´€ë ¨ ë¬¸ì„œ ìˆœìœ„)\n\n")
        f.write(f"- **ëª¨ë¸**: {best_mrr.model_name}\n")
        f.write(f"- **ì²­í¬ ì „ëµ**: {best_mrr.chunk_strategy}\n")
        f.write(f"- **MRR**: {best_mrr.mrr:.3f}\n")
        f.write(f"- **NDCG@10**: {best_mrr.ndcg_at_10:.3f}\n\n")

        # ê°€ì¥ ë¹ ë¥¸ ëª¨ë¸
        fastest = min(results, key=lambda x: x.avg_time)
        f.write("### ê²€ìƒ‰ ì†ë„ (ê°€ì¥ ë¹ ë¦„)\n\n")
        f.write(f"- **ëª¨ë¸**: {fastest.model_name}\n")
        f.write(f"- **ì²­í¬ ì „ëµ**: {fastest.chunk_strategy}\n")
        f.write(f"- **í‰ê·  ê²€ìƒ‰ ì‹œê°„**: {fastest.avg_time:.2f}ì´ˆ\n")
        f.write(f"- **NDCG@10**: {fastest.ndcg_at_10:.3f}\n\n")

        f.write("---\n\n")

        # ì²­í¬ ì „ëµë³„ ë¹„êµ
        f.write("## 4. ì²­í¬ ì „ëµë³„ ì„±ëŠ¥ ë¹„êµ\n\n")
        for strategy in unique_strategies:
            strategy_results = [r for r in results if r.chunk_strategy == strategy]
            if not strategy_results:
                continue

            f.write(f"### ì²­í¬ ì „ëµ: {strategy}\n\n")
            f.write("| ëª¨ë¸ | HR@10 | NDCG@10 | MRR |\n")
            f.write("|------|-------|---------|-----|\n")

            for result in sorted(strategy_results, key=lambda x: x.ndcg_at_10, reverse=True):
                f.write(
                    f"| {result.model_name} | {result.hit_rate_at_10:.3f} | "
                    f"{result.ndcg_at_10:.3f} | {result.mrr:.3f} |\n"
                )

            f.write("\n")

        f.write("---\n\n")

        # ëª¨ë¸ë³„ ë¹„êµ
        f.write("## 5. ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ\n\n")
        for model in unique_models:
            model_results = [r for r in results if r.model_name == model]
            if not model_results:
                continue

            f.write(f"### ëª¨ë¸: {model}\n\n")
            f.write("| ì²­í¬ ì „ëµ | HR@10 | NDCG@10 | MRR |\n")
            f.write("|-----------|-------|---------|-----|\n")

            for result in sorted(model_results, key=lambda x: x.ndcg_at_10, reverse=True):
                f.write(
                    f"| {result.chunk_strategy} | {result.hit_rate_at_10:.3f} | "
                    f"{result.ndcg_at_10:.3f} | {result.mrr:.3f} |\n"
                )

            f.write("\n")

        f.write("---\n\n")

        # ê¶Œì¥ì‚¬í•­
        f.write("## 6. ê¶Œì¥ì‚¬í•­\n\n")
        f.write("### í”„ë¡œë•ì…˜ ì‚¬ìš© ê¶Œì¥\n\n")
        f.write(
            f"**ì¢…í•© í‰ê°€ ìµœê³  ì„±ëŠ¥**: `{best_ndcg10.model_name}` + `chunk_strategy={best_ndcg10.chunk_strategy}`\n\n"
        )

        # ì„±ëŠ¥-ì†ë„ íŠ¸ë ˆì´ë“œì˜¤í”„ ë¶„ì„
        f.write("### ì„±ëŠ¥-ì†ë„ íŠ¸ë ˆì´ë“œì˜¤í”„\n\n")

        # ìƒìœ„ 3ê°œ NDCG@10 ëª¨ë¸ ì„ ë³„
        top3_ndcg = sorted(results, key=lambda x: x.ndcg_at_10, reverse=True)[:3]

        f.write("**ìƒìœ„ 3ê°œ ì¡°í•© (NDCG@10 ê¸°ì¤€)**:\n\n")
        for i, result in enumerate(top3_ndcg, 1):
            f.write(
                f"{i}. **{result.model_name} + {result.chunk_strategy}** - "
                f"NDCG@10={result.ndcg_at_10:.3f}, Time={result.avg_time:.2f}s\n"
            )

        f.write("\n")

        # ì†ë„ ìš°ì„ ì‹œ
        fast_and_good = [r for r in results if r.avg_time < 0.1 and r.ndcg_at_10 > 0.5]
        if fast_and_good:
            best_fast = max(fast_and_good, key=lambda x: x.ndcg_at_10)
            f.write("**ì†ë„ ìš°ì„  (ê²€ìƒ‰ ì‹œê°„ < 0.1ì´ˆ && NDCG@10 > 0.5)**:\n\n")
            f.write(
                f"- {best_fast.model_name} + {best_fast.chunk_strategy} - "
                f"NDCG@10={best_fast.ndcg_at_10:.3f}, Time={best_fast.avg_time:.2f}s\n\n"
            )

        f.write("---\n\n")
        f.write(
            "*ì´ ë³´ê³ ì„œëŠ” `evaluate_embeddings.py`ì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*\n"
        )

    logger.info(f"\n[ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ] {output_path}")


# ==================== ë©”ì¸ ì‹¤í–‰ ====================


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - ì—¬ëŸ¬ ì²­í¬ ì „ëµê³¼ ì„ë² ë”© ëª¨ë¸ ì¡°í•© í‰ê°€"""
    logger.info("=" * 80)
    logger.info("ì„ë² ë”© ëª¨ë¸ ë° ì²­í¬ ì „ëµ ë¹„êµ í‰ê°€ ì‹œì‘")
    logger.info("=" * 80)

    # 1. ì²­í¬ íŒŒì¼ íƒìƒ‰
    chunk_files = find_chunk_files("01_data/chunks")

    if not chunk_files:
        logger.error("ì²­í¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        return

    # 2. ëª¨ë¸ ì´ˆê¸°í™” (í•œ ë²ˆë§Œ)
    models = init_models()

    if not models:
        logger.error("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!")
        return

    # 3. ëª¨ë“  ì¡°í•© í‰ê°€
    all_results = []

    for chunk_file_path in chunk_files:
        chunk_strategy = extract_chunk_strategy(chunk_file_path)
        logger.info(f"\n{'=' * 80}")
        logger.info(f"ì²­í¬ ì „ëµ: {chunk_strategy}")
        logger.info(f"íŒŒì¼: {os.path.basename(chunk_file_path)}")
        logger.info(f"{'=' * 80}")

        # ì²­í¬ ë°ì´í„° ë¡œë“œ
        try:
            with open(chunk_file_path, "rb") as f:
                chunks = pickle.load(f)
            logger.info(f"  - ì´ ì²­í¬ ìˆ˜: {len(chunks)}")
        except Exception as e:
            logger.error(f"  âœ— ì²­í¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
            continue

        # ìƒ˜í”Œë§ (í‰ê°€ ì‹œê°„ ë‹¨ì¶•)
        # ì „ì²´ í‰ê°€ë¥¼ ì›í•˜ë©´ SAMPLE_SIZE = None ì„¤ì •
        SAMPLE_SIZE = None  # ì „ì²´ ë°ì´í„° í‰ê°€
        if SAMPLE_SIZE and len(chunks) > SAMPLE_SIZE:
            logger.warning(f"  - í‰ê°€ ì‹œê°„ ë‹¨ì¶•ì„ ìœ„í•´ {SAMPLE_SIZE}ê°œ ìƒ˜í”Œë§")
            np.random.seed(42)
            sample_indices = np.random.choice(len(chunks), SAMPLE_SIZE, replace=False)
            chunks = [chunks[i] for i in sample_indices]

        # ê° ëª¨ë¸ í‰ê°€
        for model_name, embedding_model in models.items():
            try:
                result = evaluate_model(
                    model_name=model_name,
                    embedding_model=embedding_model,
                    chunks=chunks,
                    test_queries=TEST_QUERIES,
                    chunk_strategy=chunk_strategy,
                    top_k=10,
                )
                all_results.append(result)
            except Exception as e:
                logger.error(f"ëª¨ë¸ {model_name} í‰ê°€ ì‹¤íŒ¨: {e}")

    # 4. ê²°ê³¼ ì¶œë ¥ (í„°ë¯¸ë„)
    logger.info("\n" + "=" * 80)
    logger.info("ìµœì¢… í‰ê°€ ê²°ê³¼")
    logger.info("=" * 80)

    # ì •ë ¬ (NDCG@10 ê¸°ì¤€)
    all_results.sort(key=lambda x: x.ndcg_at_10, reverse=True)

    print(
        "\n{:<15} {:<10} {:>8} {:>8} {:>8} {:>8} {:>8} {:>10}".format(
            "Model", "Chunk", "HR@5", "HR@10", "MRR", "NDCG@5", "NDCG@10", "Time(s)"
        )
    )
    print("-" * 90)

    for result in all_results:
        print(
            "{:<15} {:<10} {:>8.3f} {:>8.3f} {:>8.3f} {:>8.3f} {:>8.3f} {:>10.2f}".format(
                result.model_name,
                result.chunk_strategy,
                result.hit_rate_at_5,
                result.hit_rate_at_10,
                result.mrr,
                result.ndcg_at_5,
                result.ndcg_at_10,
                result.avg_time,
            )
        )

    # ìµœê³  ì„±ëŠ¥ ì¡°í•©
    if all_results:
        best_model = all_results[0]
        logger.info(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ì¡°í•© (NDCG@10 ê¸°ì¤€):")
        logger.info(f"   - ëª¨ë¸: {best_model.model_name}")
        logger.info(f"   - ì²­í¬ ì „ëµ: {best_model.chunk_strategy}")
        logger.info(f"   - NDCG@10: {best_model.ndcg_at_10:.3f}")
        logger.info(f"   - Hit Rate@10: {best_model.hit_rate_at_10:.3f}")
        logger.info(f"   - MRR: {best_model.mrr:.3f}")
        logger.info(f"   - í‰ê·  ê²€ìƒ‰ ì‹œê°„: {best_model.avg_time:.2f}ì´ˆ")

    # 5. ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„±
    if all_results:
        report_path = "embedding_evaluation_report.md"
        generate_markdown_report(all_results, report_path)
        logger.info(f"\nâœ“ ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {report_path}")


if __name__ == "__main__":
    main()
