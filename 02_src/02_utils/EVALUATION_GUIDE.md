# 임베딩 모델 평가 가이드

## 개요

`evaluate_embeddings.py`는 여러 임베딩 모델과 청크 전략 조합을 자동으로 평가하여 최적의 조합을 찾아주는 스크립트입니다.

## 주요 기능

### 1. 평가 대상
- **7개 임베딩 모델** 자동 평가
- **여러 청크 전략** 자동 탐색 및 평가 (`01_data/chunks/*_C.pkl`)
- 모든 가능한 조합 테스트

### 2. 평가 메트릭
| 메트릭 | 설명 | 범위 |
|--------|------|------|
| **Hit Rate@K** | 상위 K개 결과 중 최소 1개의 관련 문서 포함 비율 | 0~1 |
| **MRR** | Mean Reciprocal Rank - 첫 번째 관련 문서의 역순위 평균 | 0~1 |
| **NDCG@K** | Normalized Discounted Cumulative Gain - 순위를 고려한 검색 품질 | 0~1 |
| **Avg Time** | 쿼리당 평균 검색 시간 | 초 |

### 3. 출력
- **터미널**: 결과 테이블 실시간 출력
- **마크다운 보고서**: `embedding_evaluation_report.md` 자동 생성
  - 전체 결과 표
  - 최고 성능 조합 (4가지 기준)
  - 청크 전략별 성능 비교
  - 모델별 성능 비교
  - 프로덕션 권장사항

## 실행 시간 예상

### 샘플링 모드 (기본값: 2000개 청크)
- **청크 전략 1개 × 모델 7개**: 약 10~20분
- **청크 전략 6개 × 모델 7개**: 약 1~2시간

### 전체 평가 모드
전체 평가를 원하면 `main()` 함수의 `SAMPLE_SIZE` 변수를 수정하세요:

```python
# evaluate_embeddings.py 769번째 줄
SAMPLE_SIZE = None  # 전체 평가 (시간 오래 걸림)
# SAMPLE_SIZE = 2000  # 2000개 샘플링 (기본값)
```

- **청크 전략 1개 × 모델 7개**: 약 30분~1시간
- **청크 전략 6개 × 모델 7개**: 약 3~6시간

## 결과 해석

### 1. 터미널 출력 예시

```
Model           Chunk       HR@5   HR@10     MRR  NDCG@5 NDCG@10    Time(s)
----------------------------------------------------------------------------------
MPNet           200_30     0.900   0.950   0.650   0.720    0.780       0.05
BGE-M3          100_15     0.890   0.940   0.640   0.710    0.770       0.08
...
```

### 2. 마크다운 보고서

`embedding_evaluation_report.md` 파일에서 다음 정보 확인:

#### 최고 성능 조합 (4가지 기준)
1. **NDCG@10 기준**: 종합 검색 품질이 가장 높은 조합
2. **Hit Rate@10 기준**: 검색 재현율이 가장 높은 조합
3. **MRR 기준**: 첫 번째 관련 문서가 가장 상위에 나타나는 조합
4. **검색 속도**: 가장 빠른 조합

#### 권장사항
- **프로덕션 사용**: NDCG@10 최고 성능 조합 추천
- **성능-속도 트레이드오프**: 상위 3개 조합 비교
- **속도 우선**: 빠르면서도 성능이 좋은 조합

## 평가 프로세스

스크립트 실행 시 다음 단계가 자동으로 진행됩니다:

```
1. 청크 파일 탐색 (01_data/chunks/*_C.pkl)
   ↓
2. 임베딩 모델 7개 초기화
   ↓
3. 각 청크 전략에 대해:
   - 청크 데이터 로드
   - (선택적) 샘플링
   - 각 모델로 평가:
     * 문서 임베딩 생성 (배치)
     * 테스트 쿼리 10개 실행
     * 메트릭 계산 (HR@5, HR@10, MRR, NDCG@5, NDCG@10)
   ↓
4. 결과 정렬 및 출력 (NDCG@10 기준)
   ↓
5. 마크다운 보고서 생성
```

## 커스터마이징

### 1. 테스트 쿼리 수정

`TEST_QUERIES` 변수에서 평가용 쿼리를 수정할 수 있습니다 (84~125번째 줄):

```python
TEST_QUERIES = [
    TestQuery(
        query="your custom query",
        relevant_keywords=["keyword1", "keyword2"],
    ),
    # ...
]
```

### 2. 평가 모델 선택

특정 모델만 평가하려면 `init_models()` 함수를 수정하세요 (130~219번째 줄).

### 3. 샘플링 크기 조정

`main()` 함수에서 `SAMPLE_SIZE` 변수 수정 (769번째 줄):

```python
SAMPLE_SIZE = 1000  # 1000개만 샘플링 (더 빠름)
SAMPLE_SIZE = None  # 전체 평가 (시간 오래 걸림)
```

### 4. Top-K 값 변경

`main()` 함수에서 `top_k` 파라미터 수정 (779번째 줄):

```python
result = evaluate_model(
    # ...
    top_k=20,  # 상위 20개 검색 (기본값: 10)
)
```

## 문제 해결

### 문제: "청크 파일을 찾을 수 없습니다"
- `01_data/chunks/` 디렉토리에 `*_C.pkl` 파일이 있는지 확인
- 먼저 `chunking.py`와 `clustering.py`를 실행하여 청크 파일 생성

### 문제: "OPENAI_API_KEY not found"
- `.env` 파일에 `OPENAI_API_KEY` 설정 확인
- OpenAI 모델 없이 평가하려면 경고를 무시해도 됩니다

### 문제: GPU 메모리 부족
- `evaluate_model()` 함수의 `batch_size` 축소 (352번째 줄):
  ```python
  batch_size = 50  # 기본값: 100
  ```

### 문제: 실행 시간이 너무 오래 걸림
- `SAMPLE_SIZE`를 더 작게 설정 (예: 500 또는 1000)
- 평가할 청크 파일을 줄이기 (일부 파일만 남기고 삭제)

## 예시: 빠른 테스트 실행

빠른 테스트를 위해 다음과 같이 설정하세요:

1. **청크 파일 1개만 남기기**:
   ```bash
   # 예: chunks_100_15_C.pkl만 평가
   # 나머지 파일을 임시로 다른 디렉토리로 이동
   ```

2. **샘플링 크기 축소** (main() 함수):
   ```python
   SAMPLE_SIZE = 500  # 500개만 샘플링
   ```

3. **실행**:
   ```bash
   python 02_src/02_utils/evaluate_embeddings.py
   ```

예상 소요 시간: 약 5~10분

## 참고 사항

- 첫 실행 시 임베딩 모델을 다운로드하므로 시간이 더 걸릴 수 있습니다.
- GPU를 사용할 수 있으면 평가 속도가 훨씬 빠릅니다.
- 평가 결과는 매번 동일합니다 (random seed 고정).
- 마크다운 보고서는 매 실행 시 덮어씌워집니다.

## 추가 정보

- 스크립트 파일: `02_src/02_utils/evaluate_embeddings.py`
- 보고서 출력: `embedding_evaluation_report.md` (프로젝트 루트)
- 로그 레벨: INFO (상세한 진행 상황 출력)
