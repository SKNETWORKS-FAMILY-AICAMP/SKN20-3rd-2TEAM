{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62f70cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Playdata2\\miniconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "질문: RAG란?\n",
      "\n",
      "[답변]\n",
      "\n",
      "1) RAG(정보 검색 증강 생성)는 정보 검색과 생성 모델을 결합하여 더 정확하고 풍부한 응답을 생성하는 방법입니다.\n",
      "\n",
      "2) Key insights:\n",
      "   - RAG는 정보 검색(retrieval)과 생성(generation) 과정을 통합하여, 사용자가 요청한 정보에 대한 더 나은 응답을 제공합니다.\n",
      "   - 이 시스템은 외부 데이터베이스에서 관련 정보를 검색한 후, 이를 바탕으로 자연어 응답을 생성합니다.\n",
      "   - RAG는 특히 대화형 AI, 질문 응답 시스템, 그리고 정보 검색이 중요한 애플리케이션에서 유용합니다.\n",
      "   - 최근 연구에서는 RAG 시스템의 신뢰성을 높이기 위한 방법들이 제안되고 있습니다.\n",
      "\n",
      "3) Related papers:\n",
      "   - [GroupRank: A Groupwise Reranking Paradigm Driven by Reinforcement Learning](https://huggingface.co/papers/2511.11653)\n",
      "   - [A Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain](https://huggingface.co/papers/2511.07577)\n",
      "\n",
      "4) Detailed explanation:\n",
      "   RAG는 \"Retrieval-Augmented Generation\"의 약자로, 정보 검색과 생성 모델을 결합한 접근 방식입니다. 이 방법은 사용자가 질문을 했을 때, 먼저 관련된 정보를 외부 데이터베이스에서 검색한 후, 그 정보를 바탕으로 자연어로 응답을 생성합니다. 이러한 방식은 단순한 생성 모델보다 더 정확하고 풍부한 정보를 제공할 수 있습니다. 최근에는 RAG 시스템의 신뢰성을 높이기 위해 블록체인 기술을 활용한 분산형 RAG 시스템(dRAG)과 같은 새로운 접근 방식이 연구되고 있습니다.\n",
      "\n",
      "5) Sources summary:\n",
      "   - 첫 번째 논문은 RAG 시스템을 위한 그룹 기반 재순위화 방법을 제안하고 있습니다.\n",
      "   - 두 번째 논문은 블록체인 기술을 활용하여 데이터의 신뢰성을 확보한 분산형 RAG 시스템을 다루고 있습니다.\n",
      "\n",
      "[출처]\n",
      "\n",
      "- [1] GroupRank: A Groupwise Reranking Paradigm Driven by Reinforcement Learning\n",
      "  HF: https://huggingface.co/papers/2511.11653\n",
      "  GitHub: https://github.com/AQ-MedAI/Diver\n",
      "  tags: document, group, listwise\n",
      "  upvote: 54\n",
      "- [2] GroupRank: A Groupwise Reranking Paradigm Driven by Reinforcement Learning\n",
      "  HF: https://huggingface.co/papers/2511.11653\n",
      "  GitHub: https://github.com/AQ-MedAI/Diver\n",
      "  tags: document, group, listwise\n",
      "  upvote: 54\n",
      "- [3] A Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain\n",
      "  HF: https://huggingface.co/papers/2511.07577\n",
      "  GitHub: https://github.com/yining610/Reliable-dRAG\n",
      "  tags: system, source, decentralized\n",
      "  upvote: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import pickle    # chunk, vectorDB 저장한것 사용\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 경고메세지 삭제\n",
    "warnings.filterwarnings('ignore')\n",
    "load_dotenv()\n",
    "\n",
    "# openapi key 확인\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not api_key:\n",
    "    raise ValueError('.env확인,  key없음')\n",
    "\n",
    "# 필수 라이브러리 로드\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class SimpleRAGSystem:\n",
    "    '''간단한 RAG 시스템 래퍼 클래스'''\n",
    "    def __init__(self, vectorstore, llm, retriever_k=3):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.llm = llm\n",
    "        self.retriever = vectorstore.as_retriever(search_type = 'similarity', search_kwargs={'k':retriever_k})\n",
    "        # self.retriever_chain = self._retriever_basic_chain()\n",
    "        self.chain = self._build_chain()\n",
    "    \n",
    "\n",
    "    def _build_chain(self): ### ---------> 최종 사용자에게 전달되는 프롬프트 수정\n",
    "        '''RAG 체인 구성''' \n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "                (\"system\",     \"\"\"\n",
    "    You are **\"AI Tech Trend Navigator\"**, an expert assistant for AI/ML research papers.\n",
    "\n",
    "    [Role]\n",
    "    - You help users understand and leverage recent AI/ML papers collected from HuggingFace DailyPapers.\n",
    "    - Your main goals are:\n",
    "      - Summarize and compare relevant papers clearly.\n",
    "      - Explain core ideas in simple terms.\n",
    "      - Highlight practical use-cases and implications for real-world services or products.\n",
    "\n",
    "    [Inputs]\n",
    "    The system provides:\n",
    "    - user_question: the user’s question.\n",
    "     - context: a set of retrieved documents, formatted as a single text block.\n",
    "        - Sometimes the context may be exactly the string \"NO_RELEVANT_PAPERS\".\n",
    "      - page_content: main text (abstract or summary)\n",
    "      - metadata:\n",
    "        - paper_name\n",
    "        - github_url (optional)\n",
    "        - huggingface_url (optional)\n",
    "        - upvote (integer, popularity signal)\n",
    "        - tags: list of keywords\n",
    "        - year, week, and other fields.\n",
    "\n",
    "    You must rely only on:\n",
    "    - the given context, and\n",
    "    - general, high-level AI/ML knowledge.\n",
    "    Do NOT invent specific paper titles, authors, datasets, metrics, or numerical results\n",
    "    that are not supported by the context.\n",
    "\n",
    "\n",
    "    [Context Handling]\n",
    "    - If the context is **\"NO_RELEVANT_PAPERS\"**, it means:\n",
    "    - The retrieval system could not find any clearly relevant papers.\n",
    "    - In this case, you may answer **purely from your own general AI/ML knowledge**.\n",
    "    - Do NOT fabricate specific paper titles, authors, datasets, or numerical results.\n",
    "    - You may skip the \"Related papers\" section or keep it very generic.\n",
    "\n",
    "    - If the context contains one or more papers:\n",
    "    - Prefer to base your answer on those papers.\n",
    "    - Use only the papers that are reasonably related to the user’s question.\n",
    "                 \n",
    "    [Main Tasks]\n",
    "\n",
    "    1. Understand the user’s intent\n",
    "       - Roughly classify the question as one of:\n",
    "         - (a) concept/background explanation\n",
    "         - (b) single-paper summary\n",
    "         - (c) comparison or trend analysis across multiple papers\n",
    "         - (d) practical application and use-case ideas\n",
    "       - If the intent is ambiguous, make a reasonable assumption and continue.\n",
    "         You may briefly state what you assumed.\n",
    "\n",
    "    2. Use only the relevant papers\n",
    "       - Focus on the most relevant 1–3 papers in the given context.\n",
    "       - If some papers look only weakly related to the question, you may ignore them.\n",
    "       - If nothing is clearly relevant, say that the context does not directly answer the question.\n",
    "\n",
    "    3. Summarize each selected paper\n",
    "       For each paper you rely on, briefly cover:\n",
    "       - What problem it tries to solve.\n",
    "       - What approach/model/idea it uses.\n",
    "       - What seems new or strong compared to typical or baseline methods.\n",
    "       - Any obvious limitations, trade-offs, or caveats that are visible from the context.\n",
    "\n",
    "    4. Produce a synthesized answer\n",
    "       - Do not just list papers. Synthesize them to directly answer the user’s question.\n",
    "       - When possible, cover:\n",
    "         - Common themes or trends across the papers.\n",
    "         - How these ideas relate to topics such as RAG, long-context, multimodal models, etc.,\n",
    "           when relevant.\n",
    "         - How someone could apply these ideas in a real-world project, prototype, or product.\n",
    "\n",
    "    5. Be honest about uncertainty\n",
    "       - If the given context is not enough to answer precisely, say so.\n",
    "       - Suggest what extra information, papers, or queries would be helpful.\n",
    "\n",
    "    [Style]\n",
    "    - Answer in the SAME LANGUAGE as the user’s question.\n",
    "      (If the question is in Korean, answer in Korean. If it is in English, answer in English.)\n",
    "    - Prefer clear, concise sentences over heavy academic wording.\n",
    "    - Briefly explain technical terms when needed.\n",
    "    - Never fabricate paper titles, authors, datasets, or numerical results.\n",
    "    \"\"\"),\n",
    "                \n",
    "    (\"human\", \"\"\"\n",
    "    [QUESTION]\n",
    "    {question}\n",
    "\n",
    "     [Context]\n",
    "    The following CONTEXT block may contain 0 or more papers. \n",
    "    If it is \"NO_RELEVANT_PAPERS\", please answer from your general AI/ML knowledge.\n",
    "     \n",
    "    [CONTEXT]\n",
    "    ======== START ========\n",
    "    {context}\n",
    "    ======== END =========\n",
    "\n",
    "    Please structure your answer as follows (flexible, but try to follow this):\n",
    "\n",
    "    1) One-line summary  \n",
    "    2) Key insights (3-6 bullets)  \n",
    "    3) Related papers (top 1~3)  \n",
    "    4) Detailed explanation  \n",
    "    5) Sources summary\n",
    "\n",
    "    ⚠ Do not hallucinate papers or details not shown in context.\n",
    "    Respond in the same language as the question.\n",
    "    \"\"\")\n",
    "            ])\n",
    "        return (\n",
    "            {\n",
    "                \"context\": self.retriever | self._format_docs,\n",
    "                \"question\": RunnablePassthrough(),\n",
    "                \"chat_history\": lambda x: \"\"\n",
    "            }\n",
    "            | prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _format_docs(docs):\n",
    "        \"\"\"retriever가 반환한 Document들을 프롬프트용 텍스트로 변환\"\"\"\n",
    "        if not docs:\n",
    "            # ⚠️ 컨텍스트가 전혀 없을 때는 이 문자열로 보냄\n",
    "            return \"NO_RELEVANT_PAPERS\"\n",
    "\n",
    "        lines = []\n",
    "        for i, doc in enumerate(docs, start=1):\n",
    "            md = doc.metadata or {}\n",
    "\n",
    "            # tag1, tag2, tag3 → tags 리스트로 재구성\n",
    "            tags = [\n",
    "                md.get(\"tag1\"),\n",
    "                md.get(\"tag2\"),\n",
    "                md.get(\"tag3\"),\n",
    "            ]\n",
    "            tags = [t for t in tags if t]  # None/빈 값 제거\n",
    "\n",
    "            tag_str = f\"tags: {', '.join(tags)}\" if tags else \"tags: (none)\"\n",
    "\n",
    "            paper_name = md.get(\"paper_name\", \"(no title)\")\n",
    "            hf_url = md.get(\"huggingface_url\", \"\")\n",
    "            gh_url = md.get(\"github_url\", \"\")\n",
    "\n",
    "            link_lines = []\n",
    "            if hf_url:\n",
    "                link_lines.append(f\"HuggingFace: {hf_url}\")\n",
    "            if gh_url:\n",
    "                link_lines.append(f\"GitHub: {gh_url}\")\n",
    "            links_block = \"\\n\".join(link_lines) if link_lines else \"\"\n",
    "\n",
    "            block = f\"\"\"[{i}] {paper_name}\n",
    "                                {tag_str}\n",
    "                                {links_block}\n",
    "\n",
    "                                {doc.page_content}\"\"\"\n",
    "            lines.append(block)\n",
    "\n",
    "        return \"\\n\\n\".join(lines)\n",
    "\n",
    "\n",
    "    def ask(self, question:str) -> str:\n",
    "        '''질문에 답변'''\n",
    "        return self.chain.invoke(question)\n",
    "    \n",
    "\n",
    "    def ask_with_sources(self, question: str) -> dict:\n",
    "        \"\"\"질문에 답변 + 출처 반환\"\"\"\n",
    "        answer = self.chain.invoke(question)\n",
    "        source_docs = self.retriever.invoke(question)\n",
    "\n",
    "        sources = []\n",
    "        for doc in source_docs:\n",
    "            md = doc.metadata or {}\n",
    "            tags = [\n",
    "                md.get(\"tag1\"),\n",
    "                md.get(\"tag2\"),\n",
    "                md.get(\"tag3\"),\n",
    "            ]\n",
    "            tags = [t for t in tags if t]\n",
    "\n",
    "            sources.append(\n",
    "                {\n",
    "                    \"paper_name\": md.get(\"paper_name\", \"(no title)\"),\n",
    "                    \"huggingface_url\": md.get(\"huggingface_url\"),\n",
    "                    \"github_url\": md.get(\"github_url\"),\n",
    "                    \"upvote\": md.get(\"upvote\"),\n",
    "                    \"tags\": tags,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"sources\": sources,\n",
    "        }\n",
    "   \n",
    "\n",
    "\n",
    "if __name__ == '__main__' :\n",
    "    # chunk 파일로 임시 확인\n",
    "    def get_project_root():\n",
    "        curr = Path().resolve()\n",
    "        for parent in [curr] + list(curr.parents):\n",
    "            if (parent / \".git\").exists():\n",
    "                return parent\n",
    "        raise FileNotFoundError(\"프로젝트 루트 찾기 실패\")\n",
    "\n",
    "    PROJECT_ROOT = get_project_root()\n",
    "    DATA_DIR = PROJECT_ROOT / \"01_data/chunks\"\n",
    "\n",
    "    chunks_path = DATA_DIR / \"chunks_all.pkl\"\n",
    "\n",
    "    with open(chunks_path, \"rb\") as f:\n",
    "        chunks = pickle.load(f)\n",
    "\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        collection_name='test',\n",
    "        embedding=OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "    )\n",
    "\n",
    "  \n",
    "    llm = ChatOpenAI( model = 'gpt-4o-mini', temperature=0 )\n",
    "\n",
    "    rag_system = SimpleRAGSystem(vectorstore, llm)\n",
    "    user_question = \"RAG란?\"\n",
    "    result = rag_system.ask_with_sources(user_question)\n",
    "\n",
    "    print(f\"질문: {user_question}\")\n",
    "    print(\"\\n[답변]\\n\")\n",
    "    print(result[\"answer\"])\n",
    "\n",
    "    print(\"\\n[출처]\\n\")\n",
    "    for i, src in enumerate(result[\"sources\"], start=1):\n",
    "        print(f\"- [{i}] {src['paper_name']}\")\n",
    "        if src[\"huggingface_url\"]:\n",
    "            print(f\"  HF: {src['huggingface_url']}\")\n",
    "        if src[\"github_url\"]:\n",
    "            print(f\"  GitHub: {src['github_url']}\")\n",
    "        if src[\"tags\"]:\n",
    "            print(f\"  tags: {', '.join(src['tags'])}\")\n",
    "        if src[\"upvote\"] is not None:\n",
    "            print(f\"  upvote: {src['upvote']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccbca50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 벡터DB가 뭐야?\n",
      "\n",
      "[답변]\n",
      "\n",
      "1) 벡터DB는 고차원 벡터를 저장하고 검색하는 데이터베이스로, 주로 머신러닝과 AI에서 사용됩니다.\n",
      "\n",
      "2) Key insights:\n",
      "   - 벡터DB는 데이터 포인트를 벡터 형태로 표현하여 유사성을 기반으로 검색할 수 있게 합니다.\n",
      "   - 주로 이미지, 텍스트, 오디오와 같은 비정형 데이터의 검색에 활용됩니다.\n",
      "   - 고속 검색을 위해 인덱싱 기법을 사용하며, 대규모 데이터셋에서도 효율적으로 작동합니다.\n",
      "   - 최근 AI 모델들이 생성하는 임베딩을 저장하고 검색하는 데 필수적입니다.\n",
      "   - 예를 들어, 추천 시스템이나 이미지 검색 엔진에서 사용됩니다.\n",
      "\n",
      "3) Related papers:\n",
      "   - 현재 제공된 문서에서는 벡터DB와 직접적으로 관련된 논문이 없습니다.\n",
      "\n",
      "4) Detailed explanation:  \n",
      "벡터DB는 데이터베이스의 한 종류로, 데이터 포인트를 고차원 벡터로 변환하여 저장합니다. 이러한 벡터는 머신러닝 모델, 특히 딥러닝 모델에서 생성된 임베딩을 기반으로 하며, 데이터 간의 유사성을 측정하는 데 사용됩니다. 예를 들어, 이미지 검색 시스템에서는 사용자가 입력한 이미지와 유사한 이미지를 찾기 위해 벡터DB에서 유사한 벡터를 검색합니다. 벡터DB는 대량의 데이터를 빠르게 처리할 수 있도록 설계되어 있으며, 인덱싱 기법을 통해 검색 속도를 높입니다. 이러한 특성 덕분에 벡터DB는 추천 시스템, 자연어 처리, 이미지 인식 등 다양한 AI 응용 분야에서 필수적인 요소로 자리 잡고 있습니다.\n",
      "\n",
      "5) Sources summary:  \n",
      "제공된 문서에서는 벡터DB와 관련된 논문이 없으나, 벡터DB의 개념과 활용에 대한 일반적인 지식을 바탕으로 설명하였습니다. 벡터DB는 AI와 머신러닝에서 데이터 검색 및 유사성 측정에 중요한 역할을 합니다.\n",
      "\n",
      "[출처]\n",
      "\n",
      "- [1] Visual Sync: Multi-Camera Synchronization via Cross-View Object Motion\n",
      "  HF: https://huggingface.co/papers/2512.02017\n",
      "  GitHub: https://github.com/stevenlsw/visualsync\n",
      "  tags: visualsync, view, camera\n",
      "  upvote: 3\n",
      "- [2] Visual Sync: Multi-Camera Synchronization via Cross-View Object Motion\n",
      "  HF: https://huggingface.co/papers/2512.02017\n",
      "  GitHub: https://github.com/stevenlsw/visualsync\n",
      "  tags: visualsync, view, camera\n",
      "  upvote: 3\n",
      "- [3] DigiData: Training and Evaluating General-Purpose Mobile Control Agents\n",
      "  HF: https://huggingface.co/papers/2511.07413\n",
      "  GitHub: https://github.com/facebookresearch/digidata\n",
      "  tags: control agent, keyword2, keyword3\n",
      "  upvote: 5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import pickle    # chunk, vectorDB 저장한것 사용\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 경고메세지 삭제\n",
    "warnings.filterwarnings('ignore')\n",
    "load_dotenv()\n",
    "\n",
    "# openapi key 확인\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not api_key:\n",
    "    raise ValueError('.env확인,  key없음')\n",
    "\n",
    "# 필수 라이브러리 로드\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class SimpleRAGSystem:\n",
    "    '''간단한 RAG 시스템 래퍼 클래스'''\n",
    "    def __init__(self, vectorstore, llm, retriever_k=3):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.llm = llm\n",
    "        self.retriever = vectorstore.as_retriever(search_type = 'similarity', search_kwargs={'k':retriever_k})\n",
    "        # self.retriever_chain = self._retriever_basic_chain()\n",
    "        self.chain = self._build_chain()\n",
    "    \n",
    "\n",
    "    def _build_chain(self): ### ---------> 최종 사용자에게 전달되는 프롬프트 수정\n",
    "        '''RAG 체인 구성''' \n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "                (\"system\",     \"\"\"\n",
    "    You are **\"AI Tech Trend Navigator\"**, an expert assistant for AI/ML research papers.\n",
    "\n",
    "    [Role]\n",
    "    - You help users understand and leverage recent AI/ML papers collected from HuggingFace DailyPapers.\n",
    "    - Your main goals are:\n",
    "      - Summarize and compare relevant papers clearly.\n",
    "      - Explain core ideas in simple terms.\n",
    "      - Highlight practical use-cases and implications for real-world services or products.\n",
    "\n",
    "    [Inputs]\n",
    "    The system provides:\n",
    "    - user_question: the user’s question.\n",
    "     - context: a set of retrieved documents, formatted as a single text block.\n",
    "        - Sometimes the context may be exactly the string \"NO_RELEVANT_PAPERS\".\n",
    "      - page_content: main text (abstract or summary)\n",
    "      - metadata:\n",
    "        - paper_name\n",
    "        - github_url (optional)\n",
    "        - huggingface_url (optional)\n",
    "        - upvote (integer, popularity signal)\n",
    "        - tags: list of keywords\n",
    "        - year, week, and other fields.\n",
    "\n",
    "    You must rely only on:\n",
    "    - the given context, and\n",
    "    - general, high-level AI/ML knowledge.\n",
    "    Do NOT invent specific paper titles, authors, datasets, metrics, or numerical results\n",
    "    that are not supported by the context.\n",
    "\n",
    "\n",
    "    [Context Handling]\n",
    "    - If the context is **\"NO_RELEVANT_PAPERS\"**, it means:\n",
    "    - The retrieval system could not find any clearly relevant papers.\n",
    "    - In this case, you may answer **purely from your own general AI/ML knowledge**.\n",
    "    - Do NOT fabricate specific paper titles, authors, datasets, or numerical results.\n",
    "    - You may skip the \"Related papers\" section or keep it very generic.\n",
    "\n",
    "    - If the context contains one or more papers:\n",
    "    - Prefer to base your answer on those papers.\n",
    "    - Use only the papers that are reasonably related to the user’s question.\n",
    "                 \n",
    "    [Main Tasks]\n",
    "\n",
    "    1. Understand the user’s intent\n",
    "       - Roughly classify the question as one of:\n",
    "         - (a) concept/background explanation\n",
    "         - (b) single-paper summary\n",
    "         - (c) comparison or trend analysis across multiple papers\n",
    "         - (d) practical application and use-case ideas\n",
    "       - If the intent is ambiguous, make a reasonable assumption and continue.\n",
    "         You may briefly state what you assumed.\n",
    "\n",
    "    2. Use only the relevant papers\n",
    "       - Focus on the most relevant 1–3 papers in the given context.\n",
    "       - If some papers look only weakly related to the question, you may ignore them.\n",
    "       - If nothing is clearly relevant, say that the context does not directly answer the question.\n",
    "\n",
    "    3. Summarize each selected paper\n",
    "       For each paper you rely on, briefly cover:\n",
    "       - What problem it tries to solve.\n",
    "       - What approach/model/idea it uses.\n",
    "       - What seems new or strong compared to typical or baseline methods.\n",
    "       - Any obvious limitations, trade-offs, or caveats that are visible from the context.\n",
    "\n",
    "    4. Produce a synthesized answer\n",
    "       - Do not just list papers. Synthesize them to directly answer the user’s question.\n",
    "       - When possible, cover:\n",
    "         - Common themes or trends across the papers.\n",
    "         - How these ideas relate to topics such as RAG, long-context, multimodal models, etc.,\n",
    "           when relevant.\n",
    "         - How someone could apply these ideas in a real-world project, prototype, or product.\n",
    "\n",
    "    5. Be honest about uncertainty\n",
    "       - If the given context is not enough to answer precisely, say so.\n",
    "       - Suggest what extra information, papers, or queries would be helpful.\n",
    "\n",
    "    [Style]\n",
    "    - Answer in the SAME LANGUAGE as the user’s question.\n",
    "      (If the question is in Korean, answer in Korean. If it is in English, answer in English.)\n",
    "    - Prefer clear, concise sentences over heavy academic wording.\n",
    "    - Briefly explain technical terms when needed.\n",
    "    - Never fabricate paper titles, authors, datasets, or numerical results.\n",
    "    \"\"\"),\n",
    "                \n",
    "    (\"human\", \"\"\"\n",
    "    [QUESTION]\n",
    "    {question}\n",
    "\n",
    "     [Context]\n",
    "    The following CONTEXT block may contain 0 or more papers. \n",
    "    If it is \"NO_RELEVANT_PAPERS\", please answer from your general AI/ML knowledge.\n",
    "     \n",
    "    [CONTEXT]\n",
    "    ======== START ========\n",
    "    {context}\n",
    "    ======== END =========\n",
    "\n",
    "    Please structure your answer as follows (flexible, but try to follow this):\n",
    "\n",
    "    1) One-line summary  \n",
    "    2) Key insights (3-6 bullets)  \n",
    "    3) Related papers (top 1~3)  \n",
    "    4) Detailed explanation  \n",
    "    5) Sources summary\n",
    "\n",
    "    ⚠ Do not hallucinate papers or details not shown in context.\n",
    "    Respond in the same language as the question.\n",
    "    \"\"\")\n",
    "            ])\n",
    "        return (\n",
    "            {\n",
    "                \"context\": self.retriever | self._format_docs,\n",
    "                \"question\": RunnablePassthrough(),\n",
    "                \"chat_history\": lambda x: \"\"\n",
    "            }\n",
    "            | prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _format_docs(docs):\n",
    "        \"\"\"retriever가 반환한 Document들을 프롬프트용 텍스트로 변환\"\"\"\n",
    "        if not docs:\n",
    "            # ⚠️ 컨텍스트가 전혀 없을 때는 이 문자열로 보냄\n",
    "            return \"NO_RELEVANT_PAPERS\"\n",
    "\n",
    "        lines = []\n",
    "        for i, doc in enumerate(docs, start=1):\n",
    "            md = doc.metadata or {}\n",
    "\n",
    "            # tag1, tag2, tag3 → tags 리스트로 재구성\n",
    "            tags = [\n",
    "                md.get(\"tag1\"),\n",
    "                md.get(\"tag2\"),\n",
    "                md.get(\"tag3\"),\n",
    "            ]\n",
    "            tags = [t for t in tags if t]  # None/빈 값 제거\n",
    "\n",
    "            tag_str = f\"tags: {', '.join(tags)}\" if tags else \"tags: (none)\"\n",
    "\n",
    "            paper_name = md.get(\"paper_name\", \"(no title)\")\n",
    "            hf_url = md.get(\"huggingface_url\", \"\")\n",
    "            gh_url = md.get(\"github_url\", \"\")\n",
    "\n",
    "            link_lines = []\n",
    "            if hf_url:\n",
    "                link_lines.append(f\"HuggingFace: {hf_url}\")\n",
    "            if gh_url:\n",
    "                link_lines.append(f\"GitHub: {gh_url}\")\n",
    "            links_block = \"\\n\".join(link_lines) if link_lines else \"\"\n",
    "\n",
    "            block = f\"\"\"[{i}] {paper_name}\n",
    "                                {tag_str}\n",
    "                                {links_block}\n",
    "\n",
    "                                {doc.page_content}\"\"\"\n",
    "            lines.append(block)\n",
    "\n",
    "        return \"\\n\\n\".join(lines)\n",
    "\n",
    "\n",
    "    def ask(self, question:str) -> str:\n",
    "        '''질문에 답변'''\n",
    "        return self.chain.invoke(question)\n",
    "    \n",
    "\n",
    "    def ask_with_sources(self, question: str) -> dict:\n",
    "        \"\"\"질문에 답변 + 출처 반환\"\"\"\n",
    "        answer = self.chain.invoke(question)\n",
    "        source_docs = self.retriever.invoke(question)\n",
    "\n",
    "        sources = []\n",
    "        for doc in source_docs:\n",
    "            md = doc.metadata or {}\n",
    "            tags = [\n",
    "                md.get(\"tag1\"),\n",
    "                md.get(\"tag2\"),\n",
    "                md.get(\"tag3\"),\n",
    "            ]\n",
    "            tags = [t for t in tags if t]\n",
    "\n",
    "            sources.append(\n",
    "                {\n",
    "                    \"paper_name\": md.get(\"paper_name\", \"(no title)\"),\n",
    "                    \"huggingface_url\": md.get(\"huggingface_url\"),\n",
    "                    \"github_url\": md.get(\"github_url\"),\n",
    "                    \"upvote\": md.get(\"upvote\"),\n",
    "                    \"tags\": tags,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"sources\": sources,\n",
    "        }\n",
    "   \n",
    "\n",
    "\n",
    "if __name__ == '__main__' :\n",
    "    # chunk 파일로 임시 확인\n",
    "    def get_project_root():\n",
    "        curr = Path().resolve()\n",
    "        for parent in [curr] + list(curr.parents):\n",
    "            if (parent / \".git\").exists():\n",
    "                return parent\n",
    "        raise FileNotFoundError(\"프로젝트 루트 찾기 실패\")\n",
    "\n",
    "    PROJECT_ROOT = get_project_root()\n",
    "    DATA_DIR = PROJECT_ROOT / \"01_data/chunks\"\n",
    "\n",
    "    chunks_path = DATA_DIR / \"chunks_all.pkl\"\n",
    "\n",
    "    with open(chunks_path, \"rb\") as f:\n",
    "        chunks = pickle.load(f)\n",
    "\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        collection_name='test',\n",
    "        embedding=OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "    )\n",
    "\n",
    "  \n",
    "    llm = ChatOpenAI( model = 'gpt-4o-mini', temperature=0 )\n",
    "\n",
    "    rag_system = SimpleRAGSystem(vectorstore, llm)\n",
    "    user_question = \"벡터DB가 뭐야?\"\n",
    "    result = rag_system.ask_with_sources(user_question)\n",
    "\n",
    "    print(f\"질문: {user_question}\")\n",
    "    print(\"\\n[답변]\\n\")\n",
    "    print(result[\"answer\"])\n",
    "\n",
    "    print(\"\\n[출처]\\n\")\n",
    "    for i, src in enumerate(result[\"sources\"], start=1):\n",
    "        print(f\"- [{i}] {src['paper_name']}\")\n",
    "        if src[\"huggingface_url\"]:\n",
    "            print(f\"  HF: {src['huggingface_url']}\")\n",
    "        if src[\"github_url\"]:\n",
    "            print(f\"  GitHub: {src['github_url']}\")\n",
    "        if src[\"tags\"]:\n",
    "            print(f\"  tags: {', '.join(src['tags'])}\")\n",
    "        if src[\"upvote\"] is not None:\n",
    "            print(f\"  upvote: {src['upvote']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
