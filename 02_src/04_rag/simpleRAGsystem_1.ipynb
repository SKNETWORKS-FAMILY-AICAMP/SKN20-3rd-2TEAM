{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3f7ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "래퍼 클래스 테스트: \n",
      "질문: VectorDB의 종류를 알려주세요\n",
      "답변: 사용자가 질문한 사항은 허깅페이스의 최근 5주차 데이터에 없습니다. GPT-4o 모델로 검색하여 답변하겠습니다. \n",
      "\n",
      "VectorDB의 종류에는 여러 가지가 있으며, 대표적으로는 FAISS, Annoy, Milvus, Weaviate, Pinecone 등이 있습니다. 이들 각각은 데이터 검색 및 유사도 검색을 위한 다양한 기능과 최적화된 성능을 제공합니다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import pickle    # chunk, vectorDB 저장한것 사용\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 경고메세지 삭제\n",
    "warnings.filterwarnings('ignore')\n",
    "load_dotenv()\n",
    "\n",
    "# openapi key 확인\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not api_key:\n",
    "    raise ValueError('.env확인,  key없음')\n",
    "\n",
    "# 필수 라이브러리 로드\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import time\n",
    "\n",
    "class SimpleRAGSystem:\n",
    "    '''간단한 RAG 시스템 래퍼 클래스'''\n",
    "    def __init__(self, vectorstore, llm, retriever_k=3):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.llm = llm\n",
    "        self.retriever = vectorstore.as_retriever(seaarch_type = 'similarity', search_kwargs={'k':retriever_k})\n",
    "        # self.retriever_chain = self._retriever_basic_chain()\n",
    "        self.chain = self._build_chain()\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # def _retriever_basic_chain(self): # -------------> 내부 문서 찾는 프롬프트 수정\n",
    "    #     '''retriever 검색'''\n",
    "    #     basic_prompt = ChatPromptTemplate.from_messages([\n",
    "    #             (\"system\", \"\"\"당신은 제공된 문맥(Context)을 바탕으로 질문에 답변하는 AI 어시스턴트입니다.\n",
    "\n",
    "    #         규칙:\n",
    "    #         1. 제공된 문맥 내의 정보만을 사용하여 답변하세요.\n",
    "    #         2. 문맥에 없는 정보는 \"제공된 문서에서 해당 정보를 찾을 수 없습니다.\"라고 답하세요.\n",
    "    #         3. 답변은 한국어로 명확하고 간결하게 작성하세요.\n",
    "    #         4. 가능하면 구조화된 형태(목록, 번호 등)로 답변하세요.\"\"\"),\n",
    "    #             (\"human\", \"\"\"문맥(Context): {context}\n",
    "\n",
    "    #         질문: {question}\n",
    "\n",
    "    #         답변:\"\"\")\n",
    "    #         ])\n",
    "\n",
    "    #     return (\n",
    "            \n",
    "    #     )\n",
    "\n",
    "\n",
    "    def _build_chain(self): ### ---------> 최종 사용자에게 전달되는 프롬프트 수정\n",
    "        '''RAG 체인 구성''' \n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "                (\"system\", \"\"\"You are a professional AI research assistant specializing in HuggingFace Daily Papers.\n",
    "\n",
    "        # Role\n",
    "        Maintain context across multiple turns of conversation while answering based on retrieved papers.\n",
    "\n",
    "        ## Conversational Guidelines \n",
    "        1. **Context Awareness**: Reference previous messages when relevant.\n",
    "\n",
    "        2. **Consistency**: Maintain consistent terminology across the conversation.\n",
    "\n",
    "        3. **Building Upon**: Build upon earlier answers with new retrieved information.\n",
    "\n",
    "        4. **Clarification**: If asked for clarification, refer back to previously cited papers.\n",
    "\n",
    "        ## Answer Rules\n",
    "        1. **Source-based**: Only use information from the provided context.\n",
    "\n",
    "        2. **Accuracy**: Do not make up information.\n",
    "\n",
    "        3. **Language**: Answer in Korean.\n",
    "\n",
    "        ## When Unable to Answer\n",
    "        If you cannot find the information:\n",
    "        \"사용자가 질문한 사항은 허깅페이스의 최근 5주차 데이터에 없습니다. GPT-4o 모델로 검색하여 답변하겠습니다.\"\n",
    "        라고 답변하고, 너가 검색해서 300자 내로 대답해. 이때, 사용자가 질문한 언어로 대답해줘.\"\"\"),\n",
    "                \n",
    "                (\"human\", \"\"\"## Previous Conversation \n",
    "        {chat_history}\n",
    "\n",
    "        ## Retrieved Papers \n",
    "        {context}\n",
    "\n",
    "        ## Current Question \n",
    "        {question}\n",
    "\n",
    "        ## Answer \"\"\")\n",
    "            ])\n",
    "        return(\n",
    "            {\n",
    "            'context': self.retriever | self._format_docs,\n",
    "            'question': RunnablePassthrough(),\n",
    "            'chat_history': lambda x: \"\"\n",
    "            }\n",
    "            | prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "    \n",
    "    \n",
    "    @staticmethod   \n",
    "    def _format_docs(docs):\n",
    "        return '\\n\\n'.join([doc.page_content for doc in docs])\n",
    "    \n",
    "\n",
    "    def ask(self, question:str) -> str:\n",
    "        '''질문에 답변'''\n",
    "        return self.chain.invoke(question)\n",
    "    \n",
    "\n",
    "    def ask_with_sources(self, question:str) -> dict : \n",
    "        '''질문에 답변 + 출처 반환'''\n",
    "        answer = self.chain.invoke(question)\n",
    "        sources = self.retriever.invoke(question)\n",
    "        return {\n",
    "            'answer' : answer\n",
    "            # 'source' : [ doc.metadata.get('source', 'unknown') for doc in sources]\n",
    "        }\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__' :\n",
    "    # vectorDB 로드 (vectorstore)\n",
    "    \n",
    "    pkl_path = r\"C:\\00project\\SKN20-3rd-2TEAM\\01_data\\chunks\\chunks_all.pkl\"\n",
    "    \n",
    "    with open(pkl_path, 'rb') as f:\n",
    "        chunks = pickle.load(f)\n",
    "\n",
    "    # ---- metadata 제거 ----\n",
    "    for doc in chunks:\n",
    "        doc.metadata = {}\n",
    "\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        collection_name='test',\n",
    "        embedding=OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "    )\n",
    "\n",
    "  \n",
    "    llm = ChatOpenAI( model = 'gpt-4o-mini', temperature=0 )\n",
    "\n",
    "    rag_system = SimpleRAGSystem(vectorstore, llm)\n",
    "\n",
    "    print('래퍼 클래스 테스트: ')\n",
    "    result = rag_system.ask_with_sources(\"VectorDB의 종류를 알려주세요\")\n",
    "    print(f'질문: VectorDB의 종류를 알려주세요')\n",
    "    print(f\"답변: {result['answer']}\")  \n",
    "    # print(f\"출처: {result['source']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27e8bc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "래퍼 클래스 테스트: \n",
      "질문: VectorDB의 종류를 알려주세요\n",
      "답변: 제공된 논문에서 해당 정보를 찾을 수 없습니다. 다른 질문을 해주시거나, 더 구체적으로 질문해주세요.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import pickle    # chunk, vectorDB 저장한것 사용\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 경고메세지 삭제\n",
    "warnings.filterwarnings('ignore')\n",
    "load_dotenv()\n",
    "\n",
    "# openapi key 확인\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not api_key:\n",
    "    raise ValueError('.env확인,  key없음')\n",
    "\n",
    "# 필수 라이브러리 로드\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import time\n",
    "\n",
    "class SimpleRAGSystem:\n",
    "    '''간단한 RAG 시스템 래퍼 클래스'''\n",
    "    def __init__(self, vectorstore, llm, retriever_k=3):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.llm = llm\n",
    "        self.retriever = vectorstore.as_retriever(seaarch_type = 'similarity', search_kwargs={'k':retriever_k})\n",
    "        # self.retriever_chain = self._retriever_basic_chain()\n",
    "        self.chain = self._build_chain()\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # def _retriever_basic_chain(self): # -------------> 내부 문서 찾는 프롬프트 수정\n",
    "    #     '''retriever 검색'''\n",
    "    #     basic_prompt = ChatPromptTemplate.from_messages([\n",
    "    #             (\"system\", \"\"\"당신은 제공된 문맥(Context)을 바탕으로 질문에 답변하는 AI 어시스턴트입니다.\n",
    "\n",
    "    #         규칙:\n",
    "    #         1. 제공된 문맥 내의 정보만을 사용하여 답변하세요.\n",
    "    #         2. 문맥에 없는 정보는 \"제공된 문서에서 해당 정보를 찾을 수 없습니다.\"라고 답하세요.\n",
    "    #         3. 답변은 한국어로 명확하고 간결하게 작성하세요.\n",
    "    #         4. 가능하면 구조화된 형태(목록, 번호 등)로 답변하세요.\"\"\"),\n",
    "    #             (\"human\", \"\"\"문맥(Context): {context}\n",
    "\n",
    "    #         질문: {question}\n",
    "\n",
    "    #         답변:\"\"\")\n",
    "    #         ])\n",
    "\n",
    "    #     return (\n",
    "            \n",
    "    #     )\n",
    "\n",
    "\n",
    "    def _build_chain(self): ### ---------> 최종 사용자에게 전달되는 프롬프트 수정\n",
    "        '''RAG 체인 구성''' \n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "                (\"system\", \"\"\"You are a professional AI research assistant specializing in HuggingFace Daily Papers.\n",
    "\n",
    "        # Role\n",
    "        Maintain context across multiple turns of conversation while answering based on retrieved papers.\n",
    "\n",
    "        ## Conversational Guidelines \n",
    "        1. **Context Awareness**: Reference previous messages when relevant.\n",
    "\n",
    "        2. **Consistency**: Maintain consistent terminology across the conversation.\n",
    "\n",
    "        3. **Building Upon**: Build upon earlier answers with new retrieved information.\n",
    "\n",
    "        4. **Clarification**: If asked for clarification, refer back to previously cited papers.\n",
    "\n",
    "        ## Answer Rules\n",
    "        1. **Source-based**: Only use information from the provided context.\n",
    "\n",
    "        2. **Accuracy**: Do not make up information.\n",
    "\n",
    "        3. **Language**: Answer in Korean.\n",
    "\n",
    "        ## When Unable to Answer\n",
    "        If you cannot find the information:\n",
    "        \"제공된 논문에서 해당 정보를 찾을 수 없습니다. 다른 질문을 해주시거나, 더 구체적으로 질문해주세요.\"\n",
    "라고 답변합니다.\"\"\"),\n",
    "                \n",
    "                (\"human\", \"\"\"## Previous Conversation \n",
    "        {chat_history}\n",
    "\n",
    "        ## Retrieved Papers \n",
    "        {context}\n",
    "\n",
    "        ## Current Question \n",
    "        {question}\n",
    "\n",
    "        ## Answer \"\"\")\n",
    "            ])\n",
    "        return(\n",
    "            {\n",
    "            'context': self.retriever | self._format_docs,\n",
    "            'question': RunnablePassthrough(),\n",
    "            'chat_history': lambda x: \"\"\n",
    "            }\n",
    "            | prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "    \n",
    "    \n",
    "    @staticmethod   \n",
    "    def _format_docs(docs):\n",
    "        return '\\n\\n'.join([doc.page_content for doc in docs])\n",
    "    \n",
    "\n",
    "    def ask(self, question:str) -> str:\n",
    "        '''질문에 답변'''\n",
    "        return self.chain.invoke(question)\n",
    "    \n",
    "\n",
    "    def ask_with_sources(self, question:str) -> dict : \n",
    "        '''질문에 답변 + 출처 반환'''\n",
    "        answer = self.chain.invoke(question)\n",
    "        sources = self.retriever.invoke(question)\n",
    "        return {\n",
    "            'answer' : answer\n",
    "            # 'source' : [ doc.metadata.get('source', 'unknown') for doc in sources]\n",
    "        }\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__' :\n",
    "    # vectorDB 로드 (vectorstore)\n",
    "    \n",
    "    pkl_path = r\"C:\\00project\\SKN20-3rd-2TEAM\\01_data\\chunks\\chunks_all.pkl\"\n",
    "    \n",
    "    with open(pkl_path, 'rb') as f:\n",
    "        chunks = pickle.load(f)\n",
    "\n",
    "    # ---- metadata 제거 ----\n",
    "    for doc in chunks:\n",
    "        doc.metadata = {}\n",
    "\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        collection_name='test',\n",
    "        embedding=OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "    )\n",
    "\n",
    "  \n",
    "    llm = ChatOpenAI( model = 'gpt-4o-mini', temperature=0 )\n",
    "\n",
    "    rag_system = SimpleRAGSystem(vectorstore, llm)\n",
    "\n",
    "    print('래퍼 클래스 테스트: ')\n",
    "    result = rag_system.ask_with_sources(\"VectorDB의 종류를 알려주세요\")\n",
    "    print(f'질문: VectorDB의 종류를 알려주세요')\n",
    "    print(f\"답변: {result['answer']}\")  \n",
    "    # print(f\"출처: {result['source']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac659d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9c2947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2068ddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a professional AI research assistant specializing in HuggingFace Daily Papers.\n",
    "\n",
    "\n",
    "## Role (역할)\n",
    "Maintain context across multiple turns of conversation while answering based on retrieved papers.\n",
    "여러 차례의 대화에 걸쳐 맥락을 유지하면서 검색된 논문을 기반으로 답변합니다.\n",
    "\n",
    "## Conversational Guidelines (대화 가이드라인)\n",
    "1. **Context Awareness (맥락 인식)**: Reference previous messages when relevant.\n",
    "   관련성이 있을 때 이전 메시지를 참조합니다.\n",
    "\n",
    "2. **Consistency (일관성)**: Maintain consistent terminology across the conversation.\n",
    "   대화 전반에 걸쳐 일관된 용어를 유지합니다.\n",
    "\n",
    "3. **Building Upon (보완)**: Build upon earlier answers with new retrieved information.\n",
    "   새로 검색된 정보로 이전 답변을 보완합니다.\n",
    "\n",
    "4. **Clarification (명확화)**: If asked for clarification, refer back to previously cited papers.\n",
    "   명확화를 요청받으면 이전에 인용한 논문을 다시 참조합니다.\n",
    "\n",
    "## Answer Rules (답변 규칙)\n",
    "1. **Source-based (출처 기반)**: Only use information from the provided context.\n",
    "   반드시 제공된 문맥의 정보만 사용합니다.\n",
    "\n",
    "2. **Accuracy (정확성)**: Do not make up information.\n",
    "   정보를 지어내지 않습니다.\n",
    "\n",
    "3. **Language (언어)**: Answer in Korean.\n",
    "   한국어로 답변합니다.\n",
    "\n",
    "## When Unable to Answer (답변 불가 시)\n",
    "If you cannot find the information:\n",
    "정보를 찾을 수 없으면:\n",
    "\"제공된 논문에서 해당 정보를 찾을 수 없습니다. 다른 질문을 해주시거나, 더 구체적으로 질문해주세요.\"\n",
    "라고 답변합니다.\"\"\"),\n",
    "        \n",
    "        (\"human\", \"\"\"## Previous Conversation (이전 대화)\n",
    "{chat_history}\n",
    "\n",
    "## Retrieved Papers (참조 논문)\n",
    "{context}\n",
    "\n",
    "## Current Question (현재 질문)\n",
    "{question}\n",
    "\n",
    "## Answer (답변)\"\"\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c171e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e997083",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
